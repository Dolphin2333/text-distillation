Torch Seed Specified with rank: 0
Dataset: agnews_emb
Dataset Path: ./scripts
Namespace(seed=0, mp_distributed=False, world_size=1, rank=0, dist_url='tcp://224.66.41.62:23456', dist_backend='nccl', workers=0, gpu=None, root='./scripts', dataset='agnews_emb', arch='text_transformer', width=256, lr=0.001, inner_optim='Adam', outer_optim='Adam', inner_lr=0.001, label_lr_scale=1, num_per_class=5, batch_per_class=5, task_sampler_nc=4, window=20, minwindow=0, totwindow=40, num_train_eval=2, train_y=False, batch_size=4096, eps=1e-08, wd=0, test_freq=5, print_freq=20, start_epoch=0, epochs=200, ddtype='standard', cctype=0, zca=False, wandb=False, clip_coef=0.9, fname='out_tf_ratbptt_ipc05_s1_ep200', out_dir='./checkpoints', name='agnews_tf_ratbptt_s1_ep200', comp_aug=False, comp_aug_real=False, syn_strategy='none', real_strategy='none', ckptname='none', limit_train=False, load_ckpt=False, complete_random=False, boost_dd=False, boost_init_from='none', boost_beta=1.0, stage=1, distributed=False, data_root='./scripts/agnews_emb')
==> Preparing data..
None None
Dataset: number of classes: 4
Training set size: 120000
Image size: channel 1, height 768, width 1
Synthetic images, not_single False, keys []
==> Building model..
Initialized data with size, x: torch.Size([20, 768]), y:torch.Size([20])
TextTransformer(
  (input_proj): Linear(in_features=192, out_features=256, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls_head): Linear(in_features=256, out_features=4, bias=True)
)
use data parallel only
GPU_0_using curriculum 20 with window 20
Epoch: [0][20/30]	Time  1.667 ( 1.776)	Data  0.047 ( 0.063)	InnerLoop  0.704 ( 0.773)	Loss 1.9364e+00 (3.7299e+00)	Acc@1  44.02 ( 32.51)
The current update step is 30
GPU_0_using curriculum 20 with window 20
Epoch: [1][20/30]	Time  1.655 ( 1.695)	Data  0.047 ( 0.073)	InnerLoop  0.697 ( 0.703)	Loss 1.2207e+00 (1.9233e+00)	Acc@1  57.13 ( 46.87)
The current update step is 60
GPU_0_using curriculum 20 with window 20
Epoch: [2][20/30]	Time  1.790 ( 1.702)	Data  0.046 ( 0.081)	InnerLoop  0.827 ( 0.700)	Loss 1.4721e+00 (1.3481e+00)	Acc@1  48.83 ( 54.74)
The current update step is 90
GPU_0_using curriculum 20 with window 20
Epoch: [3][20/30]	Time  1.673 ( 1.689)	Data  0.046 ( 0.061)	InnerLoop  0.708 ( 0.714)	Loss 1.6914e+00 (1.2343e+00)	Acc@1  48.00 ( 58.13)
The current update step is 120
GPU_0_using curriculum 20 with window 20
Epoch: [4][20/30]	Time  1.672 ( 1.693)	Data  0.046 ( 0.067)	InnerLoop  0.714 ( 0.710)	Loss 1.2916e+00 (1.2960e+00)	Acc@1  54.08 ( 58.76)
The current update step is 150
The current seed is 11876837058639758691
The current lr is: 0.001
Testing Results:
 *   Acc@1 67.605
 *   Acc@1 67.511
 *   Acc@1 68.276
 *   Acc@1 68.247
 *   Acc@1 67.776
 *   Acc@1 67.442
 *   Acc@1 63.355
 *   Acc@1 63.304
 *   Acc@1 60.145
 *   Acc@1 60.054
 *   Acc@1 58.105
 *   Acc@1 58.620
Training for 300 epoch: 65.48026315789474
Training for 600 epoch: 64.21052631578948
Training for 1000 epoch: 62.94078947368421
Training for 300 epoch: 65.4075
Training for 600 epoch: 64.15041666666667
Training for 1000 epoch: 63.03125
[[65.48026315789474, 64.21052631578948, 62.94078947368421], [65.4075, 64.15041666666667, 63.03125]]
train loss 0.6412919870058695, epoch 4, best loss 0.6412919870058695, best_epoch 4
GPU_0_using curriculum 20 with window 20
Epoch: [5][20/30]	Time  1.783 ( 1.683)	Data  0.181 ( 0.080)	InnerLoop  0.684 ( 0.695)	Loss 9.2390e-01 (1.2237e+00)	Acc@1  64.31 ( 59.67)
The current update step is 180
GPU_0_using curriculum 20 with window 20
Epoch: [6][20/30]	Time  1.811 ( 1.687)	Data  0.047 ( 0.067)	InnerLoop  0.825 ( 0.710)	Loss 1.4289e+00 (9.9439e-01)	Acc@1  56.37 ( 63.85)
The current update step is 210
GPU_0_using curriculum 20 with window 20
Epoch: [7][20/30]	Time  1.627 ( 1.663)	Data  0.047 ( 0.060)	InnerLoop  0.693 ( 0.704)	Loss 1.1955e+00 (1.0680e+00)	Acc@1  56.79 ( 61.82)
The current update step is 240
GPU_0_using curriculum 20 with window 20
Epoch: [8][20/30]	Time  1.603 ( 1.629)	Data  0.046 ( 0.065)	InnerLoop  0.674 ( 0.684)	Loss 6.7185e-01 (9.5330e-01)	Acc@1  74.34 ( 65.04)
The current update step is 270
GPU_0_using curriculum 20 with window 20
Epoch: [9][20/30]	Time  1.595 ( 1.628)	Data  0.047 ( 0.052)	InnerLoop  0.671 ( 0.697)	Loss 8.4839e-01 (8.8690e-01)	Acc@1  68.60 ( 67.99)
The current update step is 300
The current seed is 2465679112794455711
The current lr is: 0.001
Testing Results:
 *   Acc@1 70.355
 *   Acc@1 70.221
 *   Acc@1 71.118
 *   Acc@1 71.318
 *   Acc@1 71.171
 *   Acc@1 71.242
 *   Acc@1 64.592
 *   Acc@1 64.526
 *   Acc@1 67.132
 *   Acc@1 66.927
 *   Acc@1 68.289
 *   Acc@1 68.515
Training for 300 epoch: 67.47368421052632
Training for 600 epoch: 69.125
Training for 1000 epoch: 69.73026315789474
Training for 300 epoch: 67.37333333333333
Training for 600 epoch: 69.12291666666667
Training for 1000 epoch: 69.87833333333333
[[67.47368421052632, 69.125, 69.73026315789474], [67.37333333333333, 69.12291666666667, 69.87833333333333]]
train loss 0.4458038070042928, epoch 9, best loss 0.4458038070042928, best_epoch 9
GPU_0_using curriculum 20 with window 20
Epoch: [10][20/30]	Time  1.717 ( 1.621)	Data  0.181 ( 0.077)	InnerLoop  0.659 ( 0.676)	Loss 8.7854e-01 (8.9541e-01)	Acc@1  66.65 ( 66.13)
The current update step is 330
GPU_0_using curriculum 20 with window 20
Epoch: [11][20/30]	Time  1.699 ( 1.602)	Data  0.043 ( 0.064)	InnerLoop  0.804 ( 0.683)	Loss 6.5588e-01 (9.2891e-01)	Acc@1  77.47 ( 67.12)
The current update step is 360
GPU_0_using curriculum 20 with window 20
Epoch: [12][20/30]	Time  1.571 ( 1.590)	Data  0.046 ( 0.057)	InnerLoop  0.659 ( 0.680)	Loss 7.4220e-01 (7.6055e-01)	Acc@1  70.97 ( 71.96)
The current update step is 390
GPU_0_using curriculum 20 with window 20
Epoch: [13][20/30]	Time  1.554 ( 1.593)	Data  0.043 ( 0.064)	InnerLoop  0.657 ( 0.677)	Loss 1.0485e+00 (8.4774e-01)	Acc@1  66.75 ( 69.57)
The current update step is 420
GPU_0_using curriculum 20 with window 20
Epoch: [14][20/30]	Time  1.556 ( 1.594)	Data  0.044 ( 0.051)	InnerLoop  0.659 ( 0.691)	Loss 6.9122e-01 (8.2601e-01)	Acc@1  75.37 ( 70.14)
The current update step is 450
The current seed is 5459361333559042550
The current lr is: 0.001
Testing Results:
 *   Acc@1 69.579
 *   Acc@1 69.388
 *   Acc@1 68.987
 *   Acc@1 68.858
 *   Acc@1 68.368
 *   Acc@1 68.418
 *   Acc@1 74.447
 *   Acc@1 74.782
 *   Acc@1 74.711
 *   Acc@1 74.959
 *   Acc@1 73.829
 *   Acc@1 74.294
Training for 300 epoch: 72.01315789473685
Training for 600 epoch: 71.84868421052633
Training for 1000 epoch: 71.09868421052632
Training for 300 epoch: 72.08541666666667
Training for 600 epoch: 71.90833333333333
Training for 1000 epoch: 71.35583333333334
[[72.01315789473685, 71.84868421052633, 71.09868421052632], [72.08541666666667, 71.90833333333333, 71.35583333333334]]
train loss 0.3523392293135325, epoch 14, best loss 0.3523392293135325, best_epoch 14
GPU_0_using curriculum 20 with window 20
Epoch: [15][20/30]	Time  1.687 ( 1.602)	Data  0.177 ( 0.077)	InnerLoop  0.660 ( 0.671)	Loss 7.0643e-01 (8.8028e-01)	Acc@1  75.29 ( 68.23)
The current update step is 480
GPU_0_using curriculum 20 with window 20
Epoch: [16][20/30]	Time  1.708 ( 1.610)	Data  0.042 ( 0.065)	InnerLoop  0.796 ( 0.686)	Loss 6.4609e-01 (8.2776e-01)	Acc@1  77.15 ( 70.68)
The current update step is 510
GPU_0_using curriculum 20 with window 20
Epoch: [17][20/30]	Time  1.550 ( 1.595)	Data  0.044 ( 0.058)	InnerLoop  0.653 ( 0.681)	Loss 7.1373e-01 (7.9741e-01)	Acc@1  72.95 ( 71.15)
The current update step is 540
GPU_0_using curriculum 20 with window 20
Epoch: [18][20/30]	Time  1.561 ( 1.589)	Data  0.047 ( 0.064)	InnerLoop  0.656 ( 0.670)	Loss 9.3350e-01 (7.8926e-01)	Acc@1  64.48 ( 71.43)
The current update step is 570
GPU_0_using curriculum 20 with window 20
Epoch: [19][20/30]	Time  1.547 ( 1.586)	Data  0.047 ( 0.051)	InnerLoop  0.652 ( 0.681)	Loss 9.4587e-01 (8.2921e-01)	Acc@1  64.97 ( 70.21)
The current update step is 600
The current seed is 10901108825963796316
The current lr is: 0.001
Testing Results:
 *   Acc@1 70.316
 *   Acc@1 70.877
 *   Acc@1 70.184
 *   Acc@1 70.442
 *   Acc@1 69.329
 *   Acc@1 69.305
 *   Acc@1 66.829
 *   Acc@1 66.279
 *   Acc@1 64.868
 *   Acc@1 64.603
 *   Acc@1 62.908
 *   Acc@1 62.421
Training for 300 epoch: 68.57236842105263
Training for 600 epoch: 67.52631578947368
Training for 1000 epoch: 66.11842105263158
Training for 300 epoch: 68.57791666666667
Training for 600 epoch: 67.52208333333334
Training for 1000 epoch: 65.86291666666668
[[68.57236842105263, 67.52631578947368, 66.11842105263158], [68.57791666666667, 67.52208333333334, 65.86291666666668]]
train loss 0.49781217748324075, epoch 19, best loss 0.3523392293135325, best_epoch 14
GPU_0_using curriculum 20 with window 20
Epoch: [20][20/30]	Time  1.696 ( 1.596)	Data  0.178 ( 0.077)	InnerLoop  0.663 ( 0.662)	Loss 8.6265e-01 (8.9294e-01)	Acc@1  70.19 ( 68.14)
The current update step is 630
GPU_0_using curriculum 20 with window 20
Epoch: [21][20/30]	Time  1.708 ( 1.594)	Data  0.042 ( 0.064)	InnerLoop  0.799 ( 0.676)	Loss 1.0916e+00 (9.9334e-01)	Acc@1  51.93 ( 66.02)
The current update step is 660
GPU_0_using curriculum 20 with window 20
Epoch: [22][20/30]	Time  1.564 ( 1.589)	Data  0.044 ( 0.057)	InnerLoop  0.666 ( 0.676)	Loss 8.3581e-01 (7.7974e-01)	Acc@1  71.36 ( 72.17)
The current update step is 690
GPU_0_using curriculum 20 with window 20
Epoch: [23][20/30]	Time  1.545 ( 1.578)	Data  0.047 ( 0.063)	InnerLoop  0.648 ( 0.662)	Loss 7.0891e-01 (7.3298e-01)	Acc@1  71.85 ( 73.17)
The current update step is 720
GPU_0_using curriculum 20 with window 20
Epoch: [24][20/30]	Time  1.549 ( 1.582)	Data  0.044 ( 0.050)	InnerLoop  0.650 ( 0.680)	Loss 6.4997e-01 (8.4004e-01)	Acc@1  77.64 ( 69.90)
The current update step is 750
The current seed is 12010663577266728127
The current lr is: 0.001
Testing Results:
 *   Acc@1 63.776
 *   Acc@1 64.021
 *   Acc@1 63.289
 *   Acc@1 63.886
 *   Acc@1 65.118
 *   Acc@1 65.245
 *   Acc@1 69.658
 *   Acc@1 70.160
 *   Acc@1 69.368
 *   Acc@1 69.873
 *   Acc@1 69.803
 *   Acc@1 70.243
Training for 300 epoch: 66.71710526315789
Training for 600 epoch: 66.32894736842105
Training for 1000 epoch: 67.46052631578948
Training for 300 epoch: 67.09041666666667
Training for 600 epoch: 66.87958333333333
Training for 1000 epoch: 67.74375
[[66.71710526315789, 66.32894736842105, 67.46052631578948], [67.09041666666667, 66.87958333333333, 67.74375]]
train loss 0.40340638297398884, epoch 24, best loss 0.3523392293135325, best_epoch 14
GPU_0_using curriculum 20 with window 20
Epoch: [25][20/30]	Time  1.698 ( 1.598)	Data  0.171 ( 0.076)	InnerLoop  0.673 ( 0.665)	Loss 1.1815e+00 (8.6354e-01)	Acc@1  62.06 ( 68.92)
The current update step is 780
GPU_0_using curriculum 20 with window 20
Epoch: [26][20/30]	Time  1.686 ( 1.593)	Data  0.043 ( 0.063)	InnerLoop  0.785 ( 0.677)	Loss 1.0183e+00 (7.9288e-01)	Acc@1  61.16 ( 71.91)
The current update step is 810
GPU_0_using curriculum 20 with window 20
Epoch: [27][20/30]	Time  1.572 ( 1.586)	Data  0.044 ( 0.057)	InnerLoop  0.675 ( 0.678)	Loss 5.7629e-01 (8.8613e-01)	Acc@1  79.64 ( 68.68)
The current update step is 840
GPU_0_using curriculum 20 with window 20
Epoch: [28][20/30]	Time  1.550 ( 1.586)	Data  0.045 ( 0.063)	InnerLoop  0.655 ( 0.672)	Loss 7.2316e-01 (7.9362e-01)	Acc@1  74.07 ( 71.52)
The current update step is 870
GPU_0_using curriculum 20 with window 20
Epoch: [29][20/30]	Time  1.579 ( 1.595)	Data  0.046 ( 0.050)	InnerLoop  0.682 ( 0.689)	Loss 9.9613e-01 (8.1278e-01)	Acc@1  65.50 ( 70.24)
The current update step is 900
The current seed is 8459227949790010799
The current lr is: 0.001
Testing Results:
 *   Acc@1 62.816
 *   Acc@1 62.626
 *   Acc@1 58.803
 *   Acc@1 58.536
 *   Acc@1 56.092
 *   Acc@1 55.977
 *   Acc@1 68.276
 *   Acc@1 68.302
 *   Acc@1 63.539
 *   Acc@1 63.698
 *   Acc@1 63.539
 *   Acc@1 63.207
Training for 300 epoch: 65.54605263157895
Training for 600 epoch: 61.171052631578945
Training for 1000 epoch: 59.81578947368421
Training for 300 epoch: 65.46375
Training for 600 epoch: 61.11708333333333
Training for 1000 epoch: 59.59166666666667
[[65.54605263157895, 61.171052631578945, 59.81578947368421], [65.46375, 61.11708333333333, 59.59166666666667]]
train loss 0.44137544147173563, epoch 29, best loss 0.3523392293135325, best_epoch 14
GPU_0_using curriculum 20 with window 20
Epoch: [30][20/30]	Time  1.695 ( 1.594)	Data  0.172 ( 0.076)	InnerLoop  0.646 ( 0.663)	Loss 5.8003e-01 (7.6167e-01)	Acc@1  78.76 ( 72.27)
The current update step is 930
GPU_0_using curriculum 20 with window 20
Epoch: [31][20/30]	Time  1.703 ( 1.592)	Data  0.044 ( 0.064)	InnerLoop  0.794 ( 0.674)	Loss 6.4852e-01 (8.9703e-01)	Acc@1  77.47 ( 69.03)
The current update step is 960
GPU_0_using curriculum 20 with window 20
Epoch: [32][20/30]	Time  1.557 ( 1.584)	Data  0.045 ( 0.058)	InnerLoop  0.655 ( 0.673)	Loss 8.3335e-01 (8.4857e-01)	Acc@1  68.75 ( 68.50)
The current update step is 990
GPU_0_using curriculum 20 with window 20
Epoch: [33][20/30]	Time  1.571 ( 1.586)	Data  0.046 ( 0.063)	InnerLoop  0.663 ( 0.669)	Loss 8.1749e-01 (8.3653e-01)	Acc@1  71.46 ( 71.44)
The current update step is 1020
GPU_0_using curriculum 20 with window 20
Epoch: [34][20/30]	Time  1.540 ( 1.574)	Data  0.045 ( 0.050)	InnerLoop  0.648 ( 0.676)	Loss 1.4012e+00 (7.9960e-01)	Acc@1  56.05 ( 72.44)
The current update step is 1050
The current seed is 17568567956430572478
The current lr is: 0.001
Testing Results:
 *   Acc@1 77.934
 *   Acc@1 78.263
 *   Acc@1 77.921
 *   Acc@1 77.978
 *   Acc@1 77.171
 *   Acc@1 77.198
 *   Acc@1 68.289
 *   Acc@1 68.256
 *   Acc@1 69.868
 *   Acc@1 69.871
 *   Acc@1 70.316
 *   Acc@1 70.390
Training for 300 epoch: 73.11184210526315
Training for 600 epoch: 73.89473684210526
Training for 1000 epoch: 73.74342105263158
Training for 300 epoch: 73.25958333333332
Training for 600 epoch: 73.92458333333335
Training for 1000 epoch: 73.79375
[[73.11184210526315, 73.89473684210526, 73.74342105263158], [73.25958333333332, 73.92458333333335, 73.79375]]
train loss 0.3988831093788147, epoch 34, best loss 0.3523392293135325, best_epoch 14
GPU_0_using curriculum 20 with window 20
Epoch: [35][20/30]	Time  1.663 ( 1.581)	Data  0.176 ( 0.075)	InnerLoop  0.641 ( 0.655)	Loss 8.1703e-01 (8.6577e-01)	Acc@1  71.29 ( 70.24)
The current update step is 1080
GPU_0_using curriculum 20 with window 20
Epoch: [36][20/30]	Time  1.667 ( 1.573)	Data  0.041 ( 0.063)	InnerLoop  0.777 ( 0.663)	Loss 7.6559e-01 (8.1088e-01)	Acc@1  67.87 ( 70.66)
The current update step is 1110
GPU_0_using curriculum 20 with window 20
Epoch: [37][20/30]	Time  1.563 ( 1.572)	Data  0.043 ( 0.056)	InnerLoop  0.641 ( 0.663)	Loss 7.1803e-01 (7.4252e-01)	Acc@1  71.95 ( 72.84)
The current update step is 1140
GPU_0_using curriculum 20 with window 20
Epoch: [38][20/30]	Time  1.532 ( 1.567)	Data  0.046 ( 0.063)	InnerLoop  0.642 ( 0.655)	Loss 8.8066e-01 (7.4596e-01)	Acc@1  69.21 ( 72.61)
The current update step is 1170
GPU_0_using curriculum 20 with window 20
Epoch: [39][20/30]	Time  1.525 ( 1.572)	Data  0.043 ( 0.050)	InnerLoop  0.637 ( 0.670)	Loss 7.3198e-01 (6.8899e-01)	Acc@1  73.17 ( 74.86)
The current update step is 1200
The current seed is 13868580269986335080
The current lr is: 0.001
Testing Results:
 *   Acc@1 67.776
 *   Acc@1 67.649
 *   Acc@1 68.276
 *   Acc@1 68.421
 *   Acc@1 70.382
 *   Acc@1 70.298
 *   Acc@1 68.961
 *   Acc@1 69.578
 *   Acc@1 68.132
 *   Acc@1 68.134
 *   Acc@1 65.289
 *   Acc@1 65.299
Training for 300 epoch: 68.36842105263159
Training for 600 epoch: 68.20394736842105
Training for 1000 epoch: 67.83552631578948
Training for 300 epoch: 68.61333333333334
Training for 600 epoch: 68.2775
Training for 1000 epoch: 67.79875
[[68.36842105263159, 68.20394736842105, 67.83552631578948], [68.61333333333334, 68.2775, 67.79875]]
train loss 0.4306217458724976, epoch 39, best loss 0.3523392293135325, best_epoch 14
GPU_0_using curriculum 20 with window 20
Epoch: [40][20/30]	Time  1.672 ( 1.577)	Data  0.178 ( 0.076)	InnerLoop  0.641 ( 0.651)	Loss 7.5722e-01 (7.7732e-01)	Acc@1  71.44 ( 71.56)
The current update step is 1230
GPU_0_using curriculum 20 with window 20
Epoch: [41][20/30]	Time  1.660 ( 1.585)	Data  0.041 ( 0.062)	InnerLoop  0.773 ( 0.670)	Loss 7.5228e-01 (8.6557e-01)	Acc@1  71.56 ( 70.29)
The current update step is 1260
GPU_0_using curriculum 20 with window 20
Epoch: [42][20/30]	Time  1.540 ( 1.569)	Data  0.041 ( 0.055)	InnerLoop  0.647 ( 0.667)	Loss 1.3203e+00 (7.4960e-01)	Acc@1  57.15 ( 71.51)
The current update step is 1290
GPU_0_using curriculum 20 with window 20
Epoch: [43][20/30]	Time  1.548 ( 1.566)	Data  0.045 ( 0.062)	InnerLoop  0.652 ( 0.660)	Loss 6.2372e-01 (7.6656e-01)	Acc@1  77.66 ( 72.41)
The current update step is 1320
GPU_0_using curriculum 20 with window 20
Epoch: [44][20/30]	Time  1.528 ( 1.566)	Data  0.045 ( 0.050)	InnerLoop  0.642 ( 0.668)	Loss 7.9124e-01 (7.5174e-01)	Acc@1  74.39 ( 73.22)
The current update step is 1350
The current seed is 3428803701746994234
The current lr is: 0.001
Testing Results:
 *   Acc@1 71.566
 *   Acc@1 72.445
 *   Acc@1 71.697
 *   Acc@1 71.948
 *   Acc@1 71.882
 *   Acc@1 72.070
 *   Acc@1 71.974
 *   Acc@1 72.236
 *   Acc@1 71.197
 *   Acc@1 71.413
 *   Acc@1 71.263
 *   Acc@1 71.368
Training for 300 epoch: 71.76973684210526
Training for 600 epoch: 71.44736842105263
Training for 1000 epoch: 71.57236842105263
Training for 300 epoch: 72.34041666666667
Training for 600 epoch: 71.68083333333334
Training for 1000 epoch: 71.71916666666667
[[71.76973684210526, 71.44736842105263, 71.57236842105263], [72.34041666666667, 71.68083333333334, 71.71916666666667]]
train loss 0.3981001272678375, epoch 44, best loss 0.3523392293135325, best_epoch 14
GPU_0_using curriculum 20 with window 20
Epoch: [45][20/30]	Time  1.661 ( 1.570)	Data  0.171 ( 0.075)	InnerLoop  0.640 ( 0.648)	Loss 6.5309e-01 (7.9209e-01)	Acc@1  75.95 ( 72.12)
The current update step is 1380
GPU_0_using curriculum 20 with window 20
Epoch: [46][20/30]	Time  1.669 ( 1.568)	Data  0.042 ( 0.063)	InnerLoop  0.773 ( 0.662)	Loss 9.5376e-01 (6.9862e-01)	Acc@1  65.41 ( 74.60)
The current update step is 1410
GPU_0_using curriculum 20 with window 20
Epoch: [47][20/30]	Time  1.547 ( 1.568)	Data  0.045 ( 0.057)	InnerLoop  0.640 ( 0.662)	Loss 6.7559e-01 (8.1007e-01)	Acc@1  75.32 ( 70.60)
The current update step is 1440
GPU_0_using curriculum 20 with window 20
Epoch: [48][20/30]	Time  1.541 ( 1.563)	Data  0.042 ( 0.063)	InnerLoop  0.637 ( 0.654)	Loss 6.3385e-01 (7.6175e-01)	Acc@1  77.81 ( 72.22)
The current update step is 1470
GPU_0_using curriculum 20 with window 20
Epoch: [49][20/30]	Time  1.536 ( 1.566)	Data  0.041 ( 0.050)	InnerLoop  0.648 ( 0.669)	Loss 8.5339e-01 (9.0021e-01)	Acc@1  71.39 ( 67.29)
The current update step is 1500
The current seed is 535760621819834815
The current lr is: 0.001
Testing Results:
 *   Acc@1 67.895
 *   Acc@1 68.080
 *   Acc@1 68.434
 *   Acc@1 68.589
 *   Acc@1 70.171
 *   Acc@1 69.582
 *   Acc@1 76.211
 *   Acc@1 75.612
 *   Acc@1 75.947
 *   Acc@1 75.462
 *   Acc@1 75.421
 *   Acc@1 75.196
Training for 300 epoch: 72.05263157894737
Training for 600 epoch: 72.19078947368422
Training for 1000 epoch: 72.79605263157895
Training for 300 epoch: 71.84625
Training for 600 epoch: 72.02541666666667
Training for 1000 epoch: 72.38916666666667
[[72.05263157894737, 72.19078947368422, 72.79605263157895], [71.84625, 72.02541666666667, 72.38916666666667]]
train loss 0.34027470251719155, epoch 49, best loss 0.34027470251719155, best_epoch 49
GPU_0_using curriculum 20 with window 20
Epoch: [50][20/30]	Time  1.661 ( 1.574)	Data  0.176 ( 0.076)	InnerLoop  0.639 ( 0.652)	Loss 6.8217e-01 (7.1476e-01)	Acc@1  70.87 ( 73.64)
The current update step is 1530
GPU_0_using curriculum 20 with window 20
Epoch: [51][20/30]	Time  1.658 ( 1.574)	Data  0.042 ( 0.063)	InnerLoop  0.771 ( 0.664)	Loss 7.1538e-01 (6.9215e-01)	Acc@1  73.24 ( 74.53)
The current update step is 1560
GPU_0_using curriculum 20 with window 20
Epoch: [52][20/30]	Time  1.557 ( 1.568)	Data  0.044 ( 0.056)	InnerLoop  0.650 ( 0.665)	Loss 7.6252e-01 (7.2576e-01)	Acc@1  70.09 ( 73.55)
The current update step is 1590
GPU_0_using curriculum 20 with window 20
Epoch: [53][20/30]	Time  1.545 ( 1.573)	Data  0.045 ( 0.063)	InnerLoop  0.651 ( 0.657)	Loss 6.0173e-01 (7.7122e-01)	Acc@1  78.91 ( 72.77)
The current update step is 1620
GPU_0_using curriculum 20 with window 20
Epoch: [54][20/30]	Time  1.536 ( 1.570)	Data  0.044 ( 0.049)	InnerLoop  0.644 ( 0.670)	Loss 8.4990e-01 (7.4573e-01)	Acc@1  68.09 ( 72.67)
The current update step is 1650
The current seed is 16154697585341079056
The current lr is: 0.001
Testing Results:
 *   Acc@1 65.763
 *   Acc@1 65.644
 *   Acc@1 65.671
 *   Acc@1 66.018
 *   Acc@1 66.224
 *   Acc@1 66.013
 *   Acc@1 52.934
 *   Acc@1 53.008
 *   Acc@1 50.684
 *   Acc@1 50.687
 *   Acc@1 47.224
 *   Acc@1 47.660
Training for 300 epoch: 59.348684210526315
Training for 600 epoch: 58.17763157894737
Training for 1000 epoch: 56.723684210526315
Training for 300 epoch: 59.325833333333335
Training for 600 epoch: 58.3525
Training for 1000 epoch: 56.836666666666666
[[59.348684210526315, 58.17763157894737, 56.723684210526315], [59.325833333333335, 58.3525, 56.836666666666666]]
train loss 0.7414670419375101, epoch 54, best loss 0.34027470251719155, best_epoch 49
GPU_0_using curriculum 20 with window 20
Epoch: [55][20/30]	Time  1.677 ( 1.571)	Data  0.176 ( 0.075)	InnerLoop  0.648 ( 0.650)	Loss 6.7056e-01 (7.8782e-01)	Acc@1  74.78 ( 71.34)
The current update step is 1680
GPU_0_using curriculum 20 with window 20
Epoch: [56][20/30]	Time  1.664 ( 1.577)	Data  0.043 ( 0.063)	InnerLoop  0.776 ( 0.666)	Loss 9.1970e-01 (7.6600e-01)	Acc@1  59.42 ( 71.70)
The current update step is 1710
GPU_0_using curriculum 20 with window 20
Epoch: [57][20/30]	Time  1.535 ( 1.564)	Data  0.043 ( 0.056)	InnerLoop  0.644 ( 0.663)	Loss 6.2544e-01 (7.1085e-01)	Acc@1  76.46 ( 73.83)
The current update step is 1740
GPU_0_using curriculum 20 with window 20
Epoch: [58][20/30]	Time  1.541 ( 1.568)	Data  0.042 ( 0.063)	InnerLoop  0.646 ( 0.658)	Loss 7.4060e-01 (7.3951e-01)	Acc@1  71.68 ( 72.92)
The current update step is 1770
GPU_0_using curriculum 20 with window 20
Epoch: [59][20/30]	Time  1.554 ( 1.575)	Data  0.045 ( 0.051)	InnerLoop  0.652 ( 0.675)	Loss 7.2000e-01 (6.6539e-01)	Acc@1  70.58 ( 75.41)
The current update step is 1800
The current seed is 10250314877572185392
The current lr is: 0.001
Testing Results:
 *   Acc@1 74.211
 *   Acc@1 75.014
 *   Acc@1 75.171
 *   Acc@1 75.491
 *   Acc@1 75.329
 *   Acc@1 75.904
 *   Acc@1 72.842
 *   Acc@1 73.286
 *   Acc@1 72.789
 *   Acc@1 72.593
 *   Acc@1 71.882
 *   Acc@1 72.053
Training for 300 epoch: 73.52631578947368
Training for 600 epoch: 73.98026315789474
Training for 1000 epoch: 73.60526315789474
Training for 300 epoch: 74.15
Training for 600 epoch: 74.04208333333332
Training for 1000 epoch: 73.97874999999999
[[73.52631578947368, 73.98026315789474, 73.60526315789474], [74.15, 74.04208333333332, 73.97874999999999]]
train loss 0.42168826382954916, epoch 59, best loss 0.34027470251719155, best_epoch 49
GPU_0_using curriculum 20 with window 20
Epoch: [60][20/30]	Time  1.692 ( 1.581)	Data  0.173 ( 0.075)	InnerLoop  0.657 ( 0.653)	Loss 8.3142e-01 (6.9795e-01)	Acc@1  72.05 ( 74.78)
The current update step is 1830
GPU_0_using curriculum 20 with window 20
Epoch: [61][20/30]	Time  1.549 ( 1.576)	Data  0.042 ( 0.063)	InnerLoop  0.657 ( 0.663)	Loss 6.6731e-01 (7.3258e-01)	Acc@1  76.05 ( 73.27)
The current update step is 1860
GPU_0_using curriculum 20 with window 20
Epoch: [62][20/30]	Time  1.541 ( 1.576)	Data  0.044 ( 0.077)	InnerLoop  0.648 ( 0.647)	Loss 6.7810e-01 (7.9923e-01)	Acc@1  75.83 ( 71.04)
The current update step is 1890
GPU_0_using curriculum 20 with window 20
Epoch: [63][20/30]	Time  1.538 ( 1.577)	Data  0.041 ( 0.063)	InnerLoop  0.645 ( 0.661)	Loss 7.1233e-01 (6.8368e-01)	Acc@1  75.22 ( 74.98)
The current update step is 1920
GPU_0_using curriculum 20 with window 20
Epoch: [64][20/30]	Time  1.678 ( 1.584)	Data  0.175 ( 0.077)	InnerLoop  0.649 ( 0.655)	Loss 8.0417e-01 (7.5677e-01)	Acc@1  69.58 ( 73.89)
The current update step is 1950
The current seed is 18380926295856944261
The current lr is: 0.001
Testing Results:
 *   Acc@1 71.342
 *   Acc@1 71.522
 *   Acc@1 66.776
 *   Acc@1 66.418
 *   Acc@1 64.211
 *   Acc@1 64.119
 *   Acc@1 79.461
 *   Acc@1 79.746
 *   Acc@1 78.763
 *   Acc@1 79.334
 *   Acc@1 78.145
 *   Acc@1 78.399
Training for 300 epoch: 75.40131578947368
Training for 600 epoch: 72.76973684210526
Training for 1000 epoch: 71.17763157894737
Training for 300 epoch: 75.63416666666666
Training for 600 epoch: 72.87625
Training for 1000 epoch: 71.25916666666667
[[75.40131578947368, 72.76973684210526, 71.17763157894737], [75.63416666666666, 72.87625, 71.25916666666667]]
train loss 0.30585133204460146, epoch 64, best loss 0.30585133204460146, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [65][20/30]	Time  1.553 ( 1.571)	Data  0.045 ( 0.056)	InnerLoop  0.650 ( 0.665)	Loss 9.7630e-01 (7.4159e-01)	Acc@1  61.99 ( 72.76)
The current update step is 1980
GPU_0_using curriculum 20 with window 20
Epoch: [66][20/30]	Time  1.661 ( 1.575)	Data  0.174 ( 0.077)	InnerLoop  0.642 ( 0.649)	Loss 1.4201e+00 (8.6437e-01)	Acc@1  57.67 ( 68.74)
The current update step is 2010
GPU_0_using curriculum 20 with window 20
Epoch: [67][20/30]	Time  1.555 ( 1.570)	Data  0.045 ( 0.063)	InnerLoop  0.651 ( 0.658)	Loss 5.3299e-01 (7.7509e-01)	Acc@1  81.05 ( 71.33)
The current update step is 2040
GPU_0_using curriculum 20 with window 20
Epoch: [68][20/30]	Time  1.531 ( 1.570)	Data  0.043 ( 0.050)	InnerLoop  0.642 ( 0.672)	Loss 5.8842e-01 (8.2123e-01)	Acc@1  78.64 ( 70.17)
The current update step is 2070
GPU_0_using curriculum 20 with window 20
Epoch: [69][20/30]	Time  1.540 ( 1.567)	Data  0.045 ( 0.056)	InnerLoop  0.644 ( 0.663)	Loss 6.5750e-01 (7.5441e-01)	Acc@1  73.95 ( 72.22)
The current update step is 2100
The current seed is 14852699957005224229
The current lr is: 0.001
Testing Results:
 *   Acc@1 76.342
 *   Acc@1 76.451
 *   Acc@1 75.395
 *   Acc@1 76.051
 *   Acc@1 74.763
 *   Acc@1 75.291
 *   Acc@1 76.395
 *   Acc@1 76.974
 *   Acc@1 76.632
 *   Acc@1 76.582
 *   Acc@1 76.184
 *   Acc@1 76.262
Training for 300 epoch: 76.36842105263158
Training for 600 epoch: 76.01315789473685
Training for 1000 epoch: 75.47368421052632
Training for 300 epoch: 76.7125
Training for 600 epoch: 76.31666666666666
Training for 1000 epoch: 75.77625
[[76.36842105263158, 76.01315789473685, 75.47368421052632], [76.7125, 76.31666666666666, 75.77625]]
train loss 0.33691899110476176, epoch 69, best loss 0.30585133204460146, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [70][20/30]	Time  1.536 ( 1.567)	Data  0.041 ( 0.063)	InnerLoop  0.640 ( 0.656)	Loss 7.8331e-01 (7.3886e-01)	Acc@1  69.34 ( 72.69)
The current update step is 2130
GPU_0_using curriculum 20 with window 20
Epoch: [71][20/30]	Time  1.534 ( 1.565)	Data  0.045 ( 0.070)	InnerLoop  0.644 ( 0.649)	Loss 6.0920e-01 (7.1574e-01)	Acc@1  77.93 ( 74.11)
The current update step is 2160
GPU_0_using curriculum 20 with window 20
Epoch: [72][20/30]	Time  1.542 ( 1.567)	Data  0.042 ( 0.063)	InnerLoop  0.648 ( 0.656)	Loss 8.0633e-01 (7.3363e-01)	Acc@1  73.46 ( 72.16)
The current update step is 2190
GPU_0_using curriculum 20 with window 20
Epoch: [73][20/30]	Time  1.537 ( 1.569)	Data  0.041 ( 0.070)	InnerLoop  0.644 ( 0.651)	Loss 9.3256e-01 (7.3695e-01)	Acc@1  64.84 ( 72.86)
The current update step is 2220
GPU_0_using curriculum 20 with window 20
Epoch: [74][20/30]	Time  1.670 ( 1.576)	Data  0.178 ( 0.070)	InnerLoop  0.644 ( 0.657)	Loss 5.6837e-01 (7.4723e-01)	Acc@1  78.91 ( 71.65)
The current update step is 2250
The current seed is 15695487058623590174
The current lr is: 0.001
Testing Results:
 *   Acc@1 75.263
 *   Acc@1 75.380
 *   Acc@1 75.592
 *   Acc@1 75.877
 *   Acc@1 75.711
 *   Acc@1 75.907
 *   Acc@1 74.368
 *   Acc@1 74.583
 *   Acc@1 70.487
 *   Acc@1 70.718
 *   Acc@1 68.947
 *   Acc@1 68.877
Training for 300 epoch: 74.8157894736842
Training for 600 epoch: 73.03947368421052
Training for 1000 epoch: 72.32894736842105
Training for 300 epoch: 74.98166666666665
Training for 600 epoch: 73.2975
Training for 1000 epoch: 72.3925
[[74.8157894736842, 73.03947368421052, 72.32894736842105], [74.98166666666665, 73.2975, 72.3925]]
train loss 0.44648781224886575, epoch 74, best loss 0.30585133204460146, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [75][20/30]	Time  1.686 ( 1.583)	Data  0.042 ( 0.056)	InnerLoop  0.785 ( 0.675)	Loss 6.3284e-01 (6.9718e-01)	Acc@1  74.73 ( 75.69)
The current update step is 2280
GPU_0_using curriculum 20 with window 20
Epoch: [76][20/30]	Time  1.546 ( 1.572)	Data  0.045 ( 0.057)	InnerLoop  0.640 ( 0.663)	Loss 6.7682e-01 (7.7326e-01)	Acc@1  75.42 ( 72.86)
The current update step is 2310
GPU_0_using curriculum 20 with window 20
Epoch: [77][20/30]	Time  1.543 ( 1.577)	Data  0.045 ( 0.064)	InnerLoop  0.642 ( 0.661)	Loss 7.0672e-01 (7.1981e-01)	Acc@1  74.90 ( 72.82)
The current update step is 2340
GPU_0_using curriculum 20 with window 20
Epoch: [78][20/30]	Time  1.545 ( 1.578)	Data  0.045 ( 0.050)	InnerLoop  0.648 ( 0.674)	Loss 5.7990e-01 (7.0485e-01)	Acc@1  80.49 ( 74.23)
The current update step is 2370
GPU_0_using curriculum 20 with window 20
Epoch: [79][20/30]	Time  1.482 ( 1.571)	Data  0.039 ( 0.056)	InnerLoop  0.605 ( 0.664)	Loss 5.5206e-01 (6.6682e-01)	Acc@1  79.93 ( 75.58)
The current update step is 2400
The current seed is 3758531541331944497
The current lr is: 0.001
Testing Results:
 *   Acc@1 79.382
 *   Acc@1 80.126
 *   Acc@1 79.500
 *   Acc@1 79.943
 *   Acc@1 78.868
 *   Acc@1 79.233
 *   Acc@1 81.184
 *   Acc@1 80.989
 *   Acc@1 81.026
 *   Acc@1 81.127
 *   Acc@1 80.632
 *   Acc@1 80.897
Training for 300 epoch: 80.28289473684211
Training for 600 epoch: 80.26315789473685
Training for 1000 epoch: 79.75
Training for 300 epoch: 80.5575
Training for 600 epoch: 80.53541666666666
Training for 1000 epoch: 80.06458333333333
[[80.28289473684211, 80.26315789473685, 79.75], [80.5575, 80.53541666666666, 80.06458333333333]]
train loss 0.278261639769872, epoch 79, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [80][20/30]	Time  1.549 ( 1.571)	Data  0.043 ( 0.063)	InnerLoop  0.656 ( 0.659)	Loss 5.4470e-01 (6.8609e-01)	Acc@1  80.10 ( 75.36)
The current update step is 2430
GPU_0_using curriculum 20 with window 20
Epoch: [81][20/30]	Time  1.550 ( 1.576)	Data  0.042 ( 0.070)	InnerLoop  0.644 ( 0.652)	Loss 8.2026e-01 (6.8581e-01)	Acc@1  68.68 ( 74.61)
The current update step is 2460
GPU_0_using curriculum 20 with window 20
Epoch: [82][20/30]	Time  1.558 ( 1.568)	Data  0.042 ( 0.062)	InnerLoop  0.662 ( 0.656)	Loss 5.7107e-01 (7.3491e-01)	Acc@1  79.74 ( 73.90)
The current update step is 2490
GPU_0_using curriculum 20 with window 20
Epoch: [83][20/30]	Time  1.541 ( 1.573)	Data  0.041 ( 0.070)	InnerLoop  0.650 ( 0.653)	Loss 7.4174e-01 (6.6159e-01)	Acc@1  71.85 ( 75.68)
The current update step is 2520
GPU_0_using curriculum 20 with window 20
Epoch: [84][20/30]	Time  1.665 ( 1.573)	Data  0.175 ( 0.069)	InnerLoop  0.639 ( 0.654)	Loss 5.7742e-01 (6.5990e-01)	Acc@1  78.12 ( 75.48)
The current update step is 2550
The current seed is 8613328529077883740
The current lr is: 0.001
Testing Results:
 *   Acc@1 76.684
 *   Acc@1 76.797
 *   Acc@1 75.711
 *   Acc@1 76.047
 *   Acc@1 75.868
 *   Acc@1 76.017
 *   Acc@1 64.947
 *   Acc@1 65.436
 *   Acc@1 70.750
 *   Acc@1 71.067
 *   Acc@1 73.145
 *   Acc@1 73.311
Training for 300 epoch: 70.81578947368422
Training for 600 epoch: 73.23026315789474
Training for 1000 epoch: 74.50657894736841
Training for 300 epoch: 71.11625000000001
Training for 600 epoch: 73.5575
Training for 1000 epoch: 74.66416666666666
[[70.81578947368422, 73.23026315789474, 74.50657894736841], [71.11625000000001, 73.5575, 74.66416666666666]]
train loss 0.3701360416094462, epoch 84, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [85][20/30]	Time  1.688 ( 1.575)	Data  0.040 ( 0.055)	InnerLoop  0.787 ( 0.670)	Loss 5.9850e-01 (7.6832e-01)	Acc@1  76.25 ( 72.35)
The current update step is 2580
GPU_0_using curriculum 20 with window 20
Epoch: [86][20/30]	Time  1.552 ( 1.570)	Data  0.045 ( 0.056)	InnerLoop  0.643 ( 0.663)	Loss 1.0217e+00 (7.0971e-01)	Acc@1  58.06 ( 73.85)
The current update step is 2610
GPU_0_using curriculum 20 with window 20
Epoch: [87][20/30]	Time  1.537 ( 1.565)	Data  0.042 ( 0.061)	InnerLoop  0.637 ( 0.653)	Loss 5.5039e-01 (6.6879e-01)	Acc@1  80.54 ( 75.68)
The current update step is 2640
GPU_0_using curriculum 20 with window 20
Epoch: [88][20/30]	Time  1.530 ( 1.563)	Data  0.041 ( 0.048)	InnerLoop  0.638 ( 0.666)	Loss 7.9304e-01 (8.1904e-01)	Acc@1  68.02 ( 71.40)
The current update step is 2670
GPU_0_using curriculum 20 with window 20
Epoch: [89][20/30]	Time  1.527 ( 1.567)	Data  0.044 ( 0.055)	InnerLoop  0.637 ( 0.660)	Loss 8.2418e-01 (6.6731e-01)	Acc@1  71.04 ( 75.65)
The current update step is 2700
The current seed is 1192965532798441009
The current lr is: 0.001
Testing Results:
 *   Acc@1 67.895
 *   Acc@1 68.108
 *   Acc@1 65.487
 *   Acc@1 66.112
 *   Acc@1 65.842
 *   Acc@1 66.033
 *   Acc@1 74.737
 *   Acc@1 74.743
 *   Acc@1 74.447
 *   Acc@1 74.373
 *   Acc@1 75.342
 *   Acc@1 75.522
Training for 300 epoch: 71.31578947368422
Training for 600 epoch: 69.96710526315789
Training for 1000 epoch: 70.59210526315789
Training for 300 epoch: 71.42541666666668
Training for 600 epoch: 70.2425
Training for 1000 epoch: 70.7775
[[71.31578947368422, 69.96710526315789, 70.59210526315789], [71.42541666666668, 70.2425, 70.7775]]
train loss 0.3285042068481445, epoch 89, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [90][20/30]	Time  1.526 ( 1.563)	Data  0.042 ( 0.055)	InnerLoop  0.637 ( 0.659)	Loss 6.6849e-01 (6.7737e-01)	Acc@1  73.93 ( 74.25)
The current update step is 2730
GPU_0_using curriculum 20 with window 20
Epoch: [91][20/30]	Time  1.532 ( 1.562)	Data  0.042 ( 0.074)	InnerLoop  0.636 ( 0.639)	Loss 7.6726e-01 (7.2183e-01)	Acc@1  72.85 ( 74.16)
The current update step is 2760
GPU_0_using curriculum 20 with window 20
Epoch: [92][20/30]	Time  1.548 ( 1.572)	Data  0.044 ( 0.075)	InnerLoop  0.645 ( 0.646)	Loss 6.8363e-01 (6.6815e-01)	Acc@1  75.61 ( 75.97)
The current update step is 2790
GPU_0_using curriculum 20 with window 20
Epoch: [93][20/30]	Time  1.668 ( 1.570)	Data  0.043 ( 0.074)	InnerLoop  0.773 ( 0.646)	Loss 1.0108e+00 (7.8030e-01)	Acc@1  59.06 ( 70.20)
The current update step is 2820
GPU_0_using curriculum 20 with window 20
Epoch: [94][20/30]	Time  1.528 ( 1.564)	Data  0.041 ( 0.061)	InnerLoop  0.637 ( 0.654)	Loss 8.9930e-01 (6.8287e-01)	Acc@1  62.84 ( 74.64)
The current update step is 2850
The current seed is 11074057635176222282
The current lr is: 0.001
Testing Results:
 *   Acc@1 71.118
 *   Acc@1 71.086
 *   Acc@1 69.934
 *   Acc@1 69.729
 *   Acc@1 69.263
 *   Acc@1 68.823
 *   Acc@1 76.711
 *   Acc@1 76.888
 *   Acc@1 74.513
 *   Acc@1 74.836
 *   Acc@1 72.645
 *   Acc@1 73.294
Training for 300 epoch: 73.91447368421052
Training for 600 epoch: 72.22368421052632
Training for 1000 epoch: 70.95394736842104
Training for 300 epoch: 73.98666666666666
Training for 600 epoch: 72.2825
Training for 1000 epoch: 71.05833333333334
[[73.91447368421052, 72.22368421052632, 70.95394736842104], [73.98666666666666, 72.2825, 71.05833333333334]]
train loss 0.37705805271466575, epoch 94, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [95][20/30]	Time  1.542 ( 1.573)	Data  0.042 ( 0.061)	InnerLoop  0.641 ( 0.661)	Loss 6.6455e-01 (7.5041e-01)	Acc@1  73.02 ( 72.54)
The current update step is 2880
GPU_0_using curriculum 20 with window 20
Epoch: [96][20/30]	Time  1.673 ( 1.573)	Data  0.041 ( 0.061)	InnerLoop  0.769 ( 0.662)	Loss 7.1106e-01 (7.4613e-01)	Acc@1  71.63 ( 72.89)
The current update step is 2910
GPU_0_using curriculum 20 with window 20
Epoch: [97][20/30]	Time  1.538 ( 1.566)	Data  0.042 ( 0.055)	InnerLoop  0.649 ( 0.661)	Loss 5.7651e-01 (6.6724e-01)	Acc@1  81.30 ( 75.06)
The current update step is 2940
GPU_0_using curriculum 20 with window 20
Epoch: [98][20/30]	Time  1.550 ( 1.560)	Data  0.043 ( 0.062)	InnerLoop  0.664 ( 0.654)	Loss 9.5828e-01 (7.5088e-01)	Acc@1  63.18 ( 72.17)
The current update step is 2970
GPU_0_using curriculum 20 with window 20
Epoch: [99][20/30]	Time  1.529 ( 1.564)	Data  0.043 ( 0.049)	InnerLoop  0.638 ( 0.667)	Loss 9.7100e-01 (7.7124e-01)	Acc@1  62.62 ( 71.35)
The current update step is 3000
The current seed is 8376341304542558369
The current lr is: 0.001
Testing Results:
 *   Acc@1 75.539
 *   Acc@1 75.573
 *   Acc@1 74.711
 *   Acc@1 74.507
 *   Acc@1 73.947
 *   Acc@1 74.159
 *   Acc@1 72.987
 *   Acc@1 73.277
 *   Acc@1 72.526
 *   Acc@1 72.472
 *   Acc@1 73.118
 *   Acc@1 73.036
Training for 300 epoch: 74.26315789473685
Training for 600 epoch: 73.61842105263159
Training for 1000 epoch: 73.53289473684211
Training for 300 epoch: 74.42458333333335
Training for 600 epoch: 73.48916666666666
Training for 1000 epoch: 73.5975
[[74.26315789473685, 73.61842105263159, 73.53289473684211], [74.42458333333335, 73.48916666666666, 73.5975]]
train loss 0.3568415429592133, epoch 99, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [100][20/30]	Time  1.662 ( 1.571)	Data  0.168 ( 0.074)	InnerLoop  0.637 ( 0.647)	Loss 8.1815e-01 (7.8450e-01)	Acc@1  70.04 ( 71.51)
The current update step is 3030
GPU_0_using curriculum 20 with window 20
Epoch: [101][20/30]	Time  1.534 ( 1.561)	Data  0.044 ( 0.062)	InnerLoop  0.636 ( 0.651)	Loss 6.0072e-01 (8.6394e-01)	Acc@1  79.13 ( 68.70)
The current update step is 3060
GPU_0_using curriculum 20 with window 20
Epoch: [102][20/30]	Time  1.553 ( 1.561)	Data  0.044 ( 0.074)	InnerLoop  0.642 ( 0.639)	Loss 7.6348e-01 (7.3378e-01)	Acc@1  69.82 ( 73.57)
The current update step is 3090
GPU_0_using curriculum 20 with window 20
Epoch: [103][20/30]	Time  1.528 ( 1.561)	Data  0.043 ( 0.062)	InnerLoop  0.633 ( 0.652)	Loss 6.7773e-01 (7.3174e-01)	Acc@1  75.54 ( 73.10)
The current update step is 3120
GPU_0_using curriculum 20 with window 20
Epoch: [104][20/30]	Time  1.658 ( 1.567)	Data  0.168 ( 0.074)	InnerLoop  0.638 ( 0.646)	Loss 1.2405e+00 (7.7491e-01)	Acc@1  60.50 ( 71.83)
The current update step is 3150
The current seed is 16912039975266596613
The current lr is: 0.001
Testing Results:
 *   Acc@1 70.263
 *   Acc@1 71.078
 *   Acc@1 69.987
 *   Acc@1 70.450
 *   Acc@1 69.539
 *   Acc@1 70.713
 *   Acc@1 71.447
 *   Acc@1 71.584
 *   Acc@1 72.553
 *   Acc@1 72.502
 *   Acc@1 72.553
 *   Acc@1 72.849
Training for 300 epoch: 70.85526315789474
Training for 600 epoch: 71.26973684210526
Training for 1000 epoch: 71.04605263157895
Training for 300 epoch: 71.33125
Training for 600 epoch: 71.47583333333333
Training for 1000 epoch: 71.78125
[[70.85526315789474, 71.26973684210526, 71.04605263157895], [71.33125, 71.47583333333333, 71.78125]]
train loss 0.3609002498467763, epoch 104, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [105][20/30]	Time  1.535 ( 1.562)	Data  0.045 ( 0.055)	InnerLoop  0.639 ( 0.659)	Loss 9.2883e-01 (8.9044e-01)	Acc@1  67.68 ( 68.19)
The current update step is 3180
GPU_0_using curriculum 20 with window 20
Epoch: [106][20/30]	Time  1.655 ( 1.567)	Data  0.169 ( 0.075)	InnerLoop  0.638 ( 0.647)	Loss 7.1789e-01 (7.6978e-01)	Acc@1  70.90 ( 71.20)
The current update step is 3210
GPU_0_using curriculum 20 with window 20
Epoch: [107][20/30]	Time  1.527 ( 1.557)	Data  0.042 ( 0.061)	InnerLoop  0.638 ( 0.653)	Loss 8.3874e-01 (7.6084e-01)	Acc@1  67.63 ( 72.54)
The current update step is 3240
GPU_0_using curriculum 20 with window 20
Epoch: [108][20/30]	Time  1.539 ( 1.565)	Data  0.042 ( 0.049)	InnerLoop  0.641 ( 0.666)	Loss 8.3758e-01 (7.5772e-01)	Acc@1  67.60 ( 72.14)
The current update step is 3270
GPU_0_using curriculum 20 with window 20
Epoch: [109][20/30]	Time  1.547 ( 1.560)	Data  0.045 ( 0.075)	InnerLoop  0.638 ( 0.639)	Loss 5.9521e-01 (7.9796e-01)	Acc@1  78.93 ( 70.76)
The current update step is 3300
The current seed is 3497287548946689994
The current lr is: 0.001
Testing Results:
 *   Acc@1 77.276
 *   Acc@1 77.667
 *   Acc@1 76.434
 *   Acc@1 76.876
 *   Acc@1 75.882
 *   Acc@1 76.021
 *   Acc@1 73.974
 *   Acc@1 74.154
 *   Acc@1 71.105
 *   Acc@1 71.297
 *   Acc@1 68.684
 *   Acc@1 69.143
Training for 300 epoch: 75.625
Training for 600 epoch: 73.76973684210526
Training for 1000 epoch: 72.28289473684211
Training for 300 epoch: 75.91041666666666
Training for 600 epoch: 74.08625
Training for 1000 epoch: 72.58208333333333
[[75.625, 73.76973684210526, 72.28289473684211], [75.91041666666666, 74.08625, 72.58208333333333]]
train loss 0.4141706109682719, epoch 109, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [110][20/30]	Time  1.677 ( 1.572)	Data  0.173 ( 0.074)	InnerLoop  0.656 ( 0.649)	Loss 5.2629e-01 (7.6211e-01)	Acc@1  80.93 ( 71.57)
The current update step is 3330
GPU_0_using curriculum 20 with window 20
Epoch: [111][20/30]	Time  1.538 ( 1.566)	Data  0.045 ( 0.062)	InnerLoop  0.639 ( 0.655)	Loss 6.0712e-01 (7.1051e-01)	Acc@1  78.42 ( 73.06)
The current update step is 3360
GPU_0_using curriculum 20 with window 20
Epoch: [112][20/30]	Time  1.513 ( 1.552)	Data  0.043 ( 0.073)	InnerLoop  0.627 ( 0.634)	Loss 6.1465e-01 (6.4825e-01)	Acc@1  77.95 ( 75.99)
The current update step is 3390
GPU_0_using curriculum 20 with window 20
Epoch: [113][20/30]	Time  1.517 ( 1.556)	Data  0.044 ( 0.061)	InnerLoop  0.632 ( 0.649)	Loss 5.2199e-01 (6.8054e-01)	Acc@1  81.40 ( 74.37)
The current update step is 3420
GPU_0_using curriculum 20 with window 20
Epoch: [114][20/30]	Time  1.642 ( 1.564)	Data  0.169 ( 0.074)	InnerLoop  0.633 ( 0.645)	Loss 7.1943e-01 (6.9969e-01)	Acc@1  71.68 ( 74.89)
The current update step is 3450
The current seed is 5133706703919128559
The current lr is: 0.001
Testing Results:
 *   Acc@1 76.908
 *   Acc@1 77.035
 *   Acc@1 76.395
 *   Acc@1 76.570
 *   Acc@1 76.211
 *   Acc@1 76.347
 *   Acc@1 75.763
 *   Acc@1 75.778
 *   Acc@1 74.724
 *   Acc@1 75.427
 *   Acc@1 75.737
 *   Acc@1 75.622
Training for 300 epoch: 76.33552631578948
Training for 600 epoch: 75.55921052631578
Training for 1000 epoch: 75.97368421052633
Training for 300 epoch: 76.40666666666667
Training for 600 epoch: 75.99875
Training for 1000 epoch: 75.98416666666667
[[76.33552631578948, 75.55921052631578, 75.97368421052633], [76.40666666666667, 75.99875, 75.98416666666667]]
train loss 0.34071761485735574, epoch 114, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [115][20/30]	Time  1.546 ( 1.571)	Data  0.042 ( 0.055)	InnerLoop  0.647 ( 0.665)	Loss 6.5030e-01 (6.9712e-01)	Acc@1  75.85 ( 74.95)
The current update step is 3480
GPU_0_using curriculum 20 with window 20
Epoch: [116][20/30]	Time  1.684 ( 1.575)	Data  0.168 ( 0.074)	InnerLoop  0.637 ( 0.650)	Loss 5.8325e-01 (7.2440e-01)	Acc@1  78.69 ( 73.80)
The current update step is 3510
GPU_0_using curriculum 20 with window 20
Epoch: [117][20/30]	Time  1.551 ( 1.571)	Data  0.041 ( 0.062)	InnerLoop  0.646 ( 0.658)	Loss 8.0042e-01 (6.8947e-01)	Acc@1  72.53 ( 73.75)
The current update step is 3540
GPU_0_using curriculum 20 with window 20
Epoch: [118][20/30]	Time  1.558 ( 1.579)	Data  0.043 ( 0.051)	InnerLoop  0.650 ( 0.675)	Loss 9.8505e-01 (6.7999e-01)	Acc@1  66.26 ( 74.99)
The current update step is 3570
GPU_0_using curriculum 20 with window 20
Epoch: [119][20/30]	Time  1.551 ( 1.577)	Data  0.043 ( 0.075)	InnerLoop  0.655 ( 0.652)	Loss 5.9810e-01 (6.4311e-01)	Acc@1  77.20 ( 76.61)
The current update step is 3600
The current seed is 14272849845850056510
The current lr is: 0.001
Testing Results:
 *   Acc@1 75.711
 *   Acc@1 75.843
 *   Acc@1 75.974
 *   Acc@1 76.043
 *   Acc@1 74.974
 *   Acc@1 75.197
 *   Acc@1 78.184
 *   Acc@1 78.733
 *   Acc@1 78.329
 *   Acc@1 78.916
 *   Acc@1 78.276
 *   Acc@1 79.061
Training for 300 epoch: 76.94736842105263
Training for 600 epoch: 77.15131578947368
Training for 1000 epoch: 76.625
Training for 300 epoch: 77.28791666666666
Training for 600 epoch: 77.47958333333334
Training for 1000 epoch: 77.12875
[[76.94736842105263, 77.15131578947368, 76.625], [77.28791666666666, 77.47958333333334, 77.12875]]
train loss 0.2987071947892507, epoch 119, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [120][20/30]	Time  1.694 ( 1.582)	Data  0.172 ( 0.075)	InnerLoop  0.668 ( 0.658)	Loss 5.6948e-01 (6.5555e-01)	Acc@1  78.98 ( 76.10)
The current update step is 3630
GPU_0_using curriculum 20 with window 20
Epoch: [121][20/30]	Time  1.597 ( 1.579)	Data  0.046 ( 0.062)	InnerLoop  0.673 ( 0.664)	Loss 6.2219e-01 (6.8891e-01)	Acc@1  75.63 ( 74.93)
The current update step is 3660
GPU_0_using curriculum 20 with window 20
Epoch: [122][20/30]	Time  1.548 ( 1.571)	Data  0.043 ( 0.074)	InnerLoop  0.649 ( 0.647)	Loss 4.8268e-01 (6.3671e-01)	Acc@1  83.50 ( 76.80)
The current update step is 3690
GPU_0_using curriculum 20 with window 20
Epoch: [123][20/30]	Time  1.543 ( 1.575)	Data  0.043 ( 0.062)	InnerLoop  0.647 ( 0.662)	Loss 8.3019e-01 (6.7765e-01)	Acc@1  72.85 ( 75.09)
The current update step is 3720
GPU_0_using curriculum 20 with window 20
Epoch: [124][20/30]	Time  1.683 ( 1.583)	Data  0.170 ( 0.075)	InnerLoop  0.658 ( 0.656)	Loss 6.9499e-01 (6.1806e-01)	Acc@1  73.17 ( 77.55)
The current update step is 3750
The current seed is 2807829305795457615
The current lr is: 0.001
Testing Results:
 *   Acc@1 79.053
 *   Acc@1 78.933
 *   Acc@1 77.855
 *   Acc@1 78.282
 *   Acc@1 77.947
 *   Acc@1 77.729
 *   Acc@1 80.500
 *   Acc@1 80.621
 *   Acc@1 78.763
 *   Acc@1 79.189
 *   Acc@1 78.763
 *   Acc@1 78.870
Training for 300 epoch: 79.77631578947368
Training for 600 epoch: 78.30921052631578
Training for 1000 epoch: 78.35526315789474
Training for 300 epoch: 79.77666666666667
Training for 600 epoch: 78.73541666666667
Training for 1000 epoch: 78.29958333333335
[[79.77631578947368, 78.30921052631578, 78.35526315789474], [79.77666666666667, 78.73541666666667, 78.29958333333335]]
train loss 0.29620857140223184, epoch 124, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [125][20/30]	Time  1.552 ( 1.576)	Data  0.044 ( 0.055)	InnerLoop  0.650 ( 0.670)	Loss 5.2375e-01 (6.3919e-01)	Acc@1  80.69 ( 76.40)
The current update step is 3780
GPU_0_using curriculum 20 with window 20
Epoch: [126][20/30]	Time  1.687 ( 1.587)	Data  0.173 ( 0.075)	InnerLoop  0.656 ( 0.658)	Loss 6.8090e-01 (6.7818e-01)	Acc@1  74.54 ( 74.87)
The current update step is 3810
GPU_0_using curriculum 20 with window 20
Epoch: [127][20/30]	Time  1.540 ( 1.576)	Data  0.043 ( 0.062)	InnerLoop  0.642 ( 0.661)	Loss 6.5099e-01 (6.5389e-01)	Acc@1  76.17 ( 75.37)
The current update step is 3840
GPU_0_using curriculum 20 with window 20
Epoch: [128][20/30]	Time  1.551 ( 1.569)	Data  0.044 ( 0.049)	InnerLoop  0.649 ( 0.672)	Loss 6.0885e-01 (6.2468e-01)	Acc@1  78.15 ( 76.97)
The current update step is 3870
GPU_0_using curriculum 20 with window 20
Epoch: [129][20/30]	Time  1.549 ( 1.566)	Data  0.043 ( 0.075)	InnerLoop  0.648 ( 0.642)	Loss 9.7966e-01 (7.1151e-01)	Acc@1  66.48 ( 74.45)
The current update step is 3900
The current seed is 16516000299438678135
The current lr is: 0.001
Testing Results:
 *   Acc@1 77.500
 *   Acc@1 77.892
 *   Acc@1 79.000
 *   Acc@1 79.078
 *   Acc@1 78.987
 *   Acc@1 79.237
 *   Acc@1 79.513
 *   Acc@1 80.237
 *   Acc@1 79.145
 *   Acc@1 79.279
 *   Acc@1 78.434
 *   Acc@1 78.664
Training for 300 epoch: 78.50657894736841
Training for 600 epoch: 79.07236842105263
Training for 1000 epoch: 78.71052631578948
Training for 300 epoch: 79.06458333333333
Training for 600 epoch: 79.17875000000001
Training for 1000 epoch: 78.95083333333334
[[78.50657894736841, 79.07236842105263, 78.71052631578948], [79.06458333333333, 79.17875000000001, 78.95083333333334]]
train loss 0.2910744192123413, epoch 129, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [130][20/30]	Time  1.674 ( 1.582)	Data  0.045 ( 0.075)	InnerLoop  0.778 ( 0.654)	Loss 7.8765e-01 (6.4804e-01)	Acc@1  71.34 ( 76.06)
The current update step is 3930
GPU_0_using curriculum 20 with window 20
Epoch: [131][20/30]	Time  1.572 ( 1.571)	Data  0.045 ( 0.062)	InnerLoop  0.650 ( 0.659)	Loss 5.0745e-01 (6.2149e-01)	Acc@1  81.42 ( 76.69)
The current update step is 3960
GPU_0_using curriculum 20 with window 20
Epoch: [132][20/30]	Time  1.546 ( 1.569)	Data  0.045 ( 0.049)	InnerLoop  0.649 ( 0.671)	Loss 6.1936e-01 (6.4711e-01)	Acc@1  77.49 ( 76.07)
The current update step is 3990
GPU_0_using curriculum 20 with window 20
Epoch: [133][20/30]	Time  1.541 ( 1.572)	Data  0.052 ( 0.056)	InnerLoop  0.640 ( 0.664)	Loss 6.5572e-01 (6.8327e-01)	Acc@1  73.85 ( 74.50)
The current update step is 4020
GPU_0_using curriculum 20 with window 20
Epoch: [134][20/30]	Time  1.661 ( 1.578)	Data  0.171 ( 0.075)	InnerLoop  0.642 ( 0.649)	Loss 9.1344e-01 (6.8904e-01)	Acc@1  63.21 ( 73.88)
The current update step is 4050
The current seed is 15763309134469640862
The current lr is: 0.001
Testing Results:
 *   Acc@1 73.816
 *   Acc@1 74.364
 *   Acc@1 74.421
 *   Acc@1 74.762
 *   Acc@1 74.092
 *   Acc@1 74.672
 *   Acc@1 77.987
 *   Acc@1 78.168
 *   Acc@1 75.750
 *   Acc@1 75.881
 *   Acc@1 74.776
 *   Acc@1 75.029
Training for 300 epoch: 75.90131578947368
Training for 600 epoch: 75.08552631578948
Training for 1000 epoch: 74.43421052631578
Training for 300 epoch: 76.26625
Training for 600 epoch: 75.32124999999999
Training for 1000 epoch: 74.85041666666666
[[75.90131578947368, 75.08552631578948, 74.43421052631578], [76.26625, 75.32124999999999, 74.85041666666666]]
train loss 0.3504738343874613, epoch 134, best loss 0.278261639769872, best_epoch 79
GPU_0_using curriculum 20 with window 20
Epoch: [135][20/30]	Time  1.538 ( 1.571)	Data  0.043 ( 0.056)	InnerLoop  0.647 ( 0.669)	Loss 6.4270e-01 (6.8947e-01)	Acc@1  75.49 ( 75.47)
The current update step is 4080
GPU_0_using curriculum 20 with window 20
Epoch: [136][20/30]	Time  1.682 ( 1.564)	Data  0.178 ( 0.074)	InnerLoop  0.654 ( 0.646)	Loss 5.5807e-01 (6.4787e-01)	Acc@1  80.37 ( 76.10)
The current update step is 4110
GPU_0_using curriculum 20 with window 20
Epoch: [137][20/30]	Time  1.525 ( 1.560)	Data  0.040 ( 0.061)	InnerLoop  0.642 ( 0.654)	Loss 5.6164e-01 (6.7099e-01)	Acc@1  78.96 ( 74.97)
The current update step is 4140
GPU_0_using curriculum 20 with window 20
Epoch: [138][20/30]	Time  1.555 ( 1.564)	Data  0.041 ( 0.048)	InnerLoop  0.641 ( 0.669)	Loss 9.8671e-01 (6.8715e-01)	Acc@1  64.33 ( 74.83)
The current update step is 4170
GPU_0_using curriculum 20 with window 20
Epoch: [139][20/30]	Time  1.565 ( 1.569)	Data  0.042 ( 0.075)	InnerLoop  0.648 ( 0.646)	Loss 5.3893e-01 (6.7063e-01)	Acc@1  80.93 ( 75.29)
The current update step is 4200
The current seed is 15217381182203285816
The current lr is: 0.001
Testing Results:
 *   Acc@1 75.605
 *   Acc@1 75.497
 *   Acc@1 76.026
 *   Acc@1 75.691
 *   Acc@1 75.750
 *   Acc@1 75.242
 *   Acc@1 79.671
 *   Acc@1 79.745
 *   Acc@1 78.289
 *   Acc@1 78.790
 *   Acc@1 77.053
 *   Acc@1 77.757
Training for 300 epoch: 77.63815789473685
Training for 600 epoch: 77.15789473684211
Training for 1000 epoch: 76.40131578947368
Training for 300 epoch: 77.62083333333334
Training for 600 epoch: 77.24041666666668
Training for 1000 epoch: 76.49958333333333
[[77.63815789473685, 77.15789473684211, 76.40131578947368], [77.62083333333334, 77.24041666666668, 76.49958333333333]]
train loss 0.30683646427790323, epoch 139, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [140][20/30]	Time  1.664 ( 1.574)	Data  0.174 ( 0.076)	InnerLoop  0.643 ( 0.649)	Loss 9.5659e-01 (6.7614e-01)	Acc@1  64.84 ( 74.51)
The current update step is 4230
GPU_0_using curriculum 20 with window 20
Epoch: [141][20/30]	Time  1.531 ( 1.567)	Data  0.045 ( 0.062)	InnerLoop  0.644 ( 0.658)	Loss 5.4337e-01 (6.6694e-01)	Acc@1  80.32 ( 75.27)
The current update step is 4260
GPU_0_using curriculum 20 with window 20
Epoch: [142][20/30]	Time  1.530 ( 1.567)	Data  0.043 ( 0.075)	InnerLoop  0.641 ( 0.643)	Loss 7.3984e-01 (6.6055e-01)	Acc@1  74.22 ( 75.36)
The current update step is 4290
GPU_0_using curriculum 20 with window 20
Epoch: [143][20/30]	Time  1.541 ( 1.565)	Data  0.042 ( 0.061)	InnerLoop  0.638 ( 0.650)	Loss 5.2775e-01 (6.1053e-01)	Acc@1  81.49 ( 77.55)
The current update step is 4320
GPU_0_using curriculum 20 with window 20
Epoch: [144][20/30]	Time  1.699 ( 1.572)	Data  0.164 ( 0.074)	InnerLoop  0.637 ( 0.646)	Loss 6.0475e-01 (6.6350e-01)	Acc@1  77.93 ( 76.23)
The current update step is 4350
The current seed is 4395502568164731535
The current lr is: 0.001
Testing Results:
 *   Acc@1 72.895
 *   Acc@1 72.991
 *   Acc@1 70.066
 *   Acc@1 70.153
 *   Acc@1 69.434
 *   Acc@1 69.286
 *   Acc@1 76.750
 *   Acc@1 77.425
 *   Acc@1 77.039
 *   Acc@1 77.396
 *   Acc@1 76.763
 *   Acc@1 76.949
Training for 300 epoch: 74.82236842105263
Training for 600 epoch: 73.55263157894737
Training for 1000 epoch: 73.09868421052632
Training for 300 epoch: 75.20791666666666
Training for 600 epoch: 73.77416666666667
Training for 1000 epoch: 73.1175
[[74.82236842105263, 73.55263157894737, 73.09868421052632], [75.20791666666666, 73.77416666666667, 73.1175]]
train loss 0.3094406940619151, epoch 144, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [145][20/30]	Time  1.540 ( 1.570)	Data  0.041 ( 0.055)	InnerLoop  0.639 ( 0.663)	Loss 5.4246e-01 (5.9462e-01)	Acc@1  80.37 ( 78.21)
The current update step is 4380
GPU_0_using curriculum 20 with window 20
Epoch: [146][20/30]	Time  1.668 ( 1.580)	Data  0.169 ( 0.075)	InnerLoop  0.648 ( 0.653)	Loss 5.7227e-01 (6.1341e-01)	Acc@1  79.83 ( 77.55)
The current update step is 4410
GPU_0_using curriculum 20 with window 20
Epoch: [147][20/30]	Time  1.546 ( 1.575)	Data  0.047 ( 0.063)	InnerLoop  0.646 ( 0.661)	Loss 6.1059e-01 (6.2174e-01)	Acc@1  76.98 ( 77.06)
The current update step is 4440
GPU_0_using curriculum 20 with window 20
Epoch: [148][20/30]	Time  1.562 ( 1.578)	Data  0.050 ( 0.050)	InnerLoop  0.658 ( 0.674)	Loss 5.7503e-01 (7.4937e-01)	Acc@1  77.66 ( 72.95)
The current update step is 4470
GPU_0_using curriculum 20 with window 20
Epoch: [149][20/30]	Time  1.547 ( 1.573)	Data  0.043 ( 0.075)	InnerLoop  0.647 ( 0.645)	Loss 7.9765e-01 (6.7701e-01)	Acc@1  71.80 ( 75.83)
The current update step is 4500
The current seed is 11295168277223242493
The current lr is: 0.001
Testing Results:
 *   Acc@1 77.816
 *   Acc@1 78.237
 *   Acc@1 76.316
 *   Acc@1 76.892
 *   Acc@1 74.079
 *   Acc@1 74.539
 *   Acc@1 80.329
 *   Acc@1 80.341
 *   Acc@1 80.105
 *   Acc@1 80.187
 *   Acc@1 79.053
 *   Acc@1 79.003
Training for 300 epoch: 79.07236842105263
Training for 600 epoch: 78.21052631578948
Training for 1000 epoch: 76.56578947368422
Training for 300 epoch: 79.28916666666666
Training for 600 epoch: 78.53916666666666
Training for 1000 epoch: 76.77125000000001
[[79.07236842105263, 78.21052631578948, 76.56578947368422], [79.28916666666666, 78.53916666666666, 76.77125000000001]]
train loss 0.302521670627594, epoch 149, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [150][20/30]	Time  1.671 ( 1.584)	Data  0.044 ( 0.074)	InnerLoop  0.777 ( 0.656)	Loss 5.0524e-01 (6.9628e-01)	Acc@1  82.59 ( 74.80)
The current update step is 4530
GPU_0_using curriculum 20 with window 20
Epoch: [151][20/30]	Time  1.538 ( 1.571)	Data  0.044 ( 0.062)	InnerLoop  0.637 ( 0.653)	Loss 5.3877e-01 (6.0200e-01)	Acc@1  80.88 ( 78.17)
The current update step is 4560
GPU_0_using curriculum 20 with window 20
Epoch: [152][20/30]	Time  1.557 ( 1.568)	Data  0.048 ( 0.049)	InnerLoop  0.645 ( 0.665)	Loss 6.9853e-01 (6.3272e-01)	Acc@1  74.73 ( 75.91)
The current update step is 4590
GPU_0_using curriculum 20 with window 20
Epoch: [153][20/30]	Time  1.535 ( 1.566)	Data  0.043 ( 0.055)	InnerLoop  0.640 ( 0.658)	Loss 5.1510e-01 (6.2048e-01)	Acc@1  82.28 ( 77.18)
The current update step is 4620
GPU_0_using curriculum 20 with window 20
Epoch: [154][20/30]	Time  1.699 ( 1.571)	Data  0.165 ( 0.075)	InnerLoop  0.648 ( 0.645)	Loss 8.1669e-01 (7.2620e-01)	Acc@1  71.41 ( 73.71)
The current update step is 4650
The current seed is 2079550905450430841
The current lr is: 0.001
Testing Results:
 *   Acc@1 71.987
 *   Acc@1 72.364
 *   Acc@1 73.513
 *   Acc@1 73.632
 *   Acc@1 73.566
 *   Acc@1 73.731
 *   Acc@1 74.013
 *   Acc@1 74.132
 *   Acc@1 74.224
 *   Acc@1 74.616
 *   Acc@1 74.237
 *   Acc@1 74.828
Training for 300 epoch: 73.0
Training for 600 epoch: 73.86842105263158
Training for 1000 epoch: 73.90131578947368
Training for 300 epoch: 73.24833333333333
Training for 600 epoch: 74.12416666666667
Training for 1000 epoch: 74.27958333333333
[[73.0, 73.86842105263158, 73.90131578947368], [73.24833333333333, 74.12416666666667, 74.27958333333333]]
train loss 0.32639565065701803, epoch 154, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [155][20/30]	Time  1.536 ( 1.560)	Data  0.043 ( 0.055)	InnerLoop  0.643 ( 0.656)	Loss 6.1005e-01 (6.9670e-01)	Acc@1  76.12 ( 74.61)
The current update step is 4680
GPU_0_using curriculum 20 with window 20
Epoch: [156][20/30]	Time  1.651 ( 1.567)	Data  0.166 ( 0.074)	InnerLoop  0.630 ( 0.644)	Loss 6.2680e-01 (7.9921e-01)	Acc@1  76.37 ( 72.81)
The current update step is 4710
GPU_0_using curriculum 20 with window 20
Epoch: [157][20/30]	Time  1.529 ( 1.563)	Data  0.042 ( 0.061)	InnerLoop  0.640 ( 0.651)	Loss 6.2573e-01 (6.6248e-01)	Acc@1  77.05 ( 76.12)
The current update step is 4740
GPU_0_using curriculum 20 with window 20
Epoch: [158][20/30]	Time  1.526 ( 1.560)	Data  0.040 ( 0.049)	InnerLoop  0.639 ( 0.663)	Loss 1.0268e+00 (6.8484e-01)	Acc@1  63.11 ( 74.41)
The current update step is 4770
GPU_0_using curriculum 20 with window 20
Epoch: [159][20/30]	Time  1.524 ( 1.562)	Data  0.041 ( 0.074)	InnerLoop  0.636 ( 0.638)	Loss 6.5314e-01 (6.9356e-01)	Acc@1  74.66 ( 74.76)
The current update step is 4800
The current seed is 18172166814349787886
The current lr is: 0.001
Testing Results:
 *   Acc@1 77.079
 *   Acc@1 77.157
 *   Acc@1 77.855
 *   Acc@1 78.138
 *   Acc@1 77.816
 *   Acc@1 77.867
 *   Acc@1 77.447
 *   Acc@1 77.605
 *   Acc@1 76.671
 *   Acc@1 76.816
 *   Acc@1 75.947
 *   Acc@1 76.211
Training for 300 epoch: 77.26315789473685
Training for 600 epoch: 77.26315789473685
Training for 1000 epoch: 76.88157894736841
Training for 300 epoch: 77.38083333333333
Training for 600 epoch: 77.47666666666666
Training for 1000 epoch: 77.03875
[[77.26315789473685, 77.26315789473685, 76.88157894736841], [77.38083333333333, 77.47666666666666, 77.03875]]
train loss 0.36015401407877606, epoch 159, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [160][20/30]	Time  1.661 ( 1.576)	Data  0.043 ( 0.074)	InnerLoop  0.768 ( 0.647)	Loss 6.5741e-01 (6.8415e-01)	Acc@1  77.34 ( 74.74)
The current update step is 4830
GPU_0_using curriculum 20 with window 20
Epoch: [161][20/30]	Time  1.548 ( 1.575)	Data  0.043 ( 0.062)	InnerLoop  0.649 ( 0.659)	Loss 7.3854e-01 (6.7224e-01)	Acc@1  72.29 ( 75.00)
The current update step is 4860
GPU_0_using curriculum 20 with window 20
Epoch: [162][20/30]	Time  1.545 ( 1.573)	Data  0.044 ( 0.049)	InnerLoop  0.647 ( 0.672)	Loss 8.1583e-01 (6.4874e-01)	Acc@1  68.36 ( 76.30)
The current update step is 4890
GPU_0_using curriculum 20 with window 20
Epoch: [163][20/30]	Time  1.524 ( 1.564)	Data  0.041 ( 0.055)	InnerLoop  0.638 ( 0.660)	Loss 6.2046e-01 (6.3836e-01)	Acc@1  77.22 ( 76.55)
The current update step is 4920
GPU_0_using curriculum 20 with window 20
Epoch: [164][20/30]	Time  1.699 ( 1.568)	Data  0.168 ( 0.075)	InnerLoop  0.635 ( 0.641)	Loss 7.1526e-01 (6.8650e-01)	Acc@1  70.95 ( 74.32)
The current update step is 4950
The current seed is 16082714692591555966
The current lr is: 0.001
Testing Results:
 *   Acc@1 80.329
 *   Acc@1 80.752
 *   Acc@1 79.776
 *   Acc@1 79.730
 *   Acc@1 78.632
 *   Acc@1 79.472
 *   Acc@1 77.737
 *   Acc@1 78.308
 *   Acc@1 76.605
 *   Acc@1 76.833
 *   Acc@1 74.224
 *   Acc@1 75.017
Training for 300 epoch: 79.03289473684211
Training for 600 epoch: 78.19078947368422
Training for 1000 epoch: 76.42763157894737
Training for 300 epoch: 79.52958333333333
Training for 600 epoch: 78.28166666666667
Training for 1000 epoch: 77.24458333333334
[[79.03289473684211, 78.19078947368422, 76.42763157894737], [79.52958333333333, 78.28166666666667, 77.24458333333334]]
train loss 0.31670260734558103, epoch 164, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [165][20/30]	Time  1.526 ( 1.563)	Data  0.046 ( 0.056)	InnerLoop  0.633 ( 0.659)	Loss 6.4531e-01 (6.8841e-01)	Acc@1  77.08 ( 75.45)
The current update step is 4980
GPU_0_using curriculum 20 with window 20
Epoch: [166][20/30]	Time  1.657 ( 1.568)	Data  0.169 ( 0.075)	InnerLoop  0.635 ( 0.643)	Loss 5.6886e-01 (6.1004e-01)	Acc@1  79.81 ( 77.37)
The current update step is 5010
GPU_0_using curriculum 20 with window 20
Epoch: [167][20/30]	Time  1.554 ( 1.564)	Data  0.045 ( 0.061)	InnerLoop  0.637 ( 0.650)	Loss 6.2252e-01 (6.5140e-01)	Acc@1  79.52 ( 76.69)
The current update step is 5040
GPU_0_using curriculum 20 with window 20
Epoch: [168][20/30]	Time  1.530 ( 1.565)	Data  0.045 ( 0.051)	InnerLoop  0.636 ( 0.665)	Loss 5.2503e-01 (6.2516e-01)	Acc@1  81.15 ( 77.37)
The current update step is 5070
GPU_0_using curriculum 20 with window 20
Epoch: [169][20/30]	Time  1.562 ( 1.562)	Data  0.044 ( 0.074)	InnerLoop  0.637 ( 0.637)	Loss 5.0545e-01 (6.8115e-01)	Acc@1  82.03 ( 74.59)
The current update step is 5100
The current seed is 11578394888863041083
The current lr is: 0.001
Testing Results:
 *   Acc@1 69.171
 *   Acc@1 69.738
 *   Acc@1 70.000
 *   Acc@1 70.304
 *   Acc@1 71.289
 *   Acc@1 71.340
 *   Acc@1 70.737
 *   Acc@1 70.754
 *   Acc@1 70.105
 *   Acc@1 70.292
 *   Acc@1 70.974
 *   Acc@1 70.785
Training for 300 epoch: 69.95394736842105
Training for 600 epoch: 70.05263157894737
Training for 1000 epoch: 71.13157894736841
Training for 300 epoch: 70.24625
Training for 600 epoch: 70.29791666666667
Training for 1000 epoch: 71.0625
[[69.95394736842105, 70.05263157894737, 71.13157894736841], [70.24625, 70.29791666666667, 71.0625]]
train loss 0.39245263301531474, epoch 169, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [170][20/30]	Time  1.655 ( 1.570)	Data  0.043 ( 0.074)	InnerLoop  0.757 ( 0.645)	Loss 5.6237e-01 (7.0589e-01)	Acc@1  79.59 ( 73.78)
The current update step is 5130
GPU_0_using curriculum 20 with window 20
Epoch: [171][20/30]	Time  1.541 ( 1.566)	Data  0.045 ( 0.062)	InnerLoop  0.638 ( 0.653)	Loss 5.6836e-01 (6.0391e-01)	Acc@1  79.61 ( 77.50)
The current update step is 5160
GPU_0_using curriculum 20 with window 20
Epoch: [172][20/30]	Time  1.561 ( 1.575)	Data  0.050 ( 0.050)	InnerLoop  0.658 ( 0.674)	Loss 7.9893e-01 (6.3381e-01)	Acc@1  74.10 ( 77.04)
The current update step is 5190
GPU_0_using curriculum 20 with window 20
Epoch: [173][20/30]	Time  1.579 ( 1.572)	Data  0.041 ( 0.056)	InnerLoop  0.657 ( 0.666)	Loss 5.8359e-01 (6.8391e-01)	Acc@1  79.59 ( 75.99)
The current update step is 5220
GPU_0_using curriculum 20 with window 20
Epoch: [174][20/30]	Time  1.674 ( 1.575)	Data  0.176 ( 0.075)	InnerLoop  0.646 ( 0.649)	Loss 5.5412e-01 (6.6110e-01)	Acc@1  80.69 ( 75.79)
The current update step is 5250
The current seed is 17512044085600570630
The current lr is: 0.001
Testing Results:
 *   Acc@1 65.526
 *   Acc@1 65.075
 *   Acc@1 66.776
 *   Acc@1 66.593
 *   Acc@1 67.329
 *   Acc@1 67.084
 *   Acc@1 75.645
 *   Acc@1 75.513
 *   Acc@1 75.197
 *   Acc@1 75.330
 *   Acc@1 75.605
 *   Acc@1 75.457
Training for 300 epoch: 70.58552631578948
Training for 600 epoch: 70.98684210526315
Training for 1000 epoch: 71.46710526315789
Training for 300 epoch: 70.29375
Training for 600 epoch: 70.96166666666667
Training for 1000 epoch: 71.27083333333333
[[70.58552631578948, 70.98684210526315, 71.46710526315789], [70.29375, 70.96166666666667, 71.27083333333333]]
train loss 0.3383483858903249, epoch 174, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [175][20/30]	Time  1.555 ( 1.570)	Data  0.044 ( 0.056)	InnerLoop  0.644 ( 0.664)	Loss 6.9936e-01 (6.6093e-01)	Acc@1  72.73 ( 75.65)
The current update step is 5280
GPU_0_using curriculum 20 with window 20
Epoch: [176][20/30]	Time  1.668 ( 1.573)	Data  0.173 ( 0.076)	InnerLoop  0.644 ( 0.650)	Loss 8.0188e-01 (6.4376e-01)	Acc@1  70.85 ( 76.89)
The current update step is 5310
GPU_0_using curriculum 20 with window 20
Epoch: [177][20/30]	Time  1.544 ( 1.565)	Data  0.044 ( 0.061)	InnerLoop  0.645 ( 0.655)	Loss 6.1069e-01 (7.0504e-01)	Acc@1  76.71 ( 74.30)
The current update step is 5340
GPU_0_using curriculum 20 with window 20
Epoch: [178][20/30]	Time  1.531 ( 1.569)	Data  0.047 ( 0.049)	InnerLoop  0.640 ( 0.669)	Loss 5.2004e-01 (6.1725e-01)	Acc@1  81.64 ( 77.35)
The current update step is 5370
GPU_0_using curriculum 20 with window 20
Epoch: [179][20/30]	Time  1.531 ( 1.567)	Data  0.041 ( 0.075)	InnerLoop  0.643 ( 0.641)	Loss 6.0935e-01 (6.2902e-01)	Acc@1  77.12 ( 76.57)
The current update step is 5400
The current seed is 12043792867257406338
The current lr is: 0.001
Testing Results:
 *   Acc@1 75.316
 *   Acc@1 75.436
 *   Acc@1 75.039
 *   Acc@1 75.257
 *   Acc@1 75.316
 *   Acc@1 75.192
 *   Acc@1 61.566
 *   Acc@1 61.773
 *   Acc@1 61.947
 *   Acc@1 62.362
 *   Acc@1 62.250
 *   Acc@1 62.597
Training for 300 epoch: 68.4407894736842
Training for 600 epoch: 68.49342105263158
Training for 1000 epoch: 68.78289473684211
Training for 300 epoch: 68.60458333333334
Training for 600 epoch: 68.81
Training for 1000 epoch: 68.895
[[68.4407894736842, 68.49342105263158, 68.78289473684211], [68.60458333333334, 68.81, 68.895]]
train loss 0.4739990427176158, epoch 179, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [180][20/30]	Time  1.664 ( 1.579)	Data  0.044 ( 0.074)	InnerLoop  0.774 ( 0.653)	Loss 8.1506e-01 (6.2419e-01)	Acc@1  69.38 ( 76.56)
The current update step is 5430
GPU_0_using curriculum 20 with window 20
Epoch: [181][20/30]	Time  1.540 ( 1.568)	Data  0.044 ( 0.062)	InnerLoop  0.645 ( 0.655)	Loss 5.3895e-01 (7.1661e-01)	Acc@1  81.08 ( 73.95)
The current update step is 5460
GPU_0_using curriculum 20 with window 20
Epoch: [182][20/30]	Time  1.524 ( 1.562)	Data  0.044 ( 0.049)	InnerLoop  0.637 ( 0.665)	Loss 5.7968e-01 (6.0079e-01)	Acc@1  78.71 ( 78.45)
The current update step is 5490
GPU_0_using curriculum 20 with window 20
Epoch: [183][20/30]	Time  1.537 ( 1.564)	Data  0.043 ( 0.055)	InnerLoop  0.638 ( 0.662)	Loss 8.4526e-01 (6.9485e-01)	Acc@1  72.17 ( 74.82)
The current update step is 5520
GPU_0_using curriculum 20 with window 20
Epoch: [184][20/30]	Time  1.685 ( 1.583)	Data  0.167 ( 0.075)	InnerLoop  0.642 ( 0.649)	Loss 5.7204e-01 (6.3849e-01)	Acc@1  79.81 ( 76.23)
The current update step is 5550
The current seed is 6743906384200104141
The current lr is: 0.001
Testing Results:
 *   Acc@1 73.250
 *   Acc@1 73.272
 *   Acc@1 73.868
 *   Acc@1 73.957
 *   Acc@1 73.079
 *   Acc@1 73.337
 *   Acc@1 73.987
 *   Acc@1 74.638
 *   Acc@1 76.329
 *   Acc@1 77.401
 *   Acc@1 77.513
 *   Acc@1 77.748
Training for 300 epoch: 73.61842105263159
Training for 600 epoch: 75.09868421052632
Training for 1000 epoch: 75.29605263157895
Training for 300 epoch: 73.955
Training for 600 epoch: 75.67916666666667
Training for 1000 epoch: 75.54208333333334
[[73.61842105263159, 75.09868421052632, 75.29605263157895], [73.955, 75.67916666666667, 75.54208333333334]]
train loss 0.2915654326121012, epoch 184, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [185][20/30]	Time  1.557 ( 1.578)	Data  0.043 ( 0.056)	InnerLoop  0.642 ( 0.663)	Loss 6.3387e-01 (6.9904e-01)	Acc@1  76.20 ( 73.21)
The current update step is 5580
GPU_0_using curriculum 20 with window 20
Epoch: [186][20/30]	Time  1.660 ( 1.568)	Data  0.170 ( 0.074)	InnerLoop  0.645 ( 0.648)	Loss 6.1199e-01 (6.2560e-01)	Acc@1  77.86 ( 76.66)
The current update step is 5610
GPU_0_using curriculum 20 with window 20
Epoch: [187][20/30]	Time  1.526 ( 1.565)	Data  0.042 ( 0.062)	InnerLoop  0.642 ( 0.655)	Loss 5.8199e-01 (6.4079e-01)	Acc@1  80.30 ( 76.40)
The current update step is 5640
GPU_0_using curriculum 20 with window 20
Epoch: [188][20/30]	Time  1.535 ( 1.569)	Data  0.043 ( 0.050)	InnerLoop  0.639 ( 0.667)	Loss 7.3973e-01 (6.9135e-01)	Acc@1  74.98 ( 74.89)
The current update step is 5670
GPU_0_using curriculum 20 with window 20
Epoch: [189][20/30]	Time  1.539 ( 1.573)	Data  0.043 ( 0.076)	InnerLoop  0.648 ( 0.645)	Loss 6.0704e-01 (6.9533e-01)	Acc@1  77.17 ( 74.77)
The current update step is 5700
The current seed is 18291190300611162552
The current lr is: 0.001
Testing Results:
 *   Acc@1 79.000
 *   Acc@1 79.592
 *   Acc@1 78.145
 *   Acc@1 78.797
 *   Acc@1 76.618
 *   Acc@1 77.263
 *   Acc@1 76.408
 *   Acc@1 76.557
 *   Acc@1 76.776
 *   Acc@1 77.164
 *   Acc@1 77.447
 *   Acc@1 77.760
Training for 300 epoch: 77.70394736842105
Training for 600 epoch: 77.46052631578948
Training for 1000 epoch: 77.03289473684211
Training for 300 epoch: 78.07416666666667
Training for 600 epoch: 77.98083333333334
Training for 1000 epoch: 77.51125
[[77.70394736842105, 77.46052631578948, 77.03289473684211], [78.07416666666667, 77.98083333333334, 77.51125]]
train loss 0.3183039637088776, epoch 189, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [190][20/30]	Time  1.662 ( 1.582)	Data  0.042 ( 0.076)	InnerLoop  0.771 ( 0.653)	Loss 6.1178e-01 (6.5143e-01)	Acc@1  75.81 ( 75.92)
The current update step is 5730
GPU_0_using curriculum 20 with window 20
Epoch: [191][20/30]	Time  1.546 ( 1.571)	Data  0.044 ( 0.062)	InnerLoop  0.642 ( 0.659)	Loss 8.4197e-01 (6.6373e-01)	Acc@1  67.21 ( 75.49)
The current update step is 5760
GPU_0_using curriculum 20 with window 20
Epoch: [192][20/30]	Time  1.525 ( 1.564)	Data  0.044 ( 0.049)	InnerLoop  0.637 ( 0.669)	Loss 7.0633e-01 (6.6565e-01)	Acc@1  74.73 ( 74.94)
The current update step is 5790
GPU_0_using curriculum 20 with window 20
Epoch: [193][20/30]	Time  1.522 ( 1.565)	Data  0.042 ( 0.055)	InnerLoop  0.639 ( 0.659)	Loss 5.0838e-01 (6.0697e-01)	Acc@1  81.52 ( 77.07)
The current update step is 5820
GPU_0_using curriculum 20 with window 20
Epoch: [194][20/30]	Time  1.711 ( 1.576)	Data  0.165 ( 0.074)	InnerLoop  0.661 ( 0.648)	Loss 5.6032e-01 (5.8250e-01)	Acc@1  79.44 ( 78.58)
The current update step is 5850
The current seed is 4065073392240997117
The current lr is: 0.001
Testing Results:
 *   Acc@1 66.974
 *   Acc@1 67.312
 *   Acc@1 65.513
 *   Acc@1 65.914
 *   Acc@1 65.184
 *   Acc@1 65.698
 *   Acc@1 78.908
 *   Acc@1 79.110
 *   Acc@1 77.553
 *   Acc@1 78.032
 *   Acc@1 76.526
 *   Acc@1 76.833
Training for 300 epoch: 72.94078947368422
Training for 600 epoch: 71.53289473684211
Training for 1000 epoch: 70.85526315789474
Training for 300 epoch: 73.21083333333334
Training for 600 epoch: 71.97291666666666
Training for 1000 epoch: 71.26541666666667
[[72.94078947368422, 71.53289473684211, 70.85526315789474], [73.21083333333334, 71.97291666666666, 71.26541666666667]]
train loss 0.30567936862309775, epoch 194, best loss 0.278261639769872, best_epoch 139
GPU_0_using curriculum 20 with window 20
Epoch: [195][20/30]	Time  1.549 ( 1.571)	Data  0.042 ( 0.055)	InnerLoop  0.651 ( 0.664)	Loss 5.3180e-01 (6.5660e-01)	Acc@1  80.32 ( 76.29)
The current update step is 5880
GPU_0_using curriculum 20 with window 20
Epoch: [196][20/30]	Time  1.756 ( 1.614)	Data  0.182 ( 0.077)	InnerLoop  0.693 ( 0.673)	Loss 5.5748e-01 (5.7000e-01)	Acc@1  78.22 ( 79.12)
The current update step is 5910
GPU_0_using curriculum 20 with window 20
Epoch: [197][20/30]	Time  1.592 ( 1.595)	Data  0.045 ( 0.063)	InnerLoop  0.668 ( 0.672)	Loss 4.9752e-01 (6.5641e-01)	Acc@1  83.42 ( 75.70)
The current update step is 5940
GPU_0_using curriculum 20 with window 20
Epoch: [198][20/30]	Time  1.548 ( 1.573)	Data  0.043 ( 0.050)	InnerLoop  0.646 ( 0.675)	Loss 5.7374e-01 (6.5440e-01)	Acc@1  77.91 ( 75.09)
The current update step is 5970
GPU_0_using curriculum 20 with window 20
Epoch: [199][20/30]	Time  1.569 ( 1.570)	Data  0.045 ( 0.074)	InnerLoop  0.664 ( 0.646)	Loss 6.2629e-01 (6.2791e-01)	Acc@1  76.17 ( 77.06)
The current update step is 6000
The current seed is 6195064257846159448
The current lr is: 0.001
Testing Results:
 *   Acc@1 78.789
 *   Acc@1 79.312
 *   Acc@1 77.224
 *   Acc@1 76.914
 *   Acc@1 75.961
 *   Acc@1 76.118
 *   Acc@1 67.382
 *   Acc@1 67.681
 *   Acc@1 67.842
 *   Acc@1 68.147
 *   Acc@1 68.355
 *   Acc@1 68.394
Training for 300 epoch: 73.08552631578948
Training for 600 epoch: 72.53289473684211
Training for 1000 epoch: 72.15789473684211
Training for 300 epoch: 73.49666666666667
Training for 600 epoch: 72.53083333333333
Training for 1000 epoch: 72.25625
[[73.08552631578948, 72.53289473684211, 72.15789473684211], [73.49666666666667, 72.53083333333333, 72.25625]]
train loss 0.42665736910502117, epoch 199, best loss 0.278261639769872, best_epoch 199
=== Final results:
{'acc': 80.28289473684211, 'test': [80.28289473684211, 80.26315789473685, 79.75], 'train': [80.28289473684211, 80.26315789473685, 79.75], 'ind': 0, 'epoch': 80, 'data': array([[-0.01430656, -0.0502343 , -0.00785431, ...,  0.0738548 ,
        -0.00313396,  0.00678399],
       [-0.09122472,  0.04448134, -0.00617976, ...,  0.01445823,
        -0.00423555, -0.02017434],
       [-0.03145383,  0.01570073, -0.02697725, ..., -0.00613981,
         0.02956895, -0.03792242],
       ...,
       [-0.01389758,  0.07025357,  0.00285081, ...,  0.01246964,
        -0.05189035,  0.01539369],
       [-0.0317652 ,  0.08628026, -0.04898779, ...,  0.06882333,
         0.02318314, -0.02346122],
       [ 0.05299794,  0.06752086,  0.00872027, ...,  0.08186905,
        -0.08629809, -0.06885049]], shape=(20, 768), dtype=float32)}
