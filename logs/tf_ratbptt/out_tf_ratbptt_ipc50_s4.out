Torch Seed Specified with rank: 0
Dataset: agnews_emb
Dataset Path: ./scripts
Namespace(seed=0, mp_distributed=False, world_size=1, rank=0, dist_url='tcp://224.66.41.62:23456', dist_backend='nccl', workers=0, gpu=None, root='./scripts', dataset='agnews_emb', arch='text_transformer', width=256, lr=0.001, inner_optim='Adam', outer_optim='Adam', inner_lr=0.001, label_lr_scale=1, num_per_class=50, batch_per_class=10, task_sampler_nc=4, window=40, minwindow=0, totwindow=100, num_train_eval=4, train_y=False, batch_size=4096, eps=1e-08, wd=0, test_freq=5, print_freq=20, start_epoch=0, epochs=100, ddtype='standard', cctype=0, zca=False, wandb=False, clip_coef=0.9, fname='out_tf_ratbptt_ipc50_s4', out_dir='./checkpoints', name='agnews_tf_ratbptt_s4', comp_aug=False, comp_aug_real=False, syn_strategy='none', real_strategy='none', ckptname='none', limit_train=False, load_ckpt=False, complete_random=False, boost_dd=False, boost_init_from='none', boost_beta=1.0, stage=4, distributed=False, data_root='./scripts/agnews_emb')
==> Preparing data..
None None
Dataset: number of classes: 4
Training set size: 120000
Image size: channel 1, height 768, width 1
Synthetic images, not_single False, keys []
==> Building model..
Initialized data with size, x: torch.Size([200, 768]), y:torch.Size([200])
TextTransformer(
  (input_proj): Linear(in_features=192, out_features=256, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls_head): Linear(in_features=256, out_features=4, bias=True)
)
use data parallel only
GPU_0_using curriculum 40 with window 40
Epoch: [0][20/30]	Time  3.943 ( 4.071)	Data  0.046 ( 0.067)	InnerLoop  1.664 ( 1.748)	Loss 5.4278e+00 (4.4625e+00)	Acc@1  22.90 ( 23.95)
The current update step is 30
GPU_0_using curriculum 40 with window 40
Epoch: [1][20/30]	Time  3.899 ( 3.939)	Data  0.043 ( 0.071)	InnerLoop  1.632 ( 1.655)	Loss 3.3371e+00 (4.1843e+00)	Acc@1  32.32 ( 26.93)
The current update step is 60
GPU_0_using curriculum 40 with window 40
Epoch: [2][20/30]	Time  4.008 ( 3.928)	Data  0.167 ( 0.073)	InnerLoop  1.622 ( 1.647)	Loss 3.1660e+00 (3.3456e+00)	Acc@1  30.88 ( 30.31)
The current update step is 90
GPU_0_using curriculum 40 with window 40
Epoch: [3][20/30]	Time  3.916 ( 3.862)	Data  0.043 ( 0.057)	InnerLoop  1.723 ( 1.644)	Loss 4.3413e+00 (3.7401e+00)	Acc@1  26.39 ( 29.91)
The current update step is 120
GPU_0_using curriculum 40 with window 40
Epoch: [4][20/30]	Time  3.709 ( 3.780)	Data  0.045 ( 0.067)	InnerLoop  1.562 ( 1.600)	Loss 5.2438e+00 (4.5752e+00)	Acc@1  25.46 ( 27.00)
The current update step is 150
The current seed is 4524949931165927461
The current lr is: 0.001
Testing Results:
 *   Acc@1 23.803
 *   Acc@1 23.605
 *   Acc@1 22.447
 *   Acc@1 22.419
 *   Acc@1 22.539
 *   Acc@1 22.377
 *   Acc@1 22.671
 *   Acc@1 22.412
 *   Acc@1 27.118
 *   Acc@1 26.675
 *   Acc@1 27.276
 *   Acc@1 27.012
 *   Acc@1 27.197
 *   Acc@1 27.474
 *   Acc@1 28.118
 *   Acc@1 28.128
 *   Acc@1 25.197
 *   Acc@1 24.995
 *   Acc@1 26.776
 *   Acc@1 26.807
 *   Acc@1 26.776
 *   Acc@1 26.806
 *   Acc@1 26.868
 *   Acc@1 26.760
 *   Acc@1 26.224
 *   Acc@1 25.852
 *   Acc@1 26.500
 *   Acc@1 26.165
 *   Acc@1 26.500
 *   Acc@1 26.392
 *   Acc@1 25.579
 *   Acc@1 25.151
Training for 300 epoch: 25.585526315789473
Training for 600 epoch: 25.75
Training for 1000 epoch: 25.753289473684212
Training for 3000 epoch: 25.80921052631579
Training for 300 epoch: 25.281666666666666
Training for 600 epoch: 25.600625
Training for 1000 epoch: 25.762083333333333
Training for 3000 epoch: 25.612708333333334
[[25.585526315789473, 25.75, 25.753289473684212, 25.80921052631579], [25.281666666666666, 25.600625, 25.762083333333333, 25.612708333333334]]
train loss 1.7172054486592612, epoch 4, best loss 1.7172054486592612, best_epoch 4
GPU_0_using curriculum 40 with window 40
Epoch: [5][20/30]	Time  3.649 ( 3.685)	Data  0.037 ( 0.058)	InnerLoop  1.549 ( 1.574)	Loss 3.5394e+00 (3.9987e+00)	Acc@1  29.22 ( 25.28)
The current update step is 180
GPU_0_using curriculum 40 with window 40
Epoch: [6][20/30]	Time  3.762 ( 3.692)	Data  0.044 ( 0.071)	InnerLoop  1.661 ( 1.567)	Loss 4.8287e+00 (3.8270e+00)	Acc@1  24.71 ( 27.46)
The current update step is 210
GPU_0_using curriculum 40 with window 40
Epoch: [7][20/30]	Time  3.619 ( 3.691)	Data  0.040 ( 0.046)	InnerLoop  1.531 ( 1.589)	Loss 4.0191e+00 (4.0065e+00)	Acc@1  26.25 ( 27.36)
The current update step is 240
GPU_0_using curriculum 40 with window 40
Epoch: [8][20/30]	Time  3.626 ( 3.679)	Data  0.046 ( 0.071)	InnerLoop  1.525 ( 1.553)	Loss 3.7358e+00 (3.7590e+00)	Acc@1  29.15 ( 29.06)
The current update step is 270
GPU_0_using curriculum 40 with window 40
Epoch: [9][20/30]	Time  3.625 ( 3.677)	Data  0.038 ( 0.054)	InnerLoop  1.529 ( 1.569)	Loss 4.4133e+00 (4.2413e+00)	Acc@1  21.80 ( 25.08)
The current update step is 300
The current seed is 4020478238999953456
The current lr is: 0.001
Testing Results:
 *   Acc@1 28.500
 *   Acc@1 27.369
 *   Acc@1 27.974
 *   Acc@1 27.450
 *   Acc@1 28.645
 *   Acc@1 27.611
 *   Acc@1 29.355
 *   Acc@1 28.112
 *   Acc@1 25.645
 *   Acc@1 25.531
 *   Acc@1 26.474
 *   Acc@1 26.777
 *   Acc@1 26.671
 *   Acc@1 26.780
 *   Acc@1 26.711
 *   Acc@1 26.924
 *   Acc@1 27.026
 *   Acc@1 26.212
 *   Acc@1 27.053
 *   Acc@1 26.405
 *   Acc@1 26.908
 *   Acc@1 26.419
 *   Acc@1 27.737
 *   Acc@1 27.396
 *   Acc@1 28.408
 *   Acc@1 28.837
 *   Acc@1 28.461
 *   Acc@1 28.852
 *   Acc@1 28.539
 *   Acc@1 28.649
 *   Acc@1 28.237
 *   Acc@1 28.198
Training for 300 epoch: 27.394736842105264
Training for 600 epoch: 27.49013157894737
Training for 1000 epoch: 27.690789473684212
Training for 3000 epoch: 28.00986842105263
Training for 300 epoch: 26.987083333333338
Training for 600 epoch: 27.371249999999996
Training for 1000 epoch: 27.36479166666667
Training for 3000 epoch: 27.65729166666667
[[27.394736842105264, 27.49013157894737, 27.690789473684212, 28.00986842105263], [26.987083333333338, 27.371249999999996, 27.36479166666667, 27.65729166666667]]
train loss 1.3546445835113525, epoch 9, best loss 1.3546445835113525, best_epoch 9
GPU_0_using curriculum 40 with window 40
Epoch: [10][20/30]	Time  3.776 ( 3.690)	Data  0.043 ( 0.058)	InnerLoop  1.650 ( 1.568)	Loss 2.8980e+00 (3.5303e+00)	Acc@1  30.08 ( 26.98)
The current update step is 330
GPU_0_using curriculum 40 with window 40
Epoch: [11][20/30]	Time  3.636 ( 3.681)	Data  0.043 ( 0.046)	InnerLoop  1.529 ( 1.577)	Loss 3.8946e+00 (3.9181e+00)	Acc@1  24.80 ( 25.83)
The current update step is 360
GPU_0_using curriculum 40 with window 40
Epoch: [12][20/30]	Time  3.643 ( 3.688)	Data  0.044 ( 0.047)	InnerLoop  1.537 ( 1.580)	Loss 2.9602e+00 (3.6669e+00)	Acc@1  30.98 ( 26.14)
The current update step is 390
GPU_0_using curriculum 40 with window 40
Epoch: [13][20/30]	Time  3.884 ( 3.948)	Data  0.044 ( 0.070)	InnerLoop  1.647 ( 1.700)	Loss 3.5689e+00 (3.4380e+00)	Acc@1  22.24 ( 26.28)
The current update step is 420
GPU_0_using curriculum 40 with window 40
Epoch: [14][20/30]	Time  3.995 ( 3.939)	Data  0.181 ( 0.077)	InnerLoop  1.619 ( 1.687)	Loss 2.6626e+00 (3.0837e+00)	Acc@1  21.36 ( 26.09)
The current update step is 450
The current seed is 148378832535464395
The current lr is: 0.001
Testing Results:
 *   Acc@1 29.434
 *   Acc@1 29.737
 *   Acc@1 29.066
 *   Acc@1 29.175
 *   Acc@1 28.566
 *   Acc@1 28.859
 *   Acc@1 27.789
 *   Acc@1 27.950
 *   Acc@1 25.763
 *   Acc@1 25.805
 *   Acc@1 25.908
 *   Acc@1 26.012
 *   Acc@1 26.224
 *   Acc@1 26.309
 *   Acc@1 26.566
 *   Acc@1 26.802
 *   Acc@1 25.289
 *   Acc@1 25.642
 *   Acc@1 25.118
 *   Acc@1 25.410
 *   Acc@1 24.868
 *   Acc@1 25.244
 *   Acc@1 24.921
 *   Acc@1 25.576
 *   Acc@1 27.711
 *   Acc@1 26.907
 *   Acc@1 27.908
 *   Acc@1 27.301
 *   Acc@1 28.184
 *   Acc@1 27.630
 *   Acc@1 29.303
 *   Acc@1 28.765
Training for 300 epoch: 27.049342105263158
Training for 600 epoch: 27.0
Training for 1000 epoch: 26.960526315789473
Training for 3000 epoch: 27.144736842105264
Training for 300 epoch: 27.022708333333334
Training for 600 epoch: 26.974375
Training for 1000 epoch: 27.010625
Training for 3000 epoch: 27.273333333333333
[[27.049342105263158, 27.0, 26.960526315789473, 27.144736842105264], [27.022708333333334, 26.974375, 27.010625, 27.273333333333333]]
train loss 1.132494519106547, epoch 14, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [15][20/30]	Time  3.768 ( 3.675)	Data  0.040 ( 0.051)	InnerLoop  1.653 ( 1.572)	Loss 3.5321e+00 (3.4301e+00)	Acc@1  28.66 ( 26.88)
The current update step is 480
GPU_0_using curriculum 40 with window 40
Epoch: [16][20/30]	Time  3.728 ( 3.676)	Data  0.042 ( 0.046)	InnerLoop  1.635 ( 1.580)	Loss 3.2621e+00 (3.3988e+00)	Acc@1  23.54 ( 26.12)
The current update step is 510
GPU_0_using curriculum 40 with window 40
Epoch: [17][20/30]	Time  3.618 ( 3.661)	Data  0.049 ( 0.065)	InnerLoop  1.512 ( 1.548)	Loss 3.4396e+00 (3.2926e+00)	Acc@1  25.85 ( 28.59)
The current update step is 540
GPU_0_using curriculum 40 with window 40
Epoch: [18][20/30]	Time  3.616 ( 3.661)	Data  0.037 ( 0.053)	InnerLoop  1.524 ( 1.562)	Loss 3.1534e+00 (3.0462e+00)	Acc@1  36.60 ( 29.21)
The current update step is 570
GPU_0_using curriculum 40 with window 40
Epoch: [19][20/30]	Time  3.727 ( 3.664)	Data  0.044 ( 0.052)	InnerLoop  1.636 ( 1.565)	Loss 2.7947e+00 (3.5769e+00)	Acc@1  26.00 ( 26.39)
The current update step is 600
The current seed is 8429459796654913455
The current lr is: 0.001
Testing Results:
 *   Acc@1 25.671
 *   Acc@1 25.441
 *   Acc@1 25.500
 *   Acc@1 25.336
 *   Acc@1 25.526
 *   Acc@1 25.337
 *   Acc@1 25.395
 *   Acc@1 25.323
 *   Acc@1 26.618
 *   Acc@1 26.359
 *   Acc@1 26.461
 *   Acc@1 26.283
 *   Acc@1 26.474
 *   Acc@1 26.285
 *   Acc@1 29.066
 *   Acc@1 28.602
 *   Acc@1 33.224
 *   Acc@1 33.019
 *   Acc@1 32.921
 *   Acc@1 33.067
 *   Acc@1 33.789
 *   Acc@1 33.264
 *   Acc@1 32.526
 *   Acc@1 32.261
 *   Acc@1 27.789
 *   Acc@1 27.198
 *   Acc@1 27.961
 *   Acc@1 27.841
 *   Acc@1 28.434
 *   Acc@1 28.378
 *   Acc@1 30.211
 *   Acc@1 29.688
Training for 300 epoch: 28.325657894736842
Training for 600 epoch: 28.210526315789473
Training for 1000 epoch: 28.55592105263158
Training for 3000 epoch: 29.299342105263158
Training for 300 epoch: 28.004166666666666
Training for 600 epoch: 28.131666666666668
Training for 1000 epoch: 28.316041666666667
Training for 3000 epoch: 28.968541666666667
[[28.325657894736842, 28.210526315789473, 28.55592105263158, 29.299342105263158], [28.004166666666666, 28.131666666666668, 28.316041666666667, 28.968541666666667]]
train loss 1.264411485226949, epoch 19, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [20][20/30]	Time  3.619 ( 3.668)	Data  0.040 ( 0.045)	InnerLoop  1.526 ( 1.571)	Loss 4.5072e+00 (3.1047e+00)	Acc@1  25.54 ( 29.33)
The current update step is 630
GPU_0_using curriculum 40 with window 40
Epoch: [21][20/30]	Time  3.597 ( 3.663)	Data  0.042 ( 0.047)	InnerLoop  1.511 ( 1.567)	Loss 2.3549e+00 (2.9262e+00)	Acc@1  29.91 ( 29.23)
The current update step is 660
GPU_0_using curriculum 40 with window 40
Epoch: [22][20/30]	Time  3.623 ( 3.668)	Data  0.037 ( 0.059)	InnerLoop  1.523 ( 1.556)	Loss 2.8676e+00 (3.4607e+00)	Acc@1  24.00 ( 27.77)
The current update step is 690
GPU_0_using curriculum 40 with window 40
Epoch: [23][20/30]	Time  3.752 ( 3.665)	Data  0.160 ( 0.065)	InnerLoop  1.525 ( 1.550)	Loss 5.4831e+00 (4.3073e+00)	Acc@1  24.68 ( 27.20)
The current update step is 720
GPU_0_using curriculum 40 with window 40
Epoch: [24][20/30]	Time  3.722 ( 3.670)	Data  0.038 ( 0.052)	InnerLoop  1.643 ( 1.571)	Loss 3.2192e+00 (3.6314e+00)	Acc@1  30.66 ( 27.08)
The current update step is 750
The current seed is 4408919899868606877
The current lr is: 0.001
Testing Results:
 *   Acc@1 23.447
 *   Acc@1 22.607
 *   Acc@1 23.000
 *   Acc@1 22.632
 *   Acc@1 23.250
 *   Acc@1 22.865
 *   Acc@1 22.605
 *   Acc@1 22.343
 *   Acc@1 25.461
 *   Acc@1 25.576
 *   Acc@1 25.592
 *   Acc@1 25.578
 *   Acc@1 25.592
 *   Acc@1 25.644
 *   Acc@1 25.342
 *   Acc@1 25.379
 *   Acc@1 27.118
 *   Acc@1 26.498
 *   Acc@1 26.987
 *   Acc@1 26.326
 *   Acc@1 26.882
 *   Acc@1 26.297
 *   Acc@1 26.013
 *   Acc@1 25.788
 *   Acc@1 25.487
 *   Acc@1 25.628
 *   Acc@1 23.395
 *   Acc@1 23.688
 *   Acc@1 21.513
 *   Acc@1 21.707
 *   Acc@1 21.224
 *   Acc@1 20.755
Training for 300 epoch: 25.378289473684212
Training for 600 epoch: 24.743421052631575
Training for 1000 epoch: 24.309210526315788
Training for 3000 epoch: 23.79605263157895
Training for 300 epoch: 25.077291666666667
Training for 600 epoch: 24.555625
Training for 1000 epoch: 24.128125
Training for 3000 epoch: 23.566041666666663
[[25.378289473684212, 24.743421052631575, 24.309210526315788, 23.79605263157895], [25.077291666666667, 24.555625, 24.128125, 23.566041666666663]]
train loss 1.5621264570871989, epoch 24, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [25][20/30]	Time  3.751 ( 3.678)	Data  0.040 ( 0.057)	InnerLoop  1.657 ( 1.570)	Loss 4.2238e+00 (3.2933e+00)	Acc@1  27.27 ( 28.41)
The current update step is 780
GPU_0_using curriculum 40 with window 40
Epoch: [26][20/30]	Time  3.635 ( 3.673)	Data  0.045 ( 0.052)	InnerLoop  1.537 ( 1.571)	Loss 3.8785e+00 (3.5242e+00)	Acc@1  29.49 ( 28.31)
The current update step is 810
GPU_0_using curriculum 40 with window 40
Epoch: [27][20/30]	Time  3.634 ( 3.688)	Data  0.042 ( 0.061)	InnerLoop  1.534 ( 1.574)	Loss 2.8061e+00 (3.6210e+00)	Acc@1  27.91 ( 29.37)
The current update step is 840
GPU_0_using curriculum 40 with window 40
Epoch: [28][20/30]	Time  3.620 ( 3.675)	Data  0.043 ( 0.065)	InnerLoop  1.529 ( 1.561)	Loss 3.1999e+00 (3.6754e+00)	Acc@1  23.85 ( 28.75)
The current update step is 870
GPU_0_using curriculum 40 with window 40
Epoch: [29][20/30]	Time  3.741 ( 3.671)	Data  0.156 ( 0.065)	InnerLoop  1.525 ( 1.556)	Loss 2.4476e+00 (3.5683e+00)	Acc@1  30.66 ( 28.33)
The current update step is 900
The current seed is 12772327310660338661
The current lr is: 0.001
Testing Results:
 *   Acc@1 27.645
 *   Acc@1 27.558
 *   Acc@1 28.158
 *   Acc@1 27.824
 *   Acc@1 28.289
 *   Acc@1 28.172
 *   Acc@1 29.961
 *   Acc@1 29.538
 *   Acc@1 25.961
 *   Acc@1 26.052
 *   Acc@1 25.750
 *   Acc@1 25.717
 *   Acc@1 25.553
 *   Acc@1 25.599
 *   Acc@1 25.316
 *   Acc@1 25.486
 *   Acc@1 29.289
 *   Acc@1 29.102
 *   Acc@1 31.118
 *   Acc@1 31.095
 *   Acc@1 30.197
 *   Acc@1 30.093
 *   Acc@1 29.895
 *   Acc@1 29.587
 *   Acc@1 30.618
 *   Acc@1 30.295
 *   Acc@1 30.276
 *   Acc@1 29.896
 *   Acc@1 29.737
 *   Acc@1 29.042
 *   Acc@1 28.171
 *   Acc@1 27.798
Training for 300 epoch: 28.37828947368421
Training for 600 epoch: 28.825657894736842
Training for 1000 epoch: 28.444078947368418
Training for 3000 epoch: 28.335526315789473
Training for 300 epoch: 28.25166666666667
Training for 600 epoch: 28.632916666666663
Training for 1000 epoch: 28.22625
Training for 3000 epoch: 28.102291666666666
[[28.37828947368421, 28.825657894736842, 28.444078947368418, 28.335526315789473], [28.25166666666667, 28.632916666666663, 28.22625, 28.102291666666666]]
train loss 1.2434204390207926, epoch 29, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [30][20/30]	Time  3.751 ( 3.685)	Data  0.037 ( 0.052)	InnerLoop  1.662 ( 1.582)	Loss 3.0493e+00 (3.2091e+00)	Acc@1  31.10 ( 28.30)
The current update step is 930
GPU_0_using curriculum 40 with window 40
Epoch: [31][20/30]	Time  3.736 ( 3.677)	Data  0.040 ( 0.047)	InnerLoop  1.646 ( 1.582)	Loss 3.6012e+00 (3.3264e+00)	Acc@1  24.07 ( 28.33)
The current update step is 960
GPU_0_using curriculum 40 with window 40
Epoch: [32][20/30]	Time  3.635 ( 3.668)	Data  0.042 ( 0.064)	InnerLoop  1.524 ( 1.555)	Loss 3.0150e+00 (3.7495e+00)	Acc@1  25.68 ( 25.41)
The current update step is 990
GPU_0_using curriculum 40 with window 40
Epoch: [33][20/30]	Time  3.623 ( 3.665)	Data  0.037 ( 0.052)	InnerLoop  1.534 ( 1.568)	Loss 2.5361e+00 (3.1802e+00)	Acc@1  36.23 ( 28.64)
The current update step is 1020
GPU_0_using curriculum 40 with window 40
Epoch: [34][20/30]	Time  3.785 ( 3.716)	Data  0.049 ( 0.056)	InnerLoop  1.665 ( 1.589)	Loss 2.0587e+00 (3.6966e+00)	Acc@1  35.33 ( 30.29)
The current update step is 1050
The current seed is 1487212249139214132
The current lr is: 0.001
Testing Results:
 *   Acc@1 30.000
 *   Acc@1 29.476
 *   Acc@1 30.013
 *   Acc@1 29.180
 *   Acc@1 30.566
 *   Acc@1 29.675
 *   Acc@1 30.526
 *   Acc@1 29.606
 *   Acc@1 31.737
 *   Acc@1 32.736
 *   Acc@1 31.211
 *   Acc@1 31.871
 *   Acc@1 31.658
 *   Acc@1 31.916
 *   Acc@1 31.276
 *   Acc@1 31.622
 *   Acc@1 30.618
 *   Acc@1 30.386
 *   Acc@1 29.276
 *   Acc@1 28.899
 *   Acc@1 29.434
 *   Acc@1 29.011
 *   Acc@1 29.053
 *   Acc@1 29.322
 *   Acc@1 25.750
 *   Acc@1 25.901
 *   Acc@1 26.105
 *   Acc@1 26.098
 *   Acc@1 26.158
 *   Acc@1 26.268
 *   Acc@1 26.263
 *   Acc@1 26.380
Training for 300 epoch: 29.526315789473685
Training for 600 epoch: 29.151315789473685
Training for 1000 epoch: 29.453947368421055
Training for 3000 epoch: 29.279605263157897
Training for 300 epoch: 29.624583333333334
Training for 600 epoch: 29.01208333333333
Training for 1000 epoch: 29.2175
Training for 3000 epoch: 29.232291666666665
[[29.526315789473685, 29.151315789473685, 29.453947368421055, 29.279605263157897], [29.624583333333334, 29.01208333333333, 29.2175, 29.232291666666665]]
train loss 1.3900406070073446, epoch 34, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [35][20/30]	Time  3.653 ( 3.707)	Data  0.039 ( 0.048)	InnerLoop  1.540 ( 1.590)	Loss 3.5475e+00 (3.5806e+00)	Acc@1  29.39 ( 30.94)
The current update step is 1080
GPU_0_using curriculum 40 with window 40
Epoch: [36][20/30]	Time  3.809 ( 3.764)	Data  0.048 ( 0.053)	InnerLoop  1.616 ( 1.620)	Loss 3.9772e+00 (3.3143e+00)	Acc@1  27.42 ( 30.49)
The current update step is 1110
GPU_0_using curriculum 40 with window 40
Epoch: [37][20/30]	Time  3.744 ( 3.807)	Data  0.052 ( 0.067)	InnerLoop  1.574 ( 1.626)	Loss 3.9879e+00 (3.7113e+00)	Acc@1  28.00 ( 31.02)
The current update step is 1140
GPU_0_using curriculum 40 with window 40
Epoch: [38][20/30]	Time  3.807 ( 3.810)	Data  0.166 ( 0.074)	InnerLoop  1.548 ( 1.622)	Loss 3.2573e+00 (3.6045e+00)	Acc@1  27.69 ( 30.95)
The current update step is 1170
GPU_0_using curriculum 40 with window 40
Epoch: [39][20/30]	Time  3.735 ( 3.673)	Data  0.038 ( 0.050)	InnerLoop  1.636 ( 1.571)	Loss 2.7955e+00 (3.5805e+00)	Acc@1  29.25 ( 29.37)
The current update step is 1200
The current seed is 1127724696140807607
The current lr is: 0.001
Testing Results:
 *   Acc@1 28.013
 *   Acc@1 28.163
 *   Acc@1 27.882
 *   Acc@1 28.277
 *   Acc@1 28.434
 *   Acc@1 28.351
 *   Acc@1 28.789
 *   Acc@1 28.430
 *   Acc@1 25.645
 *   Acc@1 25.361
 *   Acc@1 25.539
 *   Acc@1 25.466
 *   Acc@1 25.539
 *   Acc@1 25.517
 *   Acc@1 25.632
 *   Acc@1 25.905
 *   Acc@1 31.776
 *   Acc@1 31.987
 *   Acc@1 31.724
 *   Acc@1 31.669
 *   Acc@1 31.079
 *   Acc@1 31.392
 *   Acc@1 31.250
 *   Acc@1 31.220
 *   Acc@1 27.289
 *   Acc@1 27.608
 *   Acc@1 27.118
 *   Acc@1 27.317
 *   Acc@1 26.671
 *   Acc@1 27.043
 *   Acc@1 26.671
 *   Acc@1 27.021
Training for 300 epoch: 28.180921052631582
Training for 600 epoch: 28.06578947368421
Training for 1000 epoch: 27.93092105263158
Training for 3000 epoch: 28.085526315789473
Training for 300 epoch: 28.279375
Training for 600 epoch: 28.18208333333333
Training for 1000 epoch: 28.075833333333332
Training for 3000 epoch: 28.143958333333334
[[28.180921052631582, 28.06578947368421, 27.93092105263158, 28.085526315789473], [28.279375, 28.18208333333333, 28.075833333333332, 28.143958333333334]]
train loss 1.707363013267517, epoch 39, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [40][20/30]	Time  3.754 ( 3.690)	Data  0.038 ( 0.057)	InnerLoop  1.671 ( 1.581)	Loss 2.8921e+00 (3.1445e+00)	Acc@1  30.81 ( 28.73)
The current update step is 1230
GPU_0_using curriculum 40 with window 40
Epoch: [41][20/30]	Time  3.644 ( 3.684)	Data  0.041 ( 0.052)	InnerLoop  1.544 ( 1.579)	Loss 3.1609e+00 (3.3190e+00)	Acc@1  30.54 ( 29.23)
The current update step is 1260
GPU_0_using curriculum 40 with window 40
Epoch: [42][20/30]	Time  3.657 ( 3.669)	Data  0.038 ( 0.057)	InnerLoop  1.546 ( 1.563)	Loss 3.8139e+00 (3.1835e+00)	Acc@1  25.32 ( 28.53)
The current update step is 1290
GPU_0_using curriculum 40 with window 40
Epoch: [43][20/30]	Time  3.644 ( 3.681)	Data  0.040 ( 0.064)	InnerLoop  1.549 ( 1.562)	Loss 2.9083e+00 (3.0004e+00)	Acc@1  25.68 ( 28.40)
The current update step is 1320
GPU_0_using curriculum 40 with window 40
Epoch: [44][20/30]	Time  3.729 ( 3.663)	Data  0.157 ( 0.064)	InnerLoop  1.522 ( 1.554)	Loss 2.6747e+00 (3.0568e+00)	Acc@1  30.91 ( 29.58)
The current update step is 1350
The current seed is 9217769878019875894
The current lr is: 0.001
Testing Results:
 *   Acc@1 29.908
 *   Acc@1 30.103
 *   Acc@1 30.803
 *   Acc@1 31.312
 *   Acc@1 26.382
 *   Acc@1 26.462
 *   Acc@1 26.408
 *   Acc@1 26.619
 *   Acc@1 27.961
 *   Acc@1 27.170
 *   Acc@1 28.947
 *   Acc@1 27.777
 *   Acc@1 28.013
 *   Acc@1 26.760
 *   Acc@1 29.000
 *   Acc@1 27.649
 *   Acc@1 25.474
 *   Acc@1 25.222
 *   Acc@1 31.355
 *   Acc@1 31.246
 *   Acc@1 26.961
 *   Acc@1 26.800
 *   Acc@1 26.711
 *   Acc@1 26.871
 *   Acc@1 27.276
 *   Acc@1 27.321
 *   Acc@1 26.724
 *   Acc@1 26.784
 *   Acc@1 26.803
 *   Acc@1 26.712
 *   Acc@1 27.079
 *   Acc@1 26.630
Training for 300 epoch: 27.654605263157894
Training for 600 epoch: 29.457236842105264
Training for 1000 epoch: 27.039473684210527
Training for 3000 epoch: 27.29934210526316
Training for 300 epoch: 27.453958333333333
Training for 600 epoch: 29.279583333333335
Training for 1000 epoch: 26.683749999999996
Training for 3000 epoch: 26.942291666666666
[[27.654605263157894, 29.457236842105264, 27.039473684210527, 27.29934210526316], [27.453958333333333, 29.279583333333335, 26.683749999999996, 26.942291666666666]]
train loss 1.6605955117543538, epoch 44, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [45][20/30]	Time  3.756 ( 3.681)	Data  0.038 ( 0.051)	InnerLoop  1.660 ( 1.577)	Loss 4.0277e+00 (3.1756e+00)	Acc@1  25.81 ( 27.62)
The current update step is 1380
GPU_0_using curriculum 40 with window 40
Epoch: [46][20/30]	Time  3.753 ( 3.682)	Data  0.044 ( 0.047)	InnerLoop  1.648 ( 1.583)	Loss 3.1577e+00 (3.3897e+00)	Acc@1  33.81 ( 28.10)
The current update step is 1410
GPU_0_using curriculum 40 with window 40
Epoch: [47][20/30]	Time  3.635 ( 3.675)	Data  0.044 ( 0.065)	InnerLoop  1.526 ( 1.557)	Loss 3.0092e+00 (3.0824e+00)	Acc@1  30.86 ( 28.41)
The current update step is 1440
GPU_0_using curriculum 40 with window 40
Epoch: [48][20/30]	Time  3.627 ( 3.674)	Data  0.039 ( 0.053)	InnerLoop  1.533 ( 1.569)	Loss 3.7483e+00 (3.1052e+00)	Acc@1  25.81 ( 29.74)
The current update step is 1470
GPU_0_using curriculum 40 with window 40
Epoch: [49][20/30]	Time  3.734 ( 3.676)	Data  0.048 ( 0.053)	InnerLoop  1.630 ( 1.569)	Loss 2.8247e+00 (3.2585e+00)	Acc@1  30.40 ( 27.43)
The current update step is 1500
The current seed is 2082571541938116436
The current lr is: 0.001
Testing Results:
 *   Acc@1 26.211
 *   Acc@1 26.477
 *   Acc@1 25.908
 *   Acc@1 25.980
 *   Acc@1 25.553
 *   Acc@1 25.669
 *   Acc@1 24.921
 *   Acc@1 24.997
 *   Acc@1 31.053
 *   Acc@1 30.622
 *   Acc@1 27.066
 *   Acc@1 27.251
 *   Acc@1 27.224
 *   Acc@1 27.268
 *   Acc@1 27.855
 *   Acc@1 27.084
 *   Acc@1 29.224
 *   Acc@1 29.132
 *   Acc@1 29.408
 *   Acc@1 29.284
 *   Acc@1 29.763
 *   Acc@1 29.372
 *   Acc@1 30.987
 *   Acc@1 30.812
 *   Acc@1 25.013
 *   Acc@1 25.012
 *   Acc@1 24.816
 *   Acc@1 24.983
 *   Acc@1 25.158
 *   Acc@1 25.027
 *   Acc@1 26.250
 *   Acc@1 26.129
Training for 300 epoch: 27.875
Training for 600 epoch: 26.799342105263158
Training for 1000 epoch: 26.924342105263158
Training for 3000 epoch: 27.503289473684212
Training for 300 epoch: 27.810416666666665
Training for 600 epoch: 26.874375
Training for 1000 epoch: 26.833958333333335
Training for 3000 epoch: 27.255625
[[27.875, 26.799342105263158, 26.924342105263158, 27.503289473684212], [27.810416666666665, 26.874375, 26.833958333333335, 27.255625]]
train loss 1.6730464864095052, epoch 49, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [50][20/30]	Time  3.627 ( 3.677)	Data  0.039 ( 0.046)	InnerLoop  1.527 ( 1.576)	Loss 2.4167e+00 (2.9833e+00)	Acc@1  27.17 ( 29.40)
The current update step is 1530
GPU_0_using curriculum 40 with window 40
Epoch: [51][20/30]	Time  3.631 ( 3.680)	Data  0.040 ( 0.047)	InnerLoop  1.527 ( 1.575)	Loss 4.3939e+00 (3.7407e+00)	Acc@1  25.61 ( 26.88)
The current update step is 1560
GPU_0_using curriculum 40 with window 40
Epoch: [52][20/30]	Time  3.629 ( 3.677)	Data  0.049 ( 0.061)	InnerLoop  1.523 ( 1.568)	Loss 3.9517e+00 (3.9316e+00)	Acc@1  27.61 ( 26.49)
The current update step is 1590
GPU_0_using curriculum 40 with window 40
Epoch: [53][20/30]	Time  3.752 ( 3.672)	Data  0.163 ( 0.067)	InnerLoop  1.525 ( 1.557)	Loss 3.2710e+00 (4.6015e+00)	Acc@1  25.39 ( 25.17)
The current update step is 1620
GPU_0_using curriculum 40 with window 40
Epoch: [54][20/30]	Time  3.748 ( 3.677)	Data  0.043 ( 0.054)	InnerLoop  1.654 ( 1.574)	Loss 4.0302e+00 (3.8260e+00)	Acc@1  27.59 ( 26.35)
The current update step is 1650
The current seed is 4256036834637804073
The current lr is: 0.001
Testing Results:
 *   Acc@1 24.513
 *   Acc@1 24.411
 *   Acc@1 24.829
 *   Acc@1 24.379
 *   Acc@1 24.632
 *   Acc@1 24.374
 *   Acc@1 23.987
 *   Acc@1 23.927
 *   Acc@1 25.355
 *   Acc@1 25.231
 *   Acc@1 24.855
 *   Acc@1 25.199
 *   Acc@1 25.197
 *   Acc@1 25.493
 *   Acc@1 24.855
 *   Acc@1 25.426
 *   Acc@1 26.474
 *   Acc@1 25.760
 *   Acc@1 26.382
 *   Acc@1 25.884
 *   Acc@1 26.526
 *   Acc@1 25.944
 *   Acc@1 26.618
 *   Acc@1 26.121
 *   Acc@1 23.947
 *   Acc@1 23.698
 *   Acc@1 23.803
 *   Acc@1 23.488
 *   Acc@1 23.553
 *   Acc@1 23.211
 *   Acc@1 23.513
 *   Acc@1 22.846
Training for 300 epoch: 25.07236842105263
Training for 600 epoch: 24.967105263157894
Training for 1000 epoch: 24.976973684210527
Training for 3000 epoch: 24.743421052631575
Training for 300 epoch: 24.77479166666667
Training for 600 epoch: 24.737708333333334
Training for 1000 epoch: 24.755625000000002
Training for 3000 epoch: 24.58
[[25.07236842105263, 24.967105263157894, 24.976973684210527, 24.743421052631575], [24.77479166666667, 24.737708333333334, 24.755625000000002, 24.58]]
train loss 1.5124352300008137, epoch 54, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [55][20/30]	Time  3.742 ( 3.670)	Data  0.046 ( 0.059)	InnerLoop  1.635 ( 1.561)	Loss 3.8284e+00 (3.5637e+00)	Acc@1  24.24 ( 25.05)
The current update step is 1680
GPU_0_using curriculum 40 with window 40
Epoch: [56][20/30]	Time  3.630 ( 3.670)	Data  0.050 ( 0.054)	InnerLoop  1.529 ( 1.568)	Loss 3.2770e+00 (3.4224e+00)	Acc@1  27.27 ( 26.47)
The current update step is 1710
GPU_0_using curriculum 40 with window 40
Epoch: [57][20/30]	Time  3.619 ( 3.680)	Data  0.039 ( 0.059)	InnerLoop  1.529 ( 1.565)	Loss 2.6455e+00 (3.3294e+00)	Acc@1  28.91 ( 27.29)
The current update step is 1740
GPU_0_using curriculum 40 with window 40
Epoch: [58][20/30]	Time  3.643 ( 3.678)	Data  0.040 ( 0.064)	InnerLoop  1.527 ( 1.556)	Loss 3.1496e+00 (3.2156e+00)	Acc@1  21.66 ( 27.37)
The current update step is 1770
GPU_0_using curriculum 40 with window 40
Epoch: [59][20/30]	Time  3.729 ( 3.667)	Data  0.155 ( 0.064)	InnerLoop  1.525 ( 1.553)	Loss 2.8514e+00 (3.5035e+00)	Acc@1  31.32 ( 25.69)
The current update step is 1800
The current seed is 15237378746880257161
The current lr is: 0.001
Testing Results:
 *   Acc@1 29.513
 *   Acc@1 28.764
 *   Acc@1 29.487
 *   Acc@1 29.114
 *   Acc@1 29.355
 *   Acc@1 29.045
 *   Acc@1 29.171
 *   Acc@1 28.331
 *   Acc@1 30.171
 *   Acc@1 30.502
 *   Acc@1 30.342
 *   Acc@1 30.839
 *   Acc@1 31.711
 *   Acc@1 31.925
 *   Acc@1 30.987
 *   Acc@1 30.894
 *   Acc@1 28.276
 *   Acc@1 28.247
 *   Acc@1 28.566
 *   Acc@1 28.133
 *   Acc@1 28.184
 *   Acc@1 27.984
 *   Acc@1 28.342
 *   Acc@1 28.317
 *   Acc@1 25.105
 *   Acc@1 24.823
 *   Acc@1 25.947
 *   Acc@1 25.595
 *   Acc@1 25.921
 *   Acc@1 25.768
 *   Acc@1 26.974
 *   Acc@1 26.789
Training for 300 epoch: 28.266447368421055
Training for 600 epoch: 28.585526315789473
Training for 1000 epoch: 28.792763157894736
Training for 3000 epoch: 28.86842105263158
Training for 300 epoch: 28.083750000000002
Training for 600 epoch: 28.420208333333335
Training for 1000 epoch: 28.680625
Training for 3000 epoch: 28.582708333333336
[[28.266447368421055, 28.585526315789473, 28.792763157894736, 28.86842105263158], [28.083750000000002, 28.420208333333335, 28.680625, 28.582708333333336]]
train loss 1.1822740367889404, epoch 59, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [60][20/30]	Time  3.751 ( 3.677)	Data  0.043 ( 0.051)	InnerLoop  1.638 ( 1.576)	Loss 3.5105e+00 (3.2818e+00)	Acc@1  26.88 ( 28.16)
The current update step is 1830
GPU_0_using curriculum 40 with window 40
Epoch: [61][20/30]	Time  3.731 ( 3.667)	Data  0.044 ( 0.045)	InnerLoop  1.633 ( 1.573)	Loss 2.9673e+00 (3.1409e+00)	Acc@1  27.81 ( 27.95)
The current update step is 1860
GPU_0_using curriculum 40 with window 40
Epoch: [62][20/30]	Time  3.616 ( 3.666)	Data  0.043 ( 0.064)	InnerLoop  1.527 ( 1.551)	Loss 3.6176e+00 (3.1115e+00)	Acc@1  23.10 ( 27.86)
The current update step is 1890
GPU_0_using curriculum 40 with window 40
Epoch: [63][20/30]	Time  3.654 ( 3.694)	Data  0.044 ( 0.056)	InnerLoop  1.538 ( 1.570)	Loss 4.7555e+00 (3.6018e+00)	Acc@1  24.71 ( 24.99)
The current update step is 1920
GPU_0_using curriculum 40 with window 40
Epoch: [64][20/30]	Time  3.784 ( 3.695)	Data  0.049 ( 0.056)	InnerLoop  1.656 ( 1.574)	Loss 2.7129e+00 (3.5514e+00)	Acc@1  25.49 ( 24.12)
The current update step is 1950
The current seed is 17514783588916454840
The current lr is: 0.001
Testing Results:
 *   Acc@1 26.237
 *   Acc@1 26.082
 *   Acc@1 25.197
 *   Acc@1 25.323
 *   Acc@1 24.566
 *   Acc@1 24.635
 *   Acc@1 23.658
 *   Acc@1 23.795
 *   Acc@1 20.605
 *   Acc@1 21.046
 *   Acc@1 19.763
 *   Acc@1 20.486
 *   Acc@1 19.684
 *   Acc@1 19.974
 *   Acc@1 19.197
 *   Acc@1 19.362
 *   Acc@1 26.250
 *   Acc@1 26.920
 *   Acc@1 26.382
 *   Acc@1 26.927
 *   Acc@1 26.145
 *   Acc@1 26.822
 *   Acc@1 25.421
 *   Acc@1 26.207
 *   Acc@1 20.803
 *   Acc@1 21.282
 *   Acc@1 21.132
 *   Acc@1 21.036
 *   Acc@1 20.882
 *   Acc@1 20.951
 *   Acc@1 20.750
 *   Acc@1 20.622
Training for 300 epoch: 23.473684210526315
Training for 600 epoch: 23.11842105263158
Training for 1000 epoch: 22.81907894736842
Training for 3000 epoch: 22.25657894736842
Training for 300 epoch: 23.8325
Training for 600 epoch: 23.442916666666665
Training for 1000 epoch: 23.09541666666667
Training for 3000 epoch: 22.496666666666666
[[23.473684210526315, 23.11842105263158, 22.81907894736842, 22.25657894736842], [23.8325, 23.442916666666665, 23.09541666666667, 22.496666666666666]]
train loss 1.3977740741729736, epoch 64, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [65][20/30]	Time  3.660 ( 3.693)	Data  0.045 ( 0.049)	InnerLoop  1.543 ( 1.577)	Loss 2.5950e+00 (3.1514e+00)	Acc@1  19.87 ( 24.90)
The current update step is 1980
GPU_0_using curriculum 40 with window 40
Epoch: [66][20/30]	Time  3.654 ( 3.707)	Data  0.043 ( 0.050)	InnerLoop  1.541 ( 1.585)	Loss 3.2257e+00 (3.4207e+00)	Acc@1  25.10 ( 26.05)
The current update step is 2010
GPU_0_using curriculum 40 with window 40
Epoch: [67][20/30]	Time  3.664 ( 3.703)	Data  0.043 ( 0.061)	InnerLoop  1.546 ( 1.574)	Loss 4.1543e+00 (3.4740e+00)	Acc@1  25.46 ( 26.76)
The current update step is 2040
GPU_0_using curriculum 40 with window 40
Epoch: [68][20/30]	Time  3.785 ( 3.696)	Data  0.167 ( 0.068)	InnerLoop  1.545 ( 1.564)	Loss 2.8274e+00 (3.1673e+00)	Acc@1  24.63 ( 27.52)
The current update step is 2070
GPU_0_using curriculum 40 with window 40
Epoch: [69][20/30]	Time  3.769 ( 3.710)	Data  0.043 ( 0.055)	InnerLoop  1.660 ( 1.588)	Loss 3.1773e+00 (3.0793e+00)	Acc@1  23.41 ( 27.29)
The current update step is 2100
The current seed is 7461058814231580715
The current lr is: 0.001
Testing Results:
 *   Acc@1 25.868
 *   Acc@1 25.501
 *   Acc@1 25.500
 *   Acc@1 25.333
 *   Acc@1 25.789
 *   Acc@1 25.385
 *   Acc@1 25.368
 *   Acc@1 25.295
 *   Acc@1 28.171
 *   Acc@1 28.215
 *   Acc@1 27.303
 *   Acc@1 27.395
 *   Acc@1 27.526
 *   Acc@1 27.144
 *   Acc@1 26.961
 *   Acc@1 26.822
 *   Acc@1 32.974
 *   Acc@1 32.770
 *   Acc@1 33.105
 *   Acc@1 32.488
 *   Acc@1 32.658
 *   Acc@1 32.355
 *   Acc@1 31.934
 *   Acc@1 32.017
 *   Acc@1 24.855
 *   Acc@1 24.913
 *   Acc@1 24.737
 *   Acc@1 24.900
 *   Acc@1 24.868
 *   Acc@1 24.883
 *   Acc@1 25.132
 *   Acc@1 25.140
Training for 300 epoch: 27.967105263157897
Training for 600 epoch: 27.661184210526315
Training for 1000 epoch: 27.710526315789473
Training for 3000 epoch: 27.34868421052632
Training for 300 epoch: 27.84979166666667
Training for 600 epoch: 27.52916666666667
Training for 1000 epoch: 27.441874999999996
Training for 3000 epoch: 27.31833333333333
[[27.967105263157897, 27.661184210526315, 27.710526315789473, 27.34868421052632], [27.84979166666667, 27.52916666666667, 27.441874999999996, 27.31833333333333]]
train loss 1.634431576093038, epoch 69, best loss 1.132494519106547, best_epoch 14
GPU_0_using curriculum 40 with window 40
Epoch: [70][20/30]	Time  3.769 ( 3.689)	Data  0.040 ( 0.058)	InnerLoop  1.667 ( 1.575)	Loss 2.7875e+00 (3.1194e+00)	Acc@1  25.56 ( 26.39)
The current update step is 2130
GPU_0_using curriculum 40 with window 40
Epoch: [71][20/30]	Time  3.640 ( 3.680)	Data  0.045 ( 0.052)	InnerLoop  1.531 ( 1.575)	Loss 3.8109e+00 (3.1507e+00)	Acc@1  25.12 ( 28.73)
The current update step is 2160
GPU_0_using curriculum 40 with window 40
Epoch: [72][20/30]	Time  3.625 ( 3.677)	Data  0.037 ( 0.060)	InnerLoop  1.537 ( 1.565)	Loss 3.4563e+00 (2.8419e+00)	Acc@1  27.29 ( 29.22)
The current update step is 2190
GPU_0_using curriculum 40 with window 40
Epoch: [73][20/30]	Time  3.625 ( 3.673)	Data  0.046 ( 0.066)	InnerLoop  1.525 ( 1.555)	Loss 2.4834e+00 (2.8349e+00)	Acc@1  32.47 ( 29.04)
The current update step is 2220
GPU_0_using curriculum 40 with window 40
Epoch: [74][20/30]	Time  3.733 ( 3.676)	Data  0.156 ( 0.065)	InnerLoop  1.512 ( 1.559)	Loss 2.2045e+00 (2.8706e+00)	Acc@1  27.83 ( 29.41)
The current update step is 2250
The current seed is 2513746661924892566
The current lr is: 0.001
Testing Results:
 *   Acc@1 17.487
 *   Acc@1 16.979
 *   Acc@1 17.855
 *   Acc@1 17.650
 *   Acc@1 17.855
 *   Acc@1 17.972
 *   Acc@1 18.066
 *   Acc@1 18.155
 *   Acc@1 33.921
 *   Acc@1 33.417
 *   Acc@1 34.316
 *   Acc@1 33.496
 *   Acc@1 33.395
 *   Acc@1 33.254
 *   Acc@1 33.105
 *   Acc@1 32.883
 *   Acc@1 32.789
 *   Acc@1 32.456
 *   Acc@1 33.013
 *   Acc@1 32.598
 *   Acc@1 32.474
 *   Acc@1 32.658
 *   Acc@1 32.329
 *   Acc@1 32.390
 *   Acc@1 28.474
 *   Acc@1 28.518
 *   Acc@1 29.145
 *   Acc@1 29.363
 *   Acc@1 28.868
 *   Acc@1 29.571
 *   Acc@1 32.605
 *   Acc@1 32.640
Training for 300 epoch: 28.167763157894736
Training for 600 epoch: 28.58223684210526
Training for 1000 epoch: 28.148026315789473
Training for 3000 epoch: 29.026315789473685
Training for 300 epoch: 27.842499999999998
Training for 600 epoch: 28.276875
Training for 1000 epoch: 28.363750000000003
Training for 3000 epoch: 29.016875000000002
[[28.167763157894736, 28.58223684210526, 28.148026315789473, 29.026315789473685], [27.842499999999998, 28.276875, 28.363750000000003, 29.016875000000002]]
train loss 1.3684167364756266, epoch 74, best loss 1.132494519106547, best_epoch 74
GPU_0_using curriculum 40 with window 40
Epoch: [75][20/30]	Time  3.750 ( 3.676)	Data  0.040 ( 0.051)	InnerLoop  1.656 ( 1.572)	Loss 2.9797e+00 (3.0006e+00)	Acc@1  28.96 ( 28.52)
The current update step is 2280
GPU_0_using curriculum 40 with window 40
Epoch: [76][20/30]	Time  3.725 ( 3.673)	Data  0.043 ( 0.046)	InnerLoop  1.625 ( 1.575)	Loss 2.1238e+00 (2.9123e+00)	Acc@1  30.69 ( 29.20)
The current update step is 2310
GPU_0_using curriculum 40 with window 40
Epoch: [77][20/30]	Time  3.636 ( 3.669)	Data  0.044 ( 0.063)	InnerLoop  1.529 ( 1.552)	Loss 3.8524e+00 (2.8425e+00)	Acc@1  22.66 ( 28.14)
The current update step is 2340
GPU_0_using curriculum 40 with window 40
Epoch: [78][20/30]	Time  3.623 ( 3.674)	Data  0.039 ( 0.051)	InnerLoop  1.526 ( 1.570)	Loss 2.8372e+00 (2.8671e+00)	Acc@1  30.57 ( 28.50)
The current update step is 2370
GPU_0_using curriculum 40 with window 40
Epoch: [79][20/30]	Time  3.729 ( 3.671)	Data  0.043 ( 0.052)	InnerLoop  1.630 ( 1.565)	Loss 2.2765e+00 (2.7637e+00)	Acc@1  30.79 ( 29.35)
The current update step is 2400
The current seed is 1960117069840447194
The current lr is: 0.001
Testing Results:
 *   Acc@1 33.211
 *   Acc@1 33.926
 *   Acc@1 33.868
 *   Acc@1 34.358
 *   Acc@1 33.816
 *   Acc@1 34.468
 *   Acc@1 34.158
 *   Acc@1 35.006
 *   Acc@1 28.553
 *   Acc@1 28.337
 *   Acc@1 28.526
 *   Acc@1 28.769
 *   Acc@1 29.776
 *   Acc@1 29.073
 *   Acc@1 30.197
 *   Acc@1 29.650
 *   Acc@1 30.763
 *   Acc@1 31.422
 *   Acc@1 30.763
 *   Acc@1 30.987
 *   Acc@1 30.079
 *   Acc@1 30.832
 *   Acc@1 31.737
 *   Acc@1 32.262
 *   Acc@1 28.592
 *   Acc@1 28.917
 *   Acc@1 28.434
 *   Acc@1 29.331
 *   Acc@1 29.171
 *   Acc@1 29.566
 *   Acc@1 28.434
 *   Acc@1 28.269
Training for 300 epoch: 30.279605263157894
Training for 600 epoch: 30.398026315789476
Training for 1000 epoch: 30.710526315789473
Training for 3000 epoch: 31.13157894736842
Training for 300 epoch: 30.650208333333335
Training for 600 epoch: 30.86125
Training for 1000 epoch: 30.984375
Training for 3000 epoch: 31.296666666666663
[[30.279605263157894, 30.398026315789476, 30.710526315789473, 31.13157894736842], [30.650208333333335, 30.86125, 30.984375, 31.296666666666663]]
train loss 1.1509764715194701, epoch 79, best loss 1.132494519106547, best_epoch 74
GPU_0_using curriculum 40 with window 40
Epoch: [80][20/30]	Time  3.615 ( 3.672)	Data  0.039 ( 0.046)	InnerLoop  1.531 ( 1.576)	Loss 3.1962e+00 (2.7745e+00)	Acc@1  27.05 ( 31.21)
The current update step is 2430
GPU_0_using curriculum 40 with window 40
Epoch: [81][20/30]	Time  3.622 ( 3.673)	Data  0.039 ( 0.047)	InnerLoop  1.524 ( 1.574)	Loss 2.6657e+00 (3.0216e+00)	Acc@1  30.22 ( 30.02)
The current update step is 2460
GPU_0_using curriculum 40 with window 40
Epoch: [82][20/30]	Time  3.627 ( 3.672)	Data  0.041 ( 0.059)	InnerLoop  1.527 ( 1.562)	Loss 2.4583e+00 (3.0401e+00)	Acc@1  30.74 ( 30.11)
The current update step is 2490
GPU_0_using curriculum 40 with window 40
Epoch: [83][20/30]	Time  3.729 ( 3.670)	Data  0.158 ( 0.064)	InnerLoop  1.521 ( 1.555)	Loss 3.8953e+00 (2.7152e+00)	Acc@1  29.79 ( 31.41)
The current update step is 2520
GPU_0_using curriculum 40 with window 40
Epoch: [84][20/30]	Time  3.749 ( 3.682)	Data  0.039 ( 0.051)	InnerLoop  1.647 ( 1.577)	Loss 2.1941e+00 (2.6037e+00)	Acc@1  38.75 ( 33.08)
The current update step is 2550
The current seed is 15975912047646316817
The current lr is: 0.001
Testing Results:
 *   Acc@1 35.539
 *   Acc@1 35.983
 *   Acc@1 35.750
 *   Acc@1 36.216
 *   Acc@1 35.184
 *   Acc@1 35.903
 *   Acc@1 36.303
 *   Acc@1 36.240
 *   Acc@1 34.316
 *   Acc@1 33.966
 *   Acc@1 37.066
 *   Acc@1 36.850
 *   Acc@1 36.158
 *   Acc@1 36.239
 *   Acc@1 35.974
 *   Acc@1 36.349
 *   Acc@1 33.250
 *   Acc@1 33.638
 *   Acc@1 33.421
 *   Acc@1 33.960
 *   Acc@1 33.408
 *   Acc@1 34.021
 *   Acc@1 33.092
 *   Acc@1 33.506
 *   Acc@1 32.987
 *   Acc@1 32.563
 *   Acc@1 32.316
 *   Acc@1 32.250
 *   Acc@1 32.908
 *   Acc@1 32.757
 *   Acc@1 32.434
 *   Acc@1 32.197
Training for 300 epoch: 34.02302631578947
Training for 600 epoch: 34.63815789473684
Training for 1000 epoch: 34.41447368421053
Training for 3000 epoch: 34.45065789473684
Training for 300 epoch: 34.0375
Training for 600 epoch: 34.818958333333335
Training for 1000 epoch: 34.72979166666667
Training for 3000 epoch: 34.573125
[[34.02302631578947, 34.63815789473684, 34.41447368421053, 34.45065789473684], [34.0375, 34.818958333333335, 34.72979166666667, 34.573125]]
train loss 1.1012816294352215, epoch 84, best loss 1.1012816294352215, best_epoch 84
GPU_0_using curriculum 40 with window 40
Epoch: [85][20/30]	Time  3.757 ( 3.681)	Data  0.038 ( 0.057)	InnerLoop  1.654 ( 1.569)	Loss 1.7881e+00 (2.7332e+00)	Acc@1  39.31 ( 32.54)
The current update step is 2580
GPU_0_using curriculum 40 with window 40
Epoch: [86][20/30]	Time  3.632 ( 3.672)	Data  0.041 ( 0.052)	InnerLoop  1.526 ( 1.566)	Loss 2.2655e+00 (2.8838e+00)	Acc@1  36.72 ( 32.70)
The current update step is 2610
GPU_0_using curriculum 40 with window 40
Epoch: [87][20/30]	Time  3.635 ( 3.669)	Data  0.044 ( 0.059)	InnerLoop  1.522 ( 1.558)	Loss 2.3803e+00 (2.6993e+00)	Acc@1  30.27 ( 34.90)
The current update step is 2640
GPU_0_using curriculum 40 with window 40
Epoch: [88][20/30]	Time  3.632 ( 3.668)	Data  0.040 ( 0.063)	InnerLoop  1.523 ( 1.554)	Loss 3.6918e+00 (2.8105e+00)	Acc@1  35.40 ( 35.67)
The current update step is 2670
GPU_0_using curriculum 40 with window 40
Epoch: [89][20/30]	Time  3.753 ( 3.673)	Data  0.159 ( 0.064)	InnerLoop  1.519 ( 1.554)	Loss 2.3408e+00 (2.5041e+00)	Acc@1  40.67 ( 35.77)
The current update step is 2700
The current seed is 1885590879048389526
The current lr is: 0.001
Testing Results:
 *   Acc@1 31.539
 *   Acc@1 31.886
 *   Acc@1 26.776
 *   Acc@1 27.033
 *   Acc@1 26.908
 *   Acc@1 27.211
 *   Acc@1 27.184
 *   Acc@1 27.812
 *   Acc@1 35.553
 *   Acc@1 35.929
 *   Acc@1 35.882
 *   Acc@1 36.208
 *   Acc@1 36.092
 *   Acc@1 36.211
 *   Acc@1 36.632
 *   Acc@1 36.679
 *   Acc@1 32.645
 *   Acc@1 32.962
 *   Acc@1 33.632
 *   Acc@1 34.023
 *   Acc@1 34.605
 *   Acc@1 34.737
 *   Acc@1 35.105
 *   Acc@1 35.101
 *   Acc@1 30.211
 *   Acc@1 29.956
 *   Acc@1 28.447
 *   Acc@1 29.137
 *   Acc@1 29.105
 *   Acc@1 29.067
 *   Acc@1 29.263
 *   Acc@1 29.343
Training for 300 epoch: 32.48684210526316
Training for 600 epoch: 31.184210526315788
Training for 1000 epoch: 31.67763157894737
Training for 3000 epoch: 32.046052631578945
Training for 300 epoch: 32.68333333333334
Training for 600 epoch: 31.600208333333338
Training for 1000 epoch: 31.80645833333333
Training for 3000 epoch: 32.23375
[[32.48684210526316, 31.184210526315788, 31.67763157894737, 32.046052631578945], [32.68333333333334, 31.600208333333338, 31.80645833333333, 32.23375]]
train loss 1.3464526600519815, epoch 89, best loss 1.1012816294352215, best_epoch 84
GPU_0_using curriculum 40 with window 40
Epoch: [90][20/30]	Time  3.761 ( 3.690)	Data  0.046 ( 0.053)	InnerLoop  1.653 ( 1.578)	Loss 2.2877e+00 (2.7795e+00)	Acc@1  37.13 ( 34.81)
The current update step is 2730
GPU_0_using curriculum 40 with window 40
Epoch: [91][20/30]	Time  3.751 ( 3.690)	Data  0.047 ( 0.047)	InnerLoop  1.648 ( 1.585)	Loss 2.5705e+00 (2.8037e+00)	Acc@1  35.52 ( 34.79)
The current update step is 2760
GPU_0_using curriculum 40 with window 40
Epoch: [92][20/30]	Time  3.627 ( 3.685)	Data  0.043 ( 0.064)	InnerLoop  1.526 ( 1.561)	Loss 2.4826e+00 (2.8204e+00)	Acc@1  38.67 ( 33.55)
The current update step is 2790
GPU_0_using curriculum 40 with window 40
Epoch: [93][20/30]	Time  3.629 ( 3.677)	Data  0.041 ( 0.052)	InnerLoop  1.521 ( 1.568)	Loss 2.7013e+00 (3.1471e+00)	Acc@1  34.89 ( 31.79)
The current update step is 2820
GPU_0_using curriculum 40 with window 40
Epoch: [94][20/30]	Time  3.762 ( 3.680)	Data  0.043 ( 0.052)	InnerLoop  1.670 ( 1.576)	Loss 2.0442e+00 (2.7779e+00)	Acc@1  38.77 ( 33.24)
The current update step is 2850
The current seed is 12519345170736502637
The current lr is: 0.001
Testing Results:
 *   Acc@1 31.816
 *   Acc@1 32.166
 *   Acc@1 32.145
 *   Acc@1 31.372
 *   Acc@1 30.737
 *   Acc@1 30.622
 *   Acc@1 30.658
 *   Acc@1 30.567
 *   Acc@1 35.671
 *   Acc@1 36.104
 *   Acc@1 35.671
 *   Acc@1 35.779
 *   Acc@1 35.618
 *   Acc@1 35.557
 *   Acc@1 36.092
 *   Acc@1 36.502
 *   Acc@1 31.553
 *   Acc@1 31.754
 *   Acc@1 32.395
 *   Acc@1 32.269
 *   Acc@1 32.934
 *   Acc@1 32.843
 *   Acc@1 33.118
 *   Acc@1 33.139
 *   Acc@1 31.987
 *   Acc@1 31.560
 *   Acc@1 33.434
 *   Acc@1 33.325
 *   Acc@1 34.263
 *   Acc@1 34.185
 *   Acc@1 34.789
 *   Acc@1 35.070
Training for 300 epoch: 32.75657894736842
Training for 600 epoch: 33.411184210526315
Training for 1000 epoch: 33.38815789473684
Training for 3000 epoch: 33.66447368421053
Training for 300 epoch: 32.89604166666666
Training for 600 epoch: 33.186458333333334
Training for 1000 epoch: 33.30166666666666
Training for 3000 epoch: 33.81979166666667
[[32.75657894736842, 33.411184210526315, 33.38815789473684, 33.66447368421053], [32.89604166666666, 33.186458333333334, 33.30166666666666, 33.81979166666667]]
train loss 1.2619215115865072, epoch 94, best loss 1.1012816294352215, best_epoch 84
GPU_0_using curriculum 40 with window 40
Epoch: [95][20/30]	Time  3.640 ( 3.685)	Data  0.039 ( 0.045)	InnerLoop  1.539 ( 1.586)	Loss 2.8929e+00 (3.0301e+00)	Acc@1  32.69 ( 31.65)
The current update step is 2880
GPU_0_using curriculum 40 with window 40
Epoch: [96][20/30]	Time  3.626 ( 3.668)	Data  0.040 ( 0.048)	InnerLoop  1.530 ( 1.571)	Loss 2.9145e+00 (2.7479e+00)	Acc@1  28.17 ( 33.96)
The current update step is 2910
GPU_0_using curriculum 40 with window 40
Epoch: [97][20/30]	Time  3.622 ( 3.669)	Data  0.043 ( 0.057)	InnerLoop  1.522 ( 1.561)	Loss 2.8551e+00 (2.6649e+00)	Acc@1  32.25 ( 32.63)
The current update step is 2940
GPU_0_using curriculum 40 with window 40
Epoch: [98][20/30]	Time  3.738 ( 3.668)	Data  0.152 ( 0.065)	InnerLoop  1.521 ( 1.553)	Loss 2.2918e+00 (2.8127e+00)	Acc@1  42.94 ( 35.53)
The current update step is 2970
GPU_0_using curriculum 40 with window 40
Epoch: [99][20/30]	Time  3.745 ( 3.680)	Data  0.043 ( 0.052)	InnerLoop  1.644 ( 1.577)	Loss 2.6806e+00 (2.8002e+00)	Acc@1  30.57 ( 34.42)
The current update step is 3000
The current seed is 18387166330820942456
The current lr is: 0.001
Testing Results:
 *   Acc@1 34.487
 *   Acc@1 34.401
 *   Acc@1 32.671
 *   Acc@1 33.002
 *   Acc@1 30.434
 *   Acc@1 31.003
 *   Acc@1 28.408
 *   Acc@1 28.720
 *   Acc@1 31.987
 *   Acc@1 32.477
 *   Acc@1 32.737
 *   Acc@1 32.833
 *   Acc@1 32.882
 *   Acc@1 33.128
 *   Acc@1 33.395
 *   Acc@1 33.552
 *   Acc@1 35.224
 *   Acc@1 34.856
 *   Acc@1 35.342
 *   Acc@1 35.097
 *   Acc@1 34.842
 *   Acc@1 34.672
 *   Acc@1 33.987
 *   Acc@1 33.553
 *   Acc@1 33.395
 *   Acc@1 32.972
 *   Acc@1 34.105
 *   Acc@1 33.694
 *   Acc@1 34.263
 *   Acc@1 34.015
 *   Acc@1 35.118
 *   Acc@1 34.912
Training for 300 epoch: 33.77302631578947
Training for 600 epoch: 33.713815789473685
Training for 1000 epoch: 33.10526315789474
Training for 3000 epoch: 32.72697368421053
Training for 300 epoch: 33.67645833333333
Training for 600 epoch: 33.656666666666666
Training for 1000 epoch: 33.20458333333333
Training for 3000 epoch: 32.68416666666667
[[33.77302631578947, 33.713815789473685, 33.10526315789474, 32.72697368421053], [33.67645833333333, 33.656666666666666, 33.20458333333333, 32.68416666666667]]
train loss 1.3707402395884196, epoch 99, best loss 1.1012816294352215, best_epoch 84
=== Final results:
{'acc': 34.63815789473684, 'test': [34.02302631578947, 34.63815789473684, 34.41447368421053, 34.45065789473684], 'train': [34.02302631578947, 34.63815789473684, 34.41447368421053, 34.45065789473684], 'ind': 1, 'epoch': 85, 'data': array([[-0.10693915, -0.08585519, -0.02299448, ..., -0.01323222,
         0.00476633, -0.01016677],
       [ 0.03173617,  0.00961216,  0.03248652, ..., -0.0485623 ,
        -0.00045997, -0.03535232],
       [-0.04634596,  0.02019585, -0.03362101, ...,  0.00655773,
         0.00532564, -0.03301594],
       ...,
       [ 0.02836041,  0.08681305,  0.07179818, ..., -0.04201717,
        -0.09460931,  0.09937043],
       [-0.03617921,  0.06597584,  0.04598571, ...,  0.02086292,
         0.04819997,  0.04287922],
       [-0.08784715, -0.00099122,  0.00647962, ..., -0.06648173,
        -0.06771677, -0.03636037]], shape=(200, 768), dtype=float32)}
