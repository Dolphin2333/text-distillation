Torch Seed Specified with rank: 0
Dataset: agnews_emb
Dataset Path: ./scripts
Namespace(seed=0, mp_distributed=False, world_size=1, rank=0, dist_url='tcp://224.66.41.62:23456', dist_backend='nccl', workers=0, gpu=None, root='./scripts', dataset='agnews_emb', arch='text_transformer', width=256, lr=0.0015, inner_optim='Adam', outer_optim='Adam', inner_lr=0.0012, label_lr_scale=1, num_per_class=50, batch_per_class=20, task_sampler_nc=8, window=20, minwindow=0, totwindow=40, num_train_eval=2, train_y=False, batch_size=4096, eps=1e-08, wd=0, test_freq=5, print_freq=20, start_epoch=0, epochs=600, ddtype='standard', cctype=0, zca=False, wandb=False, clip_coef=0.9, fname='out_tf_ratbptt_ipc50_s4_w20t40_ep600', out_dir='./checkpoints', name='agnews_tf_ratbptt_s4_w20t40_ep600', comp_aug=False, comp_aug_real=False, syn_strategy='none', real_strategy='none', ckptname='none', limit_train=False, load_ckpt=False, complete_random=False, boost_dd=False, boost_init_from='none', boost_beta=1.0, stage=4, distributed=False, data_root='./scripts/agnews_emb')
==> Preparing data..
None None
Dataset: number of classes: 4
Training set size: 120000
Image size: channel 1, height 768, width 1
Synthetic images, not_single False, keys []
==> Building model..
Initialized data with size, x: torch.Size([200, 768]), y:torch.Size([200])
TextTransformer(
  (input_proj): Linear(in_features=192, out_features=256, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls_head): Linear(in_features=256, out_features=4, bias=True)
)
use data parallel only
GPU_0_using curriculum 20 with window 20
Epoch: [0][20/30]	Time  1.622 ( 1.734)	Data  0.040 ( 0.058)	InnerLoop  0.685 ( 0.757)	Loss 2.1131e+00 (2.9032e+00)	Acc@1  38.94 ( 33.27)
The current update step is 30
GPU_0_using curriculum 20 with window 20
Epoch: [1][20/30]	Time  1.602 ( 1.637)	Data  0.043 ( 0.066)	InnerLoop  0.663 ( 0.678)	Loss 1.1175e+00 (1.2929e+00)	Acc@1  56.35 ( 57.27)
The current update step is 60
GPU_0_using curriculum 20 with window 20
Epoch: [2][20/30]	Time  1.726 ( 1.633)	Data  0.042 ( 0.072)	InnerLoop  0.782 ( 0.674)	Loss 1.2870e+00 (1.4451e+00)	Acc@1  55.86 ( 55.32)
The current update step is 90
GPU_0_using curriculum 20 with window 20
Epoch: [3][20/30]	Time  1.621 ( 1.623)	Data  0.042 ( 0.055)	InnerLoop  0.672 ( 0.683)	Loss 8.5159e-01 (8.9536e-01)	Acc@1  66.99 ( 67.86)
The current update step is 120
GPU_0_using curriculum 20 with window 20
Epoch: [4][20/30]	Time  1.582 ( 1.629)	Data  0.040 ( 0.062)	InnerLoop  0.658 ( 0.680)	Loss 9.9135e-01 (7.8307e-01)	Acc@1  69.90 ( 72.84)
The current update step is 150
The current seed is 13817571712191050035
The current lr is: 0.0012
Testing Results:
 *   Acc@1 52.132
 *   Acc@1 53.370
 *   Acc@1 52.882
 *   Acc@1 53.873
 *   Acc@1 52.724
 *   Acc@1 53.504
 *   Acc@1 38.605
 *   Acc@1 39.186
 *   Acc@1 39.447
 *   Acc@1 40.147
 *   Acc@1 41.092
 *   Acc@1 41.529
Training for 300 epoch: 45.368421052631575
Training for 600 epoch: 46.16447368421052
Training for 1000 epoch: 46.90789473684211
Training for 300 epoch: 46.27791666666667
Training for 600 epoch: 47.010000000000005
Training for 1000 epoch: 47.516666666666666
[[45.368421052631575, 46.16447368421052, 46.90789473684211], [46.27791666666667, 47.010000000000005, 47.516666666666666]]
train loss 1.9185452165603638, epoch 4, best loss 1.9185452165603638, best_epoch 4
GPU_0_using curriculum 20 with window 20
Epoch: [5][20/30]	Time  1.719 ( 1.629)	Data  0.163 ( 0.071)	InnerLoop  0.668 ( 0.672)	Loss 8.0868e-01 (9.4973e-01)	Acc@1  72.00 ( 67.87)
The current update step is 180
GPU_0_using curriculum 20 with window 20
Epoch: [6][20/30]	Time  1.704 ( 1.630)	Data  0.042 ( 0.060)	InnerLoop  0.784 ( 0.686)	Loss 8.7475e-01 (9.5522e-01)	Acc@1  69.12 ( 66.79)
The current update step is 210
GPU_0_using curriculum 20 with window 20
Epoch: [7][20/30]	Time  1.539 ( 1.583)	Data  0.040 ( 0.052)	InnerLoop  0.648 ( 0.667)	Loss 7.0422e-01 (8.1674e-01)	Acc@1  75.37 ( 70.34)
The current update step is 240
GPU_0_using curriculum 20 with window 20
Epoch: [8][20/30]	Time  1.543 ( 1.579)	Data  0.040 ( 0.059)	InnerLoop  0.645 ( 0.662)	Loss 6.3073e-01 (7.1089e-01)	Acc@1  74.95 ( 75.24)
The current update step is 270
GPU_0_using curriculum 20 with window 20
Epoch: [9][20/30]	Time  1.534 ( 1.569)	Data  0.040 ( 0.045)	InnerLoop  0.651 ( 0.668)	Loss 6.4549e-01 (7.2286e-01)	Acc@1  78.25 ( 73.09)
The current update step is 300
The current seed is 3993531780855083617
The current lr is: 0.0012
Testing Results:
 *   Acc@1 25.211
 *   Acc@1 24.797
 *   Acc@1 25.197
 *   Acc@1 24.781
 *   Acc@1 24.829
 *   Acc@1 24.615
 *   Acc@1 29.263
 *   Acc@1 29.246
 *   Acc@1 30.171
 *   Acc@1 30.223
 *   Acc@1 31.263
 *   Acc@1 31.237
Training for 300 epoch: 27.236842105263158
Training for 600 epoch: 27.684210526315788
Training for 1000 epoch: 28.046052631578945
Training for 300 epoch: 27.021250000000002
Training for 600 epoch: 27.501666666666665
Training for 1000 epoch: 27.925833333333333
[[27.236842105263158, 27.684210526315788, 28.046052631578945], [27.021250000000002, 27.501666666666665, 27.925833333333333]]
train loss 2.265523844273885, epoch 9, best loss 1.9185452165603638, best_epoch 4
GPU_0_using curriculum 20 with window 20
Epoch: [10][20/30]	Time  1.664 ( 1.572)	Data  0.160 ( 0.070)	InnerLoop  0.653 ( 0.651)	Loss 6.9453e-01 (8.2084e-01)	Acc@1  72.53 ( 72.15)
The current update step is 330
GPU_0_using curriculum 20 with window 20
Epoch: [11][20/30]	Time  1.661 ( 1.572)	Data  0.038 ( 0.057)	InnerLoop  0.762 ( 0.664)	Loss 6.6659e-01 (7.1862e-01)	Acc@1  77.12 ( 74.12)
The current update step is 360
GPU_0_using curriculum 20 with window 20
Epoch: [12][20/30]	Time  1.547 ( 1.565)	Data  0.042 ( 0.051)	InnerLoop  0.656 ( 0.662)	Loss 8.5479e-01 (7.8363e-01)	Acc@1  69.34 ( 71.52)
The current update step is 390
GPU_0_using curriculum 20 with window 20
Epoch: [13][20/30]	Time  1.577 ( 1.568)	Data  0.041 ( 0.058)	InnerLoop  0.646 ( 0.655)	Loss 6.2600e-01 (7.1343e-01)	Acc@1  76.95 ( 73.68)
The current update step is 420
GPU_0_using curriculum 20 with window 20
Epoch: [14][20/30]	Time  1.524 ( 1.566)	Data  0.039 ( 0.046)	InnerLoop  0.639 ( 0.669)	Loss 5.9193e-01 (6.6170e-01)	Acc@1  78.47 ( 75.57)
The current update step is 450
The current seed is 9807269361956389421
The current lr is: 0.0012
Testing Results:
 *   Acc@1 53.000
 *   Acc@1 53.339
 *   Acc@1 54.408
 *   Acc@1 54.434
 *   Acc@1 54.947
 *   Acc@1 54.933
 *   Acc@1 49.303
 *   Acc@1 49.515
 *   Acc@1 51.276
 *   Acc@1 51.651
 *   Acc@1 53.224
 *   Acc@1 53.521
Training for 300 epoch: 51.151315789473685
Training for 600 epoch: 52.84210526315789
Training for 1000 epoch: 54.08552631578947
Training for 300 epoch: 51.42708333333333
Training for 600 epoch: 53.042500000000004
Training for 1000 epoch: 54.22708333333333
[[51.151315789473685, 52.84210526315789, 54.08552631578947], [51.42708333333333, 53.042500000000004, 54.22708333333333]]
train loss 1.4099975123087565, epoch 14, best loss 1.4099975123087565, best_epoch 14
GPU_0_using curriculum 20 with window 20
Epoch: [15][20/30]	Time  1.656 ( 1.574)	Data  0.158 ( 0.068)	InnerLoop  0.645 ( 0.653)	Loss 5.8837e-01 (6.1428e-01)	Acc@1  75.78 ( 78.08)
The current update step is 480
GPU_0_using curriculum 20 with window 20
Epoch: [16][20/30]	Time  1.649 ( 1.574)	Data  0.038 ( 0.056)	InnerLoop  0.769 ( 0.667)	Loss 5.6707e-01 (6.4907e-01)	Acc@1  79.88 ( 77.27)
The current update step is 510
GPU_0_using curriculum 20 with window 20
Epoch: [17][20/30]	Time  1.534 ( 1.569)	Data  0.039 ( 0.051)	InnerLoop  0.649 ( 0.665)	Loss 5.6623e-01 (6.4837e-01)	Acc@1  80.25 ( 76.44)
The current update step is 540
GPU_0_using curriculum 20 with window 20
Epoch: [18][20/30]	Time  1.542 ( 1.564)	Data  0.038 ( 0.057)	InnerLoop  0.660 ( 0.659)	Loss 8.8824e-01 (6.7704e-01)	Acc@1  65.97 ( 75.59)
The current update step is 570
GPU_0_using curriculum 20 with window 20
Epoch: [19][20/30]	Time  1.567 ( 1.573)	Data  0.040 ( 0.045)	InnerLoop  0.659 ( 0.671)	Loss 5.6177e-01 (6.2616e-01)	Acc@1  79.17 ( 77.66)
The current update step is 600
The current seed is 10468905720174993452
The current lr is: 0.0012
Testing Results:
 *   Acc@1 44.974
 *   Acc@1 45.082
 *   Acc@1 47.079
 *   Acc@1 47.178
 *   Acc@1 49.316
 *   Acc@1 49.113
 *   Acc@1 50.816
 *   Acc@1 51.187
 *   Acc@1 52.355
 *   Acc@1 53.046
 *   Acc@1 52.605
 *   Acc@1 53.490
Training for 300 epoch: 47.89473684210526
Training for 600 epoch: 49.7171052631579
Training for 1000 epoch: 50.96052631578948
Training for 300 epoch: 48.134166666666665
Training for 600 epoch: 50.111666666666665
Training for 1000 epoch: 51.30166666666667
[[47.89473684210526, 49.7171052631579, 50.96052631578948], [48.134166666666665, 50.111666666666665, 51.30166666666667]]
train loss 1.2140044580459595, epoch 19, best loss 1.2140044580459595, best_epoch 19
GPU_0_using curriculum 20 with window 20
Epoch: [20][20/30]	Time  1.673 ( 1.566)	Data  0.164 ( 0.070)	InnerLoop  0.643 ( 0.649)	Loss 5.0243e-01 (6.6113e-01)	Acc@1  82.86 ( 76.84)
The current update step is 630
GPU_0_using curriculum 20 with window 20
Epoch: [21][20/30]	Time  1.658 ( 1.564)	Data  0.038 ( 0.058)	InnerLoop  0.771 ( 0.663)	Loss 7.4957e-01 (7.8207e-01)	Acc@1  71.14 ( 71.11)
The current update step is 660
GPU_0_using curriculum 20 with window 20
Epoch: [22][20/30]	Time  1.499 ( 1.542)	Data  0.040 ( 0.051)	InnerLoop  0.631 ( 0.655)	Loss 5.0132e-01 (5.9188e-01)	Acc@1  83.01 ( 79.47)
The current update step is 690
GPU_0_using curriculum 20 with window 20
Epoch: [23][20/30]	Time  1.511 ( 1.538)	Data  0.042 ( 0.057)	InnerLoop  0.639 ( 0.647)	Loss 5.8333e-01 (6.3687e-01)	Acc@1  79.59 ( 77.75)
The current update step is 720
GPU_0_using curriculum 20 with window 20
Epoch: [24][20/30]	Time  1.531 ( 1.537)	Data  0.039 ( 0.044)	InnerLoop  0.659 ( 0.662)	Loss 5.4163e-01 (6.1209e-01)	Acc@1  79.27 ( 78.58)
The current update step is 750
The current seed is 772451633449117459
The current lr is: 0.0012
Testing Results:
 *   Acc@1 59.118
 *   Acc@1 59.447
 *   Acc@1 59.355
 *   Acc@1 59.502
 *   Acc@1 59.355
 *   Acc@1 59.388
 *   Acc@1 45.697
 *   Acc@1 45.693
 *   Acc@1 45.303
 *   Acc@1 44.852
 *   Acc@1 43.947
 *   Acc@1 43.393
Training for 300 epoch: 52.40789473684211
Training for 600 epoch: 52.328947368421055
Training for 1000 epoch: 51.651315789473685
Training for 300 epoch: 52.569583333333334
Training for 600 epoch: 52.17708333333333
Training for 1000 epoch: 51.39041666666667
[[52.40789473684211, 52.328947368421055, 51.651315789473685], [52.569583333333334, 52.17708333333333, 51.39041666666667]]
train loss 2.197916972986857, epoch 24, best loss 1.2140044580459595, best_epoch 19
GPU_0_using curriculum 20 with window 20
Epoch: [25][20/30]	Time  1.626 ( 1.546)	Data  0.159 ( 0.069)	InnerLoop  0.629 ( 0.642)	Loss 6.3640e-01 (6.1895e-01)	Acc@1  79.17 ( 77.56)
The current update step is 780
GPU_0_using curriculum 20 with window 20
Epoch: [26][20/30]	Time  1.658 ( 1.552)	Data  0.038 ( 0.057)	InnerLoop  0.768 ( 0.659)	Loss 5.7123e-01 (6.3549e-01)	Acc@1  79.76 ( 77.95)
The current update step is 810
GPU_0_using curriculum 20 with window 20
Epoch: [27][20/30]	Time  1.519 ( 1.551)	Data  0.042 ( 0.052)	InnerLoop  0.639 ( 0.660)	Loss 5.4448e-01 (5.6338e-01)	Acc@1  81.79 ( 80.69)
The current update step is 840
GPU_0_using curriculum 20 with window 20
Epoch: [28][20/30]	Time  1.537 ( 1.550)	Data  0.041 ( 0.058)	InnerLoop  0.635 ( 0.653)	Loss 6.6213e-01 (6.2863e-01)	Acc@1  78.39 ( 78.16)
The current update step is 870
GPU_0_using curriculum 20 with window 20
Epoch: [29][20/30]	Time  1.525 ( 1.555)	Data  0.042 ( 0.046)	InnerLoop  0.644 ( 0.669)	Loss 5.8536e-01 (5.7727e-01)	Acc@1  76.56 ( 80.41)
The current update step is 900
The current seed is 14796620346771811808
The current lr is: 0.0012
Testing Results:
 *   Acc@1 68.474
 *   Acc@1 69.188
 *   Acc@1 71.118
 *   Acc@1 71.493
 *   Acc@1 72.329
 *   Acc@1 73.112
 *   Acc@1 53.882
 *   Acc@1 54.412
 *   Acc@1 57.605
 *   Acc@1 58.237
 *   Acc@1 58.855
 *   Acc@1 59.859
Training for 300 epoch: 61.17763157894737
Training for 600 epoch: 64.36184210526315
Training for 1000 epoch: 65.59210526315789
Training for 300 epoch: 61.79958333333333
Training for 600 epoch: 64.86458333333334
Training for 1000 epoch: 66.48583333333333
[[61.17763157894737, 64.36184210526315, 65.59210526315789], [61.79958333333333, 64.86458333333334, 66.48583333333333]]
train loss 1.122576648203532, epoch 29, best loss 1.122576648203532, best_epoch 29
GPU_0_using curriculum 20 with window 20
Epoch: [30][20/30]	Time  1.623 ( 1.543)	Data  0.154 ( 0.068)	InnerLoop  0.630 ( 0.642)	Loss 5.4303e-01 (5.5140e-01)	Acc@1  80.98 ( 81.13)
The current update step is 930
GPU_0_using curriculum 20 with window 20
Epoch: [31][20/30]	Time  1.641 ( 1.544)	Data  0.053 ( 0.058)	InnerLoop  0.759 ( 0.652)	Loss 4.7981e-01 (5.6638e-01)	Acc@1  84.47 ( 80.55)
The current update step is 960
GPU_0_using curriculum 20 with window 20
Epoch: [32][20/30]	Time  1.511 ( 1.533)	Data  0.042 ( 0.051)	InnerLoop  0.640 ( 0.652)	Loss 6.3145e-01 (6.6817e-01)	Acc@1  78.25 ( 76.43)
The current update step is 990
GPU_0_using curriculum 20 with window 20
Epoch: [33][20/30]	Time  1.498 ( 1.537)	Data  0.039 ( 0.057)	InnerLoop  0.629 ( 0.645)	Loss 6.2171e-01 (6.1976e-01)	Acc@1  75.71 ( 77.72)
The current update step is 1020
GPU_0_using curriculum 20 with window 20
Epoch: [34][20/30]	Time  1.502 ( 1.534)	Data  0.038 ( 0.044)	InnerLoop  0.636 ( 0.656)	Loss 5.4098e-01 (5.8979e-01)	Acc@1  81.45 ( 79.37)
The current update step is 1050
The current seed is 16236460439526442707
The current lr is: 0.0012
Testing Results:
 *   Acc@1 47.474
 *   Acc@1 47.963
 *   Acc@1 48.395
 *   Acc@1 49.006
 *   Acc@1 49.750
 *   Acc@1 49.720
 *   Acc@1 69.316
 *   Acc@1 69.707
 *   Acc@1 69.500
 *   Acc@1 70.423
 *   Acc@1 69.750
 *   Acc@1 70.511
Training for 300 epoch: 58.39473684210526
Training for 600 epoch: 58.94736842105263
Training for 1000 epoch: 59.75
Training for 300 epoch: 58.83541666666666
Training for 600 epoch: 59.71458333333334
Training for 1000 epoch: 60.11541666666667
[[58.39473684210526, 58.94736842105263, 59.75], [58.83541666666666, 59.71458333333334, 60.11541666666667]]
train loss 0.9869074384053548, epoch 34, best loss 0.9869074384053548, best_epoch 34
GPU_0_using curriculum 20 with window 20
Epoch: [35][20/30]	Time  1.637 ( 1.542)	Data  0.165 ( 0.068)	InnerLoop  0.637 ( 0.639)	Loss 5.5885e-01 (6.0128e-01)	Acc@1  80.10 ( 78.88)
The current update step is 1080
GPU_0_using curriculum 20 with window 20
Epoch: [36][20/30]	Time  1.620 ( 1.546)	Data  0.036 ( 0.057)	InnerLoop  0.755 ( 0.651)	Loss 5.1019e-01 (5.7245e-01)	Acc@1  83.25 ( 80.17)
The current update step is 1110
GPU_0_using curriculum 20 with window 20
Epoch: [37][20/30]	Time  1.488 ( 1.531)	Data  0.038 ( 0.051)	InnerLoop  0.624 ( 0.648)	Loss 5.3503e-01 (5.8121e-01)	Acc@1  81.76 ( 79.80)
The current update step is 1140
GPU_0_using curriculum 20 with window 20
Epoch: [38][20/30]	Time  1.497 ( 1.529)	Data  0.039 ( 0.055)	InnerLoop  0.629 ( 0.644)	Loss 5.4705e-01 (5.6618e-01)	Acc@1  80.37 ( 80.72)
The current update step is 1170
GPU_0_using curriculum 20 with window 20
Epoch: [39][20/30]	Time  1.505 ( 1.533)	Data  0.041 ( 0.045)	InnerLoop  0.638 ( 0.658)	Loss 5.2578e-01 (6.1299e-01)	Acc@1  81.47 ( 78.62)
The current update step is 1200
The current seed is 11845475982474185238
The current lr is: 0.0012
Testing Results:
 *   Acc@1 37.250
 *   Acc@1 37.327
 *   Acc@1 37.197
 *   Acc@1 37.607
 *   Acc@1 37.224
 *   Acc@1 37.419
 *   Acc@1 69.184
 *   Acc@1 70.030
 *   Acc@1 68.487
 *   Acc@1 69.297
 *   Acc@1 67.158
 *   Acc@1 67.956
Training for 300 epoch: 53.2171052631579
Training for 600 epoch: 52.8421052631579
Training for 1000 epoch: 52.19078947368421
Training for 300 epoch: 53.678333333333335
Training for 600 epoch: 53.45166666666667
Training for 1000 epoch: 52.6875
[[53.2171052631579, 52.8421052631579, 52.19078947368421], [53.678333333333335, 53.45166666666667, 52.6875]]
train loss 0.8910609661102294, epoch 39, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [40][20/30]	Time  1.629 ( 1.546)	Data  0.156 ( 0.068)	InnerLoop  0.641 ( 0.645)	Loss 5.1934e-01 (5.6730e-01)	Acc@1  82.91 ( 80.52)
The current update step is 1230
GPU_0_using curriculum 20 with window 20
Epoch: [41][20/30]	Time  1.626 ( 1.548)	Data  0.038 ( 0.057)	InnerLoop  0.757 ( 0.657)	Loss 5.2817e-01 (5.5111e-01)	Acc@1  80.93 ( 81.01)
The current update step is 1260
GPU_0_using curriculum 20 with window 20
Epoch: [42][20/30]	Time  1.509 ( 1.540)	Data  0.038 ( 0.050)	InnerLoop  0.635 ( 0.658)	Loss 5.1492e-01 (5.4792e-01)	Acc@1  83.11 ( 81.12)
The current update step is 1290
GPU_0_using curriculum 20 with window 20
Epoch: [43][20/30]	Time  1.508 ( 1.535)	Data  0.039 ( 0.056)	InnerLoop  0.641 ( 0.648)	Loss 5.0825e-01 (5.1301e-01)	Acc@1  83.69 ( 82.58)
The current update step is 1320
GPU_0_using curriculum 20 with window 20
Epoch: [44][20/30]	Time  1.514 ( 1.537)	Data  0.040 ( 0.046)	InnerLoop  0.643 ( 0.660)	Loss 5.0955e-01 (5.4935e-01)	Acc@1  82.84 ( 81.17)
The current update step is 1350
The current seed is 15863651599573244146
The current lr is: 0.0012
Testing Results:
 *   Acc@1 52.355
 *   Acc@1 52.337
 *   Acc@1 53.855
 *   Acc@1 53.872
 *   Acc@1 55.224
 *   Acc@1 55.174
 *   Acc@1 46.921
 *   Acc@1 47.062
 *   Acc@1 46.789
 *   Acc@1 47.179
 *   Acc@1 47.026
 *   Acc@1 47.309
Training for 300 epoch: 49.63815789473684
Training for 600 epoch: 50.32236842105263
Training for 1000 epoch: 51.125
Training for 300 epoch: 49.7
Training for 600 epoch: 50.52541666666667
Training for 1000 epoch: 51.24166666666667
[[49.63815789473684, 50.32236842105263, 51.125], [49.7, 50.52541666666667, 51.24166666666667]]
train loss 2.010160724067688, epoch 44, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [45][20/30]	Time  1.623 ( 1.543)	Data  0.156 ( 0.067)	InnerLoop  0.632 ( 0.645)	Loss 5.2181e-01 (5.4476e-01)	Acc@1  81.86 ( 81.64)
The current update step is 1380
GPU_0_using curriculum 20 with window 20
Epoch: [46][20/30]	Time  1.644 ( 1.547)	Data  0.037 ( 0.056)	InnerLoop  0.755 ( 0.655)	Loss 6.8463e-01 (5.5699e-01)	Acc@1  75.02 ( 80.91)
The current update step is 1410
GPU_0_using curriculum 20 with window 20
Epoch: [47][20/30]	Time  1.518 ( 1.540)	Data  0.040 ( 0.050)	InnerLoop  0.646 ( 0.660)	Loss 6.5825e-01 (6.2389e-01)	Acc@1  77.78 ( 78.31)
The current update step is 1440
GPU_0_using curriculum 20 with window 20
Epoch: [48][20/30]	Time  1.525 ( 1.541)	Data  0.039 ( 0.056)	InnerLoop  0.639 ( 0.650)	Loss 5.1840e-01 (5.5082e-01)	Acc@1  82.74 ( 81.04)
The current update step is 1470
GPU_0_using curriculum 20 with window 20
Epoch: [49][20/30]	Time  1.594 ( 1.544)	Data  0.039 ( 0.045)	InnerLoop  0.683 ( 0.665)	Loss 6.4504e-01 (5.4841e-01)	Acc@1  78.86 ( 81.05)
The current update step is 1500
The current seed is 13523810045775859506
The current lr is: 0.0012
Testing Results:
 *   Acc@1 58.039
 *   Acc@1 58.287
 *   Acc@1 59.250
 *   Acc@1 59.198
 *   Acc@1 58.474
 *   Acc@1 58.327
 *   Acc@1 59.842
 *   Acc@1 60.421
 *   Acc@1 59.816
 *   Acc@1 60.185
 *   Acc@1 59.039
 *   Acc@1 60.057
Training for 300 epoch: 58.94078947368421
Training for 600 epoch: 59.53289473684211
Training for 1000 epoch: 58.756578947368425
Training for 300 epoch: 59.353750000000005
Training for 600 epoch: 59.69166666666666
Training for 1000 epoch: 59.19208333333333
[[58.94078947368421, 59.53289473684211, 58.756578947368425], [59.353750000000005, 59.69166666666666, 59.19208333333333]]
train loss 1.0064688737869263, epoch 49, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [50][20/30]	Time  1.641 ( 1.552)	Data  0.159 ( 0.069)	InnerLoop  0.655 ( 0.645)	Loss 4.9917e-01 (5.5085e-01)	Acc@1  83.06 ( 81.07)
The current update step is 1530
GPU_0_using curriculum 20 with window 20
Epoch: [51][20/30]	Time  1.520 ( 1.547)	Data  0.041 ( 0.057)	InnerLoop  0.646 ( 0.653)	Loss 5.8722e-01 (5.4905e-01)	Acc@1  79.93 ( 81.32)
The current update step is 1560
GPU_0_using curriculum 20 with window 20
Epoch: [52][20/30]	Time  1.516 ( 1.538)	Data  0.038 ( 0.069)	InnerLoop  0.647 ( 0.637)	Loss 4.9790e-01 (5.6762e-01)	Acc@1  83.03 ( 80.39)
The current update step is 1590
GPU_0_using curriculum 20 with window 20
Epoch: [53][20/30]	Time  1.515 ( 1.538)	Data  0.040 ( 0.056)	InnerLoop  0.640 ( 0.649)	Loss 5.1668e-01 (5.8424e-01)	Acc@1  82.52 ( 79.81)
The current update step is 1620
GPU_0_using curriculum 20 with window 20
Epoch: [54][20/30]	Time  1.652 ( 1.547)	Data  0.155 ( 0.068)	InnerLoop  0.642 ( 0.645)	Loss 5.5585e-01 (5.4737e-01)	Acc@1  80.40 ( 80.77)
The current update step is 1650
The current seed is 1556679726382246406
The current lr is: 0.0012
Testing Results:
 *   Acc@1 42.895
 *   Acc@1 42.829
 *   Acc@1 38.947
 *   Acc@1 38.962
 *   Acc@1 38.684
 *   Acc@1 38.527
 *   Acc@1 35.750
 *   Acc@1 36.188
 *   Acc@1 37.645
 *   Acc@1 37.932
 *   Acc@1 38.618
 *   Acc@1 39.322
Training for 300 epoch: 39.32236842105263
Training for 600 epoch: 38.296052631578945
Training for 1000 epoch: 38.651315789473685
Training for 300 epoch: 39.50833333333333
Training for 600 epoch: 38.4475
Training for 1000 epoch: 38.92458333333333
[[39.32236842105263, 38.296052631578945, 38.651315789473685], [39.50833333333333, 38.4475, 38.92458333333333]]
train loss 1.952007759920756, epoch 54, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [55][20/30]	Time  1.533 ( 1.557)	Data  0.039 ( 0.052)	InnerLoop  0.652 ( 0.664)	Loss 5.2583e-01 (5.3874e-01)	Acc@1  81.27 ( 81.17)
The current update step is 1680
GPU_0_using curriculum 20 with window 20
Epoch: [56][20/30]	Time  1.670 ( 1.567)	Data  0.165 ( 0.071)	InnerLoop  0.655 ( 0.655)	Loss 5.5037e-01 (5.5631e-01)	Acc@1  80.74 ( 80.42)
The current update step is 1710
GPU_0_using curriculum 20 with window 20
Epoch: [57][20/30]	Time  1.507 ( 1.552)	Data  0.039 ( 0.057)	InnerLoop  0.634 ( 0.656)	Loss 4.6159e-01 (4.8718e-01)	Acc@1  83.86 ( 83.60)
The current update step is 1740
GPU_0_using curriculum 20 with window 20
Epoch: [58][20/30]	Time  1.533 ( 1.551)	Data  0.041 ( 0.045)	InnerLoop  0.661 ( 0.668)	Loss 5.5612e-01 (5.2754e-01)	Acc@1  80.74 ( 81.82)
The current update step is 1770
GPU_0_using curriculum 20 with window 20
Epoch: [59][20/30]	Time  1.522 ( 1.549)	Data  0.041 ( 0.069)	InnerLoop  0.640 ( 0.642)	Loss 4.7310e-01 (5.3068e-01)	Acc@1  83.54 ( 81.85)
The current update step is 1800
The current seed is 3888960913633331338
The current lr is: 0.0012
Testing Results:
 *   Acc@1 60.618
 *   Acc@1 61.347
 *   Acc@1 63.895
 *   Acc@1 64.184
 *   Acc@1 63.553
 *   Acc@1 64.075
 *   Acc@1 60.197
 *   Acc@1 61.458
 *   Acc@1 60.447
 *   Acc@1 61.497
 *   Acc@1 60.474
 *   Acc@1 61.943
Training for 300 epoch: 60.40789473684211
Training for 600 epoch: 62.171052631578945
Training for 1000 epoch: 62.01315789473684
Training for 300 epoch: 61.4025
Training for 600 epoch: 62.84041666666667
Training for 1000 epoch: 63.00916666666667
[[60.40789473684211, 62.171052631578945, 62.01315789473684], [61.4025, 62.84041666666667, 63.00916666666667]]
train loss 1.0470207579294841, epoch 59, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [60][20/30]	Time  1.622 ( 1.549)	Data  0.159 ( 0.068)	InnerLoop  0.633 ( 0.644)	Loss 4.8127e-01 (5.2126e-01)	Acc@1  82.74 ( 82.20)
The current update step is 1830
GPU_0_using curriculum 20 with window 20
Epoch: [61][20/30]	Time  1.531 ( 1.541)	Data  0.039 ( 0.056)	InnerLoop  0.641 ( 0.652)	Loss 5.6415e-01 (5.4047e-01)	Acc@1  80.93 ( 81.30)
The current update step is 1860
GPU_0_using curriculum 20 with window 20
Epoch: [62][20/30]	Time  1.530 ( 1.543)	Data  0.039 ( 0.069)	InnerLoop  0.639 ( 0.639)	Loss 4.9816e-01 (5.3066e-01)	Acc@1  83.45 ( 81.58)
The current update step is 1890
GPU_0_using curriculum 20 with window 20
Epoch: [63][20/30]	Time  1.501 ( 1.543)	Data  0.039 ( 0.057)	InnerLoop  0.634 ( 0.652)	Loss 5.2506e-01 (5.0790e-01)	Acc@1  82.32 ( 82.61)
The current update step is 1920
GPU_0_using curriculum 20 with window 20
Epoch: [64][20/30]	Time  1.621 ( 1.552)	Data  0.156 ( 0.070)	InnerLoop  0.638 ( 0.649)	Loss 4.9101e-01 (4.9989e-01)	Acc@1  83.28 ( 82.87)
The current update step is 1950
The current seed is 9696621887285460005
The current lr is: 0.0012
Testing Results:
 *   Acc@1 62.303
 *   Acc@1 63.520
 *   Acc@1 62.092
 *   Acc@1 63.399
 *   Acc@1 62.368
 *   Acc@1 63.167
 *   Acc@1 50.461
 *   Acc@1 50.877
 *   Acc@1 66.658
 *   Acc@1 67.282
 *   Acc@1 66.184
 *   Acc@1 66.948
Training for 300 epoch: 56.381578947368425
Training for 600 epoch: 64.375
Training for 1000 epoch: 64.27631578947368
Training for 300 epoch: 57.198750000000004
Training for 600 epoch: 65.34041666666667
Training for 1000 epoch: 65.05708333333334
[[56.381578947368425, 64.375, 64.27631578947368], [57.198750000000004, 65.34041666666667, 65.05708333333334]]
train loss 0.922556640402476, epoch 64, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [65][20/30]	Time  1.518 ( 1.538)	Data  0.039 ( 0.051)	InnerLoop  0.634 ( 0.655)	Loss 5.2019e-01 (5.2713e-01)	Acc@1  82.52 ( 81.98)
The current update step is 1980
GPU_0_using curriculum 20 with window 20
Epoch: [66][20/30]	Time  1.620 ( 1.542)	Data  0.155 ( 0.068)	InnerLoop  0.635 ( 0.643)	Loss 5.4609e-01 (5.3626e-01)	Acc@1  81.54 ( 81.70)
The current update step is 2010
GPU_0_using curriculum 20 with window 20
Epoch: [67][20/30]	Time  1.526 ( 1.540)	Data  0.038 ( 0.056)	InnerLoop  0.647 ( 0.650)	Loss 5.5714e-01 (5.6232e-01)	Acc@1  81.10 ( 81.13)
The current update step is 2040
GPU_0_using curriculum 20 with window 20
Epoch: [68][20/30]	Time  1.508 ( 1.550)	Data  0.042 ( 0.047)	InnerLoop  0.635 ( 0.668)	Loss 5.3738e-01 (5.2534e-01)	Acc@1  81.54 ( 81.96)
The current update step is 2070
GPU_0_using curriculum 20 with window 20
Epoch: [69][20/30]	Time  1.516 ( 1.539)	Data  0.040 ( 0.051)	InnerLoop  0.644 ( 0.658)	Loss 4.9984e-01 (5.4160e-01)	Acc@1  83.15 ( 81.27)
The current update step is 2100
The current seed is 1794290971320411926
The current lr is: 0.0012
Testing Results:
 *   Acc@1 60.816
 *   Acc@1 61.237
 *   Acc@1 62.158
 *   Acc@1 62.298
 *   Acc@1 62.237
 *   Acc@1 62.458
 *   Acc@1 58.066
 *   Acc@1 58.888
 *   Acc@1 60.105
 *   Acc@1 60.799
 *   Acc@1 60.684
 *   Acc@1 61.028
Training for 300 epoch: 59.44078947368421
Training for 600 epoch: 61.131578947368425
Training for 1000 epoch: 61.46052631578947
Training for 300 epoch: 60.0625
Training for 600 epoch: 61.54875
Training for 1000 epoch: 61.74333333333334
[[59.44078947368421, 61.131578947368425, 61.46052631578947], [60.0625, 61.54875, 61.74333333333334]]
train loss 1.013169816239675, epoch 69, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [70][20/30]	Time  1.525 ( 1.547)	Data  0.045 ( 0.057)	InnerLoop  0.640 ( 0.652)	Loss 6.1108e-01 (5.1263e-01)	Acc@1  77.17 ( 82.74)
The current update step is 2130
GPU_0_using curriculum 20 with window 20
Epoch: [71][20/30]	Time  1.561 ( 1.549)	Data  0.040 ( 0.064)	InnerLoop  0.645 ( 0.647)	Loss 8.1027e-01 (5.4055e-01)	Acc@1  67.55 ( 81.13)
The current update step is 2160
GPU_0_using curriculum 20 with window 20
Epoch: [72][20/30]	Time  1.507 ( 1.543)	Data  0.039 ( 0.056)	InnerLoop  0.639 ( 0.653)	Loss 5.0543e-01 (5.5142e-01)	Acc@1  83.42 ( 81.22)
The current update step is 2190
GPU_0_using curriculum 20 with window 20
Epoch: [73][20/30]	Time  1.509 ( 1.545)	Data  0.036 ( 0.063)	InnerLoop  0.642 ( 0.647)	Loss 5.0459e-01 (5.5258e-01)	Acc@1  83.03 ( 81.02)
The current update step is 2220
GPU_0_using curriculum 20 with window 20
Epoch: [74][20/30]	Time  1.638 ( 1.548)	Data  0.161 ( 0.064)	InnerLoop  0.641 ( 0.652)	Loss 5.4727e-01 (5.3078e-01)	Acc@1  80.15 ( 81.71)
The current update step is 2250
The current seed is 12440314029985010263
The current lr is: 0.0012
Testing Results:
 *   Acc@1 36.895
 *   Acc@1 37.387
 *   Acc@1 37.961
 *   Acc@1 38.182
 *   Acc@1 38.671
 *   Acc@1 38.519
 *   Acc@1 50.382
 *   Acc@1 50.570
 *   Acc@1 51.224
 *   Acc@1 51.181
 *   Acc@1 52.066
 *   Acc@1 51.695
Training for 300 epoch: 43.638157894736835
Training for 600 epoch: 44.59210526315789
Training for 1000 epoch: 45.368421052631575
Training for 300 epoch: 43.97833333333333
Training for 600 epoch: 44.68125
Training for 1000 epoch: 45.107083333333335
[[43.638157894736835, 44.59210526315789, 45.368421052631575], [43.97833333333333, 44.68125, 45.107083333333335]]
train loss 1.2576666565577188, epoch 74, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [75][20/30]	Time  1.625 ( 1.554)	Data  0.039 ( 0.051)	InnerLoop  0.753 ( 0.667)	Loss 4.8953e-01 (5.0918e-01)	Acc@1  82.57 ( 82.78)
The current update step is 2280
GPU_0_using curriculum 20 with window 20
Epoch: [76][20/30]	Time  1.512 ( 1.544)	Data  0.039 ( 0.051)	InnerLoop  0.642 ( 0.660)	Loss 6.3886e-01 (5.1800e-01)	Acc@1  79.64 ( 82.74)
The current update step is 2310
GPU_0_using curriculum 20 with window 20
Epoch: [77][20/30]	Time  1.520 ( 1.545)	Data  0.045 ( 0.057)	InnerLoop  0.636 ( 0.654)	Loss 5.2757e-01 (5.2246e-01)	Acc@1  81.49 ( 82.13)
The current update step is 2340
GPU_0_using curriculum 20 with window 20
Epoch: [78][20/30]	Time  1.511 ( 1.545)	Data  0.041 ( 0.045)	InnerLoop  0.641 ( 0.667)	Loss 4.8642e-01 (5.3077e-01)	Acc@1  84.08 ( 82.02)
The current update step is 2370
GPU_0_using curriculum 20 with window 20
Epoch: [79][20/30]	Time  1.526 ( 1.542)	Data  0.040 ( 0.051)	InnerLoop  0.652 ( 0.658)	Loss 4.9995e-01 (5.0273e-01)	Acc@1  83.11 ( 83.16)
The current update step is 2400
The current seed is 13132431969546250651
The current lr is: 0.0012
Testing Results:
 *   Acc@1 49.013
 *   Acc@1 49.173
 *   Acc@1 52.789
 *   Acc@1 53.160
 *   Acc@1 54.776
 *   Acc@1 54.947
 *   Acc@1 48.408
 *   Acc@1 48.678
 *   Acc@1 48.237
 *   Acc@1 48.748
 *   Acc@1 48.316
 *   Acc@1 48.613
Training for 300 epoch: 48.71052631578947
Training for 600 epoch: 50.51315789473684
Training for 1000 epoch: 51.546052631578945
Training for 300 epoch: 48.92541666666666
Training for 600 epoch: 50.95375
Training for 1000 epoch: 51.78
[[48.71052631578947, 50.51315789473684, 51.546052631578945], [48.92541666666666, 50.95375, 51.78]]
train loss 1.6942178265253702, epoch 79, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [80][20/30]	Time  1.501 ( 1.537)	Data  0.040 ( 0.056)	InnerLoop  0.639 ( 0.651)	Loss 4.6675e-01 (4.9917e-01)	Acc@1  84.42 ( 83.35)
The current update step is 2430
GPU_0_using curriculum 20 with window 20
Epoch: [81][20/30]	Time  1.524 ( 1.539)	Data  0.038 ( 0.063)	InnerLoop  0.640 ( 0.643)	Loss 4.8240e-01 (4.9395e-01)	Acc@1  83.37 ( 83.42)
The current update step is 2460
GPU_0_using curriculum 20 with window 20
Epoch: [82][20/30]	Time  1.510 ( 1.538)	Data  0.040 ( 0.058)	InnerLoop  0.634 ( 0.647)	Loss 5.1108e-01 (5.1636e-01)	Acc@1  82.64 ( 82.81)
The current update step is 2490
GPU_0_using curriculum 20 with window 20
Epoch: [83][20/30]	Time  1.501 ( 1.537)	Data  0.037 ( 0.063)	InnerLoop  0.638 ( 0.645)	Loss 5.6362e-01 (5.6898e-01)	Acc@1  80.81 ( 80.53)
The current update step is 2520
GPU_0_using curriculum 20 with window 20
Epoch: [84][20/30]	Time  1.635 ( 1.545)	Data  0.156 ( 0.063)	InnerLoop  0.641 ( 0.651)	Loss 5.2390e-01 (5.4121e-01)	Acc@1  82.84 ( 81.41)
The current update step is 2550
The current seed is 1603861291801087841
The current lr is: 0.0012
Testing Results:
 *   Acc@1 69.066
 *   Acc@1 69.695
 *   Acc@1 69.711
 *   Acc@1 69.812
 *   Acc@1 68.961
 *   Acc@1 69.430
 *   Acc@1 41.947
 *   Acc@1 41.916
 *   Acc@1 42.895
 *   Acc@1 43.233
 *   Acc@1 43.724
 *   Acc@1 43.910
Training for 300 epoch: 55.50657894736842
Training for 600 epoch: 56.30263157894737
Training for 1000 epoch: 56.3421052631579
Training for 300 epoch: 55.80541666666666
Training for 600 epoch: 56.522083333333335
Training for 1000 epoch: 56.67
[[55.50657894736842, 56.30263157894737, 56.3421052631579], [55.80541666666666, 56.522083333333335, 56.67]]
train loss 2.0721621830622357, epoch 84, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [85][20/30]	Time  1.630 ( 1.544)	Data  0.038 ( 0.051)	InnerLoop  0.758 ( 0.661)	Loss 4.5618e-01 (4.9549e-01)	Acc@1  84.84 ( 83.19)
The current update step is 2580
GPU_0_using curriculum 20 with window 20
Epoch: [86][20/30]	Time  1.523 ( 1.537)	Data  0.039 ( 0.051)	InnerLoop  0.636 ( 0.655)	Loss 4.9815e-01 (5.0733e-01)	Acc@1  83.47 ( 82.78)
The current update step is 2610
GPU_0_using curriculum 20 with window 20
Epoch: [87][20/30]	Time  1.512 ( 1.538)	Data  0.041 ( 0.057)	InnerLoop  0.642 ( 0.651)	Loss 5.0959e-01 (5.1838e-01)	Acc@1  82.18 ( 81.96)
The current update step is 2640
GPU_0_using curriculum 20 with window 20
Epoch: [88][20/30]	Time  1.524 ( 1.545)	Data  0.048 ( 0.046)	InnerLoop  0.649 ( 0.664)	Loss 6.2931e-01 (5.5151e-01)	Acc@1  79.42 ( 81.43)
The current update step is 2670
GPU_0_using curriculum 20 with window 20
Epoch: [89][20/30]	Time  1.537 ( 1.566)	Data  0.045 ( 0.053)	InnerLoop  0.648 ( 0.671)	Loss 4.7370e-01 (5.0991e-01)	Acc@1  84.79 ( 82.87)
The current update step is 2700
The current seed is 13614947448796414471
The current lr is: 0.0012
Testing Results:
 *   Acc@1 53.882
 *   Acc@1 54.453
 *   Acc@1 53.355
 *   Acc@1 53.624
 *   Acc@1 52.855
 *   Acc@1 53.401
 *   Acc@1 53.987
 *   Acc@1 54.907
 *   Acc@1 55.605
 *   Acc@1 56.434
 *   Acc@1 56.934
 *   Acc@1 57.774
Training for 300 epoch: 53.93421052631579
Training for 600 epoch: 54.48026315789474
Training for 1000 epoch: 54.89473684210526
Training for 300 epoch: 54.680416666666666
Training for 600 epoch: 55.02916666666667
Training for 1000 epoch: 55.5875
[[53.93421052631579, 54.48026315789474, 54.89473684210526], [54.680416666666666, 55.02916666666667, 55.5875]]
train loss 1.313466463025411, epoch 89, best loss 0.8910609661102294, best_epoch 39
GPU_0_using curriculum 20 with window 20
Epoch: [90][20/30]	Time  1.529 ( 1.560)	Data  0.040 ( 0.053)	InnerLoop  0.648 ( 0.668)	Loss 6.0771e-01 (5.2118e-01)	Acc@1  79.86 ( 82.16)
The current update step is 2730
GPU_0_using curriculum 20 with window 20
Epoch: [91][20/30]	Time  1.534 ( 1.565)	Data  0.040 ( 0.071)	InnerLoop  0.648 ( 0.651)	Loss 4.8104e-01 (5.0584e-01)	Acc@1  83.86 ( 82.87)
The current update step is 2760
GPU_0_using curriculum 20 with window 20
Epoch: [92][20/30]	Time  1.530 ( 1.568)	Data  0.041 ( 0.072)	InnerLoop  0.652 ( 0.654)	Loss 5.1193e-01 (5.0791e-01)	Acc@1  82.50 ( 82.60)
The current update step is 2790
GPU_0_using curriculum 20 with window 20
Epoch: [93][20/30]	Time  1.655 ( 1.560)	Data  0.041 ( 0.070)	InnerLoop  0.772 ( 0.651)	Loss 4.9515e-01 (5.3848e-01)	Acc@1  83.33 ( 81.31)
The current update step is 2820
GPU_0_using curriculum 20 with window 20
Epoch: [94][20/30]	Time  1.537 ( 1.556)	Data  0.039 ( 0.057)	InnerLoop  0.648 ( 0.659)	Loss 4.9612e-01 (5.2333e-01)	Acc@1  83.37 ( 82.07)
The current update step is 2850
The current seed is 9554944883596686354
The current lr is: 0.0012
Testing Results:
 *   Acc@1 54.553
 *   Acc@1 55.038
 *   Acc@1 57.316
 *   Acc@1 57.867
 *   Acc@1 58.579
 *   Acc@1 59.409
 *   Acc@1 62.421
 *   Acc@1 63.367
 *   Acc@1 63.684
 *   Acc@1 64.406
 *   Acc@1 64.289
 *   Acc@1 65.158
Training for 300 epoch: 58.48684210526316
Training for 600 epoch: 60.5
Training for 1000 epoch: 61.43421052631579
Training for 300 epoch: 59.2025
Training for 600 epoch: 61.136250000000004
Training for 1000 epoch: 62.28375
[[58.48684210526316, 60.5, 61.43421052631579], [59.2025, 61.136250000000004, 62.28375]]
train loss 0.8148760060310364, epoch 94, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [95][20/30]	Time  1.512 ( 1.555)	Data  0.039 ( 0.057)	InnerLoop  0.642 ( 0.663)	Loss 5.3606e-01 (4.8477e-01)	Acc@1  83.13 ( 83.53)
The current update step is 2880
GPU_0_using curriculum 20 with window 20
Epoch: [96][20/30]	Time  1.637 ( 1.552)	Data  0.038 ( 0.058)	InnerLoop  0.763 ( 0.659)	Loss 4.9904e-01 (4.7991e-01)	Acc@1  84.16 ( 83.67)
The current update step is 2910
GPU_0_using curriculum 20 with window 20
Epoch: [97][20/30]	Time  1.508 ( 1.548)	Data  0.039 ( 0.050)	InnerLoop  0.639 ( 0.661)	Loss 5.7838e-01 (4.9450e-01)	Acc@1  79.54 ( 83.09)
The current update step is 2940
GPU_0_using curriculum 20 with window 20
Epoch: [98][20/30]	Time  1.545 ( 1.545)	Data  0.040 ( 0.057)	InnerLoop  0.666 ( 0.652)	Loss 4.5431e-01 (4.8319e-01)	Acc@1  84.52 ( 83.44)
The current update step is 2970
GPU_0_using curriculum 20 with window 20
Epoch: [99][20/30]	Time  1.521 ( 1.545)	Data  0.040 ( 0.046)	InnerLoop  0.642 ( 0.666)	Loss 4.7456e-01 (4.9603e-01)	Acc@1  83.64 ( 83.15)
The current update step is 3000
The current seed is 3757388516365281135
The current lr is: 0.0012
Testing Results:
 *   Acc@1 62.724
 *   Acc@1 62.797
 *   Acc@1 64.026
 *   Acc@1 63.776
 *   Acc@1 64.276
 *   Acc@1 64.217
 *   Acc@1 63.500
 *   Acc@1 63.088
 *   Acc@1 64.724
 *   Acc@1 64.877
 *   Acc@1 65.737
 *   Acc@1 65.876
Training for 300 epoch: 63.11184210526316
Training for 600 epoch: 64.375
Training for 1000 epoch: 65.00657894736842
Training for 300 epoch: 62.94291666666666
Training for 600 epoch: 64.32625
Training for 1000 epoch: 65.04625
[[63.11184210526316, 64.375, 65.00657894736842], [62.94291666666666, 64.32625, 65.04625]]
train loss 1.0532723058700562, epoch 99, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [100][20/30]	Time  1.638 ( 1.549)	Data  0.161 ( 0.069)	InnerLoop  0.645 ( 0.647)	Loss 5.6984e-01 (5.1254e-01)	Acc@1  80.25 ( 82.25)
The current update step is 3030
GPU_0_using curriculum 20 with window 20
Epoch: [101][20/30]	Time  1.529 ( 1.563)	Data  0.039 ( 0.059)	InnerLoop  0.650 ( 0.663)	Loss 4.7300e-01 (4.9609e-01)	Acc@1  84.74 ( 82.94)
The current update step is 3060
GPU_0_using curriculum 20 with window 20
Epoch: [102][20/30]	Time  1.514 ( 1.542)	Data  0.040 ( 0.069)	InnerLoop  0.641 ( 0.641)	Loss 7.0652e-01 (4.9344e-01)	Acc@1  75.34 ( 82.90)
The current update step is 3090
GPU_0_using curriculum 20 with window 20
Epoch: [103][20/30]	Time  1.508 ( 1.539)	Data  0.039 ( 0.057)	InnerLoop  0.634 ( 0.650)	Loss 4.9372e-01 (4.9335e-01)	Acc@1  82.08 ( 83.00)
The current update step is 3120
GPU_0_using curriculum 20 with window 20
Epoch: [104][20/30]	Time  1.617 ( 1.544)	Data  0.155 ( 0.069)	InnerLoop  0.638 ( 0.645)	Loss 4.8364e-01 (4.8716e-01)	Acc@1  84.16 ( 83.44)
The current update step is 3150
The current seed is 3832573781453978343
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.118
 *   Acc@1 64.466
 *   Acc@1 64.711
 *   Acc@1 65.299
 *   Acc@1 65.342
 *   Acc@1 65.506
 *   Acc@1 54.079
 *   Acc@1 54.355
 *   Acc@1 62.105
 *   Acc@1 62.508
 *   Acc@1 62.789
 *   Acc@1 62.742
Training for 300 epoch: 59.098684210526315
Training for 600 epoch: 63.40789473684211
Training for 1000 epoch: 64.0657894736842
Training for 300 epoch: 59.41041666666666
Training for 600 epoch: 63.90375
Training for 1000 epoch: 64.12416666666667
[[59.098684210526315, 63.40789473684211, 64.0657894736842], [59.41041666666666, 63.90375, 64.12416666666667]]
train loss 1.3996164342880248, epoch 104, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [105][20/30]	Time  1.549 ( 1.550)	Data  0.039 ( 0.051)	InnerLoop  0.655 ( 0.664)	Loss 4.0138e-01 (4.7130e-01)	Acc@1  86.38 ( 83.93)
The current update step is 3180
GPU_0_using curriculum 20 with window 20
Epoch: [106][20/30]	Time  1.637 ( 1.544)	Data  0.155 ( 0.068)	InnerLoop  0.648 ( 0.646)	Loss 4.8949e-01 (4.9177e-01)	Acc@1  84.57 ( 83.21)
The current update step is 3210
GPU_0_using curriculum 20 with window 20
Epoch: [107][20/30]	Time  1.503 ( 1.541)	Data  0.038 ( 0.056)	InnerLoop  0.633 ( 0.652)	Loss 4.7306e-01 (5.0173e-01)	Acc@1  83.59 ( 82.77)
The current update step is 3240
GPU_0_using curriculum 20 with window 20
Epoch: [108][20/30]	Time  1.503 ( 1.540)	Data  0.039 ( 0.043)	InnerLoop  0.638 ( 0.665)	Loss 4.5590e-01 (5.0341e-01)	Acc@1  84.52 ( 82.78)
The current update step is 3270
GPU_0_using curriculum 20 with window 20
Epoch: [109][20/30]	Time  1.517 ( 1.543)	Data  0.040 ( 0.068)	InnerLoop  0.644 ( 0.642)	Loss 4.6915e-01 (4.8859e-01)	Acc@1  84.06 ( 83.22)
The current update step is 3300
The current seed is 8729925550889179933
The current lr is: 0.0012
Testing Results:
 *   Acc@1 54.474
 *   Acc@1 54.295
 *   Acc@1 66.224
 *   Acc@1 66.162
 *   Acc@1 67.803
 *   Acc@1 67.569
 *   Acc@1 62.342
 *   Acc@1 63.187
 *   Acc@1 63.342
 *   Acc@1 63.865
 *   Acc@1 64.013
 *   Acc@1 64.118
Training for 300 epoch: 58.40789473684211
Training for 600 epoch: 64.78289473684211
Training for 1000 epoch: 65.90789473684211
Training for 300 epoch: 58.740833333333335
Training for 600 epoch: 65.01375
Training for 1000 epoch: 65.84333333333333
[[58.40789473684211, 64.78289473684211, 65.90789473684211], [58.740833333333335, 65.01375, 65.84333333333333]]
train loss 1.0080276927312215, epoch 109, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [110][20/30]	Time  1.624 ( 1.546)	Data  0.160 ( 0.068)	InnerLoop  0.633 ( 0.646)	Loss 6.4519e-01 (5.0161e-01)	Acc@1  78.76 ( 82.98)
The current update step is 3330
GPU_0_using curriculum 20 with window 20
Epoch: [111][20/30]	Time  1.527 ( 1.539)	Data  0.039 ( 0.056)	InnerLoop  0.643 ( 0.653)	Loss 4.8426e-01 (5.0243e-01)	Acc@1  82.13 ( 82.49)
The current update step is 3360
GPU_0_using curriculum 20 with window 20
Epoch: [112][20/30]	Time  1.510 ( 1.539)	Data  0.040 ( 0.069)	InnerLoop  0.640 ( 0.640)	Loss 6.5293e-01 (4.9545e-01)	Acc@1  76.05 ( 83.02)
The current update step is 3390
GPU_0_using curriculum 20 with window 20
Epoch: [113][20/30]	Time  1.507 ( 1.543)	Data  0.038 ( 0.057)	InnerLoop  0.639 ( 0.653)	Loss 4.4328e-01 (4.9492e-01)	Acc@1  85.33 ( 82.99)
The current update step is 3420
GPU_0_using curriculum 20 with window 20
Epoch: [114][20/30]	Time  1.630 ( 1.547)	Data  0.156 ( 0.068)	InnerLoop  0.636 ( 0.648)	Loss 5.2425e-01 (5.0544e-01)	Acc@1  81.45 ( 82.58)
The current update step is 3450
The current seed is 15506956779394566803
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.342
 *   Acc@1 64.040
 *   Acc@1 68.079
 *   Acc@1 67.811
 *   Acc@1 68.763
 *   Acc@1 68.380
 *   Acc@1 66.763
 *   Acc@1 66.894
 *   Acc@1 66.368
 *   Acc@1 66.143
 *   Acc@1 65.118
 *   Acc@1 65.322
Training for 300 epoch: 65.55263157894737
Training for 600 epoch: 67.22368421052632
Training for 1000 epoch: 66.9407894736842
Training for 300 epoch: 65.46708333333333
Training for 600 epoch: 66.97708333333333
Training for 1000 epoch: 66.85083333333333
[[65.55263157894737, 67.22368421052632, 66.9407894736842], [65.46708333333333, 66.97708333333333, 66.85083333333333]]
train loss 1.0031253700574239, epoch 114, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [115][20/30]	Time  1.517 ( 1.545)	Data  0.039 ( 0.051)	InnerLoop  0.646 ( 0.661)	Loss 4.9175e-01 (5.0068e-01)	Acc@1  83.25 ( 82.83)
The current update step is 3480
GPU_0_using curriculum 20 with window 20
Epoch: [116][20/30]	Time  1.646 ( 1.550)	Data  0.158 ( 0.069)	InnerLoop  0.661 ( 0.648)	Loss 5.0665e-01 (5.3237e-01)	Acc@1  82.32 ( 81.42)
The current update step is 3510
GPU_0_using curriculum 20 with window 20
Epoch: [117][20/30]	Time  1.520 ( 1.545)	Data  0.038 ( 0.057)	InnerLoop  0.643 ( 0.654)	Loss 4.8317e-01 (5.3183e-01)	Acc@1  82.74 ( 81.29)
The current update step is 3540
GPU_0_using curriculum 20 with window 20
Epoch: [118][20/30]	Time  1.513 ( 1.545)	Data  0.041 ( 0.044)	InnerLoop  0.638 ( 0.668)	Loss 5.0476e-01 (5.0998e-01)	Acc@1  83.42 ( 82.29)
The current update step is 3570
GPU_0_using curriculum 20 with window 20
Epoch: [119][20/30]	Time  1.534 ( 1.546)	Data  0.039 ( 0.051)	InnerLoop  0.649 ( 0.658)	Loss 4.7749e-01 (4.9475e-01)	Acc@1  84.16 ( 82.86)
The current update step is 3600
The current seed is 1624746353212362161
The current lr is: 0.0012
Testing Results:
 *   Acc@1 48.066
 *   Acc@1 48.885
 *   Acc@1 49.434
 *   Acc@1 50.112
 *   Acc@1 49.487
 *   Acc@1 50.693
 *   Acc@1 67.908
 *   Acc@1 68.637
 *   Acc@1 66.421
 *   Acc@1 66.688
 *   Acc@1 65.711
 *   Acc@1 65.746
Training for 300 epoch: 57.986842105263165
Training for 600 epoch: 57.92763157894737
Training for 1000 epoch: 57.598684210526315
Training for 300 epoch: 58.76083333333334
Training for 600 epoch: 58.4
Training for 1000 epoch: 58.21958333333333
[[57.986842105263165, 57.92763157894737, 57.598684210526315], [58.76083333333334, 58.4, 58.21958333333333]]
train loss 0.8971360984166463, epoch 119, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [120][20/30]	Time  1.527 ( 1.545)	Data  0.039 ( 0.056)	InnerLoop  0.646 ( 0.655)	Loss 4.5400e-01 (4.7974e-01)	Acc@1  85.03 ( 83.68)
The current update step is 3630
GPU_0_using curriculum 20 with window 20
Epoch: [121][20/30]	Time  1.509 ( 1.547)	Data  0.038 ( 0.063)	InnerLoop  0.640 ( 0.649)	Loss 5.3274e-01 (5.4816e-01)	Acc@1  80.83 ( 80.77)
The current update step is 3660
GPU_0_using curriculum 20 with window 20
Epoch: [122][20/30]	Time  1.533 ( 1.547)	Data  0.038 ( 0.057)	InnerLoop  0.663 ( 0.656)	Loss 4.7787e-01 (5.1631e-01)	Acc@1  84.23 ( 82.30)
The current update step is 3690
GPU_0_using curriculum 20 with window 20
Epoch: [123][20/30]	Time  1.518 ( 1.544)	Data  0.038 ( 0.063)	InnerLoop  0.649 ( 0.648)	Loss 4.9598e-01 (5.2684e-01)	Acc@1  83.11 ( 82.12)
The current update step is 3720
GPU_0_using curriculum 20 with window 20
Epoch: [124][20/30]	Time  1.633 ( 1.544)	Data  0.158 ( 0.062)	InnerLoop  0.633 ( 0.648)	Loss 4.6594e-01 (5.0892e-01)	Acc@1  84.03 ( 82.60)
The current update step is 3750
The current seed is 17447213880883245815
The current lr is: 0.0012
Testing Results:
 *   Acc@1 56.184
 *   Acc@1 56.843
 *   Acc@1 58.342
 *   Acc@1 58.635
 *   Acc@1 59.053
 *   Acc@1 59.471
 *   Acc@1 61.697
 *   Acc@1 62.183
 *   Acc@1 60.211
 *   Acc@1 60.540
 *   Acc@1 60.013
 *   Acc@1 60.782
Training for 300 epoch: 58.940789473684205
Training for 600 epoch: 59.276315789473685
Training for 1000 epoch: 59.53289473684211
Training for 300 epoch: 59.51291666666667
Training for 600 epoch: 59.5875
Training for 1000 epoch: 60.126666666666665
[[58.940789473684205, 59.276315789473685, 59.53289473684211], [59.51291666666667, 59.5875, 60.126666666666665]]
train loss 1.257100970586141, epoch 124, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [125][20/30]	Time  1.634 ( 1.549)	Data  0.036 ( 0.050)	InnerLoop  0.758 ( 0.665)	Loss 4.9596e-01 (4.9168e-01)	Acc@1  83.35 ( 83.33)
The current update step is 3780
GPU_0_using curriculum 20 with window 20
Epoch: [126][20/30]	Time  1.523 ( 1.544)	Data  0.038 ( 0.050)	InnerLoop  0.633 ( 0.659)	Loss 5.1795e-01 (5.0948e-01)	Acc@1  82.25 ( 82.64)
The current update step is 3810
GPU_0_using curriculum 20 with window 20
Epoch: [127][20/30]	Time  1.513 ( 1.547)	Data  0.039 ( 0.057)	InnerLoop  0.647 ( 0.656)	Loss 5.0702e-01 (5.0897e-01)	Acc@1  82.79 ( 82.34)
The current update step is 3840
GPU_0_using curriculum 20 with window 20
Epoch: [128][20/30]	Time  1.528 ( 1.542)	Data  0.041 ( 0.044)	InnerLoop  0.645 ( 0.664)	Loss 4.7885e-01 (5.0255e-01)	Acc@1  83.89 ( 82.67)
The current update step is 3870
GPU_0_using curriculum 20 with window 20
Epoch: [129][20/30]	Time  1.508 ( 1.545)	Data  0.038 ( 0.050)	InnerLoop  0.637 ( 0.662)	Loss 6.3081e-01 (5.5226e-01)	Acc@1  77.91 ( 80.77)
The current update step is 3900
The current seed is 2701858353555020631
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.987
 *   Acc@1 65.326
 *   Acc@1 64.592
 *   Acc@1 64.112
 *   Acc@1 49.974
 *   Acc@1 50.319
 *   Acc@1 52.171
 *   Acc@1 52.678
 *   Acc@1 53.763
 *   Acc@1 54.163
 *   Acc@1 54.079
 *   Acc@1 54.693
Training for 300 epoch: 58.578947368421055
Training for 600 epoch: 59.17763157894737
Training for 1000 epoch: 52.026315789473685
Training for 300 epoch: 59.00208333333333
Training for 600 epoch: 59.13791666666667
Training for 1000 epoch: 52.50625
[[58.578947368421055, 59.17763157894737, 52.026315789473685], [59.00208333333333, 59.13791666666667, 52.50625]]
train loss 1.392723590596517, epoch 129, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [130][20/30]	Time  1.516 ( 1.545)	Data  0.039 ( 0.050)	InnerLoop  0.631 ( 0.661)	Loss 4.6258e-01 (5.1639e-01)	Acc@1  84.94 ( 82.08)
The current update step is 3930
GPU_0_using curriculum 20 with window 20
Epoch: [131][20/30]	Time  1.509 ( 1.540)	Data  0.040 ( 0.068)	InnerLoop  0.640 ( 0.642)	Loss 4.3122e-01 (4.9328e-01)	Acc@1  85.11 ( 83.09)
The current update step is 3960
GPU_0_using curriculum 20 with window 20
Epoch: [132][20/30]	Time  1.505 ( 1.548)	Data  0.039 ( 0.068)	InnerLoop  0.634 ( 0.647)	Loss 6.1689e-01 (4.9522e-01)	Acc@1  77.12 ( 83.00)
The current update step is 3990
GPU_0_using curriculum 20 with window 20
Epoch: [133][20/30]	Time  1.628 ( 1.548)	Data  0.038 ( 0.069)	InnerLoop  0.746 ( 0.645)	Loss 4.5831e-01 (4.9278e-01)	Acc@1  84.50 ( 83.20)
The current update step is 4020
GPU_0_using curriculum 20 with window 20
Epoch: [134][20/30]	Time  1.524 ( 1.541)	Data  0.041 ( 0.056)	InnerLoop  0.653 ( 0.652)	Loss 4.6303e-01 (4.7943e-01)	Acc@1  83.62 ( 83.53)
The current update step is 4050
The current seed is 15546813001092772064
The current lr is: 0.0012
Testing Results:
 *   Acc@1 59.763
 *   Acc@1 61.238
 *   Acc@1 61.039
 *   Acc@1 61.513
 *   Acc@1 60.671
 *   Acc@1 61.608
 *   Acc@1 49.553
 *   Acc@1 49.509
 *   Acc@1 49.434
 *   Acc@1 49.777
 *   Acc@1 52.842
 *   Acc@1 53.755
Training for 300 epoch: 54.65789473684211
Training for 600 epoch: 55.23684210526316
Training for 1000 epoch: 56.756578947368425
Training for 300 epoch: 55.37375
Training for 600 epoch: 55.64458333333333
Training for 1000 epoch: 57.68166666666667
[[54.65789473684211, 55.23684210526316, 56.756578947368425], [55.37375, 55.64458333333333, 57.68166666666667]]
train loss 1.4597578183492024, epoch 134, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [135][20/30]	Time  1.513 ( 1.548)	Data  0.039 ( 0.057)	InnerLoop  0.644 ( 0.660)	Loss 4.8239e-01 (4.7768e-01)	Acc@1  82.84 ( 83.67)
The current update step is 4080
GPU_0_using curriculum 20 with window 20
Epoch: [136][20/30]	Time  1.620 ( 1.547)	Data  0.037 ( 0.056)	InnerLoop  0.751 ( 0.658)	Loss 4.7077e-01 (4.8561e-01)	Acc@1  84.35 ( 83.46)
The current update step is 4110
GPU_0_using curriculum 20 with window 20
Epoch: [137][20/30]	Time  1.512 ( 1.540)	Data  0.038 ( 0.050)	InnerLoop  0.641 ( 0.658)	Loss 5.9116e-01 (4.8657e-01)	Acc@1  78.71 ( 83.24)
The current update step is 4140
GPU_0_using curriculum 20 with window 20
Epoch: [138][20/30]	Time  1.508 ( 1.540)	Data  0.039 ( 0.056)	InnerLoop  0.638 ( 0.652)	Loss 4.8900e-01 (4.7800e-01)	Acc@1  82.47 ( 83.48)
The current update step is 4170
GPU_0_using curriculum 20 with window 20
Epoch: [139][20/30]	Time  1.543 ( 1.546)	Data  0.039 ( 0.044)	InnerLoop  0.638 ( 0.668)	Loss 4.9969e-01 (5.0064e-01)	Acc@1  83.52 ( 82.82)
The current update step is 4200
The current seed is 15843216495854206672
The current lr is: 0.0012
Testing Results:
 *   Acc@1 56.158
 *   Acc@1 56.588
 *   Acc@1 58.855
 *   Acc@1 58.898
 *   Acc@1 58.789
 *   Acc@1 58.930
 *   Acc@1 61.829
 *   Acc@1 61.941
 *   Acc@1 64.053
 *   Acc@1 64.665
 *   Acc@1 63.092
 *   Acc@1 63.727
Training for 300 epoch: 58.993421052631575
Training for 600 epoch: 61.453947368421055
Training for 1000 epoch: 60.94078947368421
Training for 300 epoch: 59.264583333333334
Training for 600 epoch: 61.781666666666666
Training for 1000 epoch: 61.32833333333333
[[58.993421052631575, 61.453947368421055, 60.94078947368421], [59.264583333333334, 61.781666666666666, 61.32833333333333]]
train loss 0.9942670783678691, epoch 139, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [140][20/30]	Time  1.627 ( 1.550)	Data  0.039 ( 0.068)	InnerLoop  0.757 ( 0.647)	Loss 4.9573e-01 (4.8370e-01)	Acc@1  82.67 ( 83.71)
The current update step is 4230
GPU_0_using curriculum 20 with window 20
Epoch: [141][20/30]	Time  1.512 ( 1.542)	Data  0.039 ( 0.057)	InnerLoop  0.641 ( 0.653)	Loss 5.5125e-01 (4.8639e-01)	Acc@1  80.32 ( 83.44)
The current update step is 4260
GPU_0_using curriculum 20 with window 20
Epoch: [142][20/30]	Time  1.528 ( 1.543)	Data  0.039 ( 0.044)	InnerLoop  0.642 ( 0.666)	Loss 4.8765e-01 (4.8170e-01)	Acc@1  83.30 ( 83.38)
The current update step is 4290
GPU_0_using curriculum 20 with window 20
Epoch: [143][20/30]	Time  1.524 ( 1.543)	Data  0.040 ( 0.051)	InnerLoop  0.639 ( 0.658)	Loss 5.6121e-01 (4.9823e-01)	Acc@1  78.91 ( 82.67)
The current update step is 4320
GPU_0_using curriculum 20 with window 20
Epoch: [144][20/30]	Time  1.627 ( 1.551)	Data  0.161 ( 0.069)	InnerLoop  0.635 ( 0.647)	Loss 4.4580e-01 (4.7795e-01)	Acc@1  85.23 ( 83.68)
The current update step is 4350
The current seed is 16109828891280236141
The current lr is: 0.0012
Testing Results:
 *   Acc@1 63.816
 *   Acc@1 64.178
 *   Acc@1 65.605
 *   Acc@1 66.585
 *   Acc@1 67.474
 *   Acc@1 68.412
 *   Acc@1 67.184
 *   Acc@1 68.259
 *   Acc@1 71.763
 *   Acc@1 71.368
 *   Acc@1 71.632
 *   Acc@1 71.819
Training for 300 epoch: 65.5
Training for 600 epoch: 68.68421052631578
Training for 1000 epoch: 69.55263157894737
Training for 300 epoch: 66.21875
Training for 600 epoch: 68.97666666666666
Training for 1000 epoch: 70.11541666666666
[[65.5, 68.68421052631578, 69.55263157894737], [66.21875, 68.97666666666666, 70.11541666666666]]
train loss 0.9107698240598043, epoch 144, best loss 0.8148760060310364, best_epoch 94
GPU_0_using curriculum 20 with window 20
Epoch: [145][20/30]	Time  1.529 ( 1.544)	Data  0.039 ( 0.050)	InnerLoop  0.656 ( 0.660)	Loss 4.9631e-01 (5.0457e-01)	Acc@1  82.57 ( 82.47)
The current update step is 4380
GPU_0_using curriculum 20 with window 20
Epoch: [146][20/30]	Time  1.643 ( 1.552)	Data  0.156 ( 0.068)	InnerLoop  0.639 ( 0.647)	Loss 6.7044e-01 (5.5739e-01)	Acc@1  73.32 ( 81.02)
The current update step is 4410
GPU_0_using curriculum 20 with window 20
Epoch: [147][20/30]	Time  1.512 ( 1.547)	Data  0.038 ( 0.057)	InnerLoop  0.642 ( 0.655)	Loss 4.5750e-01 (5.1053e-01)	Acc@1  84.59 ( 82.66)
The current update step is 4440
GPU_0_using curriculum 20 with window 20
Epoch: [148][20/30]	Time  1.540 ( 1.541)	Data  0.038 ( 0.044)	InnerLoop  0.648 ( 0.665)	Loss 5.2312e-01 (5.2235e-01)	Acc@1  82.40 ( 81.68)
The current update step is 4470
GPU_0_using curriculum 20 with window 20
Epoch: [149][20/30]	Time  1.522 ( 1.545)	Data  0.040 ( 0.069)	InnerLoop  0.648 ( 0.642)	Loss 5.1779e-01 (4.9978e-01)	Acc@1  82.03 ( 82.56)
The current update step is 4500
The current seed is 7137650968999399979
The current lr is: 0.0012
Testing Results:
 *   Acc@1 73.158
 *   Acc@1 73.770
 *   Acc@1 73.987
 *   Acc@1 74.288
 *   Acc@1 73.842
 *   Acc@1 74.616
 *   Acc@1 66.329
 *   Acc@1 66.398
 *   Acc@1 68.500
 *   Acc@1 68.577
 *   Acc@1 68.408
 *   Acc@1 68.843
Training for 300 epoch: 69.74342105263159
Training for 600 epoch: 71.24342105263159
Training for 1000 epoch: 71.125
Training for 300 epoch: 70.08416666666666
Training for 600 epoch: 71.4325
Training for 1000 epoch: 71.72958333333332
[[69.74342105263159, 71.24342105263159, 71.125], [70.08416666666666, 71.4325, 71.72958333333332]]
train loss 0.7306614384333293, epoch 149, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [150][20/30]	Time  1.652 ( 1.556)	Data  0.041 ( 0.070)	InnerLoop  0.771 ( 0.650)	Loss 4.5661e-01 (5.0989e-01)	Acc@1  84.59 ( 82.60)
The current update step is 4530
GPU_0_using curriculum 20 with window 20
Epoch: [151][20/30]	Time  1.525 ( 1.544)	Data  0.039 ( 0.057)	InnerLoop  0.647 ( 0.654)	Loss 4.8680e-01 (4.9381e-01)	Acc@1  81.88 ( 82.89)
The current update step is 4560
GPU_0_using curriculum 20 with window 20
Epoch: [152][20/30]	Time  1.511 ( 1.545)	Data  0.042 ( 0.045)	InnerLoop  0.639 ( 0.668)	Loss 6.1241e-01 (5.0724e-01)	Acc@1  78.56 ( 82.39)
The current update step is 4590
GPU_0_using curriculum 20 with window 20
Epoch: [153][20/30]	Time  1.508 ( 1.545)	Data  0.039 ( 0.050)	InnerLoop  0.637 ( 0.662)	Loss 4.6210e-01 (5.2528e-01)	Acc@1  83.89 ( 81.84)
The current update step is 4620
GPU_0_using curriculum 20 with window 20
Epoch: [154][20/30]	Time  1.631 ( 1.549)	Data  0.158 ( 0.070)	InnerLoop  0.634 ( 0.648)	Loss 5.7909e-01 (5.2197e-01)	Acc@1  81.47 ( 82.25)
The current update step is 4650
The current seed is 12020528840249589685
The current lr is: 0.0012
Testing Results:
 *   Acc@1 51.105
 *   Acc@1 51.400
 *   Acc@1 51.105
 *   Acc@1 51.714
 *   Acc@1 51.184
 *   Acc@1 52.146
 *   Acc@1 49.132
 *   Acc@1 49.256
 *   Acc@1 51.908
 *   Acc@1 52.062
 *   Acc@1 53.171
 *   Acc@1 53.507
Training for 300 epoch: 50.118421052631575
Training for 600 epoch: 51.506578947368425
Training for 1000 epoch: 52.17763157894737
Training for 300 epoch: 50.32791666666667
Training for 600 epoch: 51.88791666666667
Training for 1000 epoch: 52.82625
[[50.118421052631575, 51.506578947368425, 52.17763157894737], [50.32791666666667, 51.88791666666667, 52.82625]]
train loss 1.244455993080139, epoch 154, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [155][20/30]	Time  1.524 ( 1.546)	Data  0.036 ( 0.051)	InnerLoop  0.649 ( 0.662)	Loss 4.4066e-01 (5.3299e-01)	Acc@1  84.96 ( 81.68)
The current update step is 4680
GPU_0_using curriculum 20 with window 20
Epoch: [156][20/30]	Time  1.660 ( 1.571)	Data  0.159 ( 0.071)	InnerLoop  0.658 ( 0.655)	Loss 4.6204e-01 (4.8706e-01)	Acc@1  84.08 ( 83.45)
The current update step is 4710
GPU_0_using curriculum 20 with window 20
Epoch: [157][20/30]	Time  1.548 ( 1.574)	Data  0.042 ( 0.059)	InnerLoop  0.651 ( 0.667)	Loss 4.8562e-01 (5.0871e-01)	Acc@1  83.72 ( 82.64)
The current update step is 4740
GPU_0_using curriculum 20 with window 20
Epoch: [158][20/30]	Time  1.526 ( 1.573)	Data  0.042 ( 0.047)	InnerLoop  0.643 ( 0.679)	Loss 4.6873e-01 (4.9448e-01)	Acc@1  84.47 ( 83.05)
The current update step is 4770
GPU_0_using curriculum 20 with window 20
Epoch: [159][20/30]	Time  1.547 ( 1.558)	Data  0.044 ( 0.071)	InnerLoop  0.656 ( 0.645)	Loss 4.4366e-01 (4.8664e-01)	Acc@1  84.50 ( 83.46)
The current update step is 4800
The current seed is 5926812099875956255
The current lr is: 0.0012
Testing Results:
 *   Acc@1 68.276
 *   Acc@1 69.575
 *   Acc@1 67.724
 *   Acc@1 68.593
 *   Acc@1 66.500
 *   Acc@1 67.524
 *   Acc@1 46.855
 *   Acc@1 47.358
 *   Acc@1 49.368
 *   Acc@1 50.063
 *   Acc@1 51.184
 *   Acc@1 51.890
Training for 300 epoch: 57.56578947368421
Training for 600 epoch: 58.546052631578945
Training for 1000 epoch: 58.84210526315789
Training for 300 epoch: 58.46625
Training for 600 epoch: 59.32833333333333
Training for 1000 epoch: 59.70708333333334
[[57.56578947368421, 58.546052631578945, 58.84210526315789], [58.46625, 59.32833333333333, 59.70708333333334]]
train loss 1.4102466860453289, epoch 159, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [160][20/30]	Time  1.659 ( 1.557)	Data  0.039 ( 0.070)	InnerLoop  0.773 ( 0.649)	Loss 5.1874e-01 (4.8711e-01)	Acc@1  82.01 ( 83.28)
The current update step is 4830
GPU_0_using curriculum 20 with window 20
Epoch: [161][20/30]	Time  1.522 ( 1.543)	Data  0.041 ( 0.057)	InnerLoop  0.644 ( 0.651)	Loss 5.7137e-01 (5.4443e-01)	Acc@1  78.52 ( 81.12)
The current update step is 4860
GPU_0_using curriculum 20 with window 20
Epoch: [162][20/30]	Time  1.510 ( 1.550)	Data  0.039 ( 0.046)	InnerLoop  0.637 ( 0.665)	Loss 6.2100e-01 (5.1621e-01)	Acc@1  78.93 ( 82.37)
The current update step is 4890
GPU_0_using curriculum 20 with window 20
Epoch: [163][20/30]	Time  1.505 ( 1.541)	Data  0.042 ( 0.052)	InnerLoop  0.631 ( 0.653)	Loss 4.7670e-01 (5.1160e-01)	Acc@1  83.28 ( 82.53)
The current update step is 4920
GPU_0_using curriculum 20 with window 20
Epoch: [164][20/30]	Time  1.620 ( 1.544)	Data  0.159 ( 0.069)	InnerLoop  0.631 ( 0.641)	Loss 5.5434e-01 (5.1473e-01)	Acc@1  81.32 ( 82.36)
The current update step is 4950
The current seed is 4195618622649401594
The current lr is: 0.0012
Testing Results:
 *   Acc@1 53.513
 *   Acc@1 53.474
 *   Acc@1 57.053
 *   Acc@1 57.659
 *   Acc@1 57.842
 *   Acc@1 58.182
 *   Acc@1 58.789
 *   Acc@1 58.903
 *   Acc@1 60.803
 *   Acc@1 61.113
 *   Acc@1 47.737
 *   Acc@1 48.683
Training for 300 epoch: 56.151315789473685
Training for 600 epoch: 58.92763157894737
Training for 1000 epoch: 52.78947368421053
Training for 300 epoch: 56.18875
Training for 600 epoch: 59.386250000000004
Training for 1000 epoch: 53.432916666666664
[[56.151315789473685, 58.92763157894737, 52.78947368421053], [56.18875, 59.386250000000004, 53.432916666666664]]
train loss 1.257721990521749, epoch 164, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [165][20/30]	Time  1.508 ( 1.536)	Data  0.040 ( 0.050)	InnerLoop  0.638 ( 0.654)	Loss 5.9759e-01 (5.1026e-01)	Acc@1  79.37 ( 82.65)
The current update step is 4980
GPU_0_using curriculum 20 with window 20
Epoch: [166][20/30]	Time  1.625 ( 1.543)	Data  0.156 ( 0.069)	InnerLoop  0.634 ( 0.642)	Loss 4.8847e-01 (4.9924e-01)	Acc@1  84.81 ( 83.15)
The current update step is 5010
GPU_0_using curriculum 20 with window 20
Epoch: [167][20/30]	Time  1.502 ( 1.538)	Data  0.038 ( 0.056)	InnerLoop  0.636 ( 0.648)	Loss 5.3112e-01 (4.9614e-01)	Acc@1  83.81 ( 83.02)
The current update step is 5040
GPU_0_using curriculum 20 with window 20
Epoch: [168][20/30]	Time  1.524 ( 1.536)	Data  0.040 ( 0.045)	InnerLoop  0.641 ( 0.659)	Loss 4.7588e-01 (4.9960e-01)	Acc@1  83.98 ( 83.10)
The current update step is 5070
GPU_0_using curriculum 20 with window 20
Epoch: [169][20/30]	Time  1.503 ( 1.538)	Data  0.040 ( 0.069)	InnerLoop  0.627 ( 0.637)	Loss 4.4455e-01 (5.0127e-01)	Acc@1  85.35 ( 82.59)
The current update step is 5100
The current seed is 14568390575022801895
The current lr is: 0.0012
Testing Results:
 *   Acc@1 63.750
 *   Acc@1 64.546
 *   Acc@1 67.316
 *   Acc@1 68.221
 *   Acc@1 68.092
 *   Acc@1 68.977
 *   Acc@1 67.882
 *   Acc@1 68.203
 *   Acc@1 68.539
 *   Acc@1 68.883
 *   Acc@1 68.566
 *   Acc@1 68.921
Training for 300 epoch: 65.81578947368422
Training for 600 epoch: 67.92763157894737
Training for 1000 epoch: 68.32894736842104
Training for 300 epoch: 66.37458333333333
Training for 600 epoch: 68.55208333333334
Training for 1000 epoch: 68.94875
[[65.81578947368422, 67.92763157894737, 68.32894736842104], [66.37458333333333, 68.55208333333334, 68.94875]]
train loss 0.7905785346666971, epoch 169, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [170][20/30]	Time  1.643 ( 1.549)	Data  0.040 ( 0.069)	InnerLoop  0.763 ( 0.645)	Loss 5.0133e-01 (5.0587e-01)	Acc@1  83.37 ( 82.71)
The current update step is 5130
GPU_0_using curriculum 20 with window 20
Epoch: [171][20/30]	Time  1.503 ( 1.542)	Data  0.038 ( 0.056)	InnerLoop  0.634 ( 0.648)	Loss 5.9154e-01 (5.2363e-01)	Acc@1  80.52 ( 81.92)
The current update step is 5160
GPU_0_using curriculum 20 with window 20
Epoch: [172][20/30]	Time  1.525 ( 1.537)	Data  0.039 ( 0.044)	InnerLoop  0.635 ( 0.661)	Loss 5.5346e-01 (5.2856e-01)	Acc@1  81.18 ( 81.87)
The current update step is 5190
GPU_0_using curriculum 20 with window 20
Epoch: [173][20/30]	Time  1.493 ( 1.537)	Data  0.036 ( 0.051)	InnerLoop  0.631 ( 0.653)	Loss 5.1627e-01 (5.0437e-01)	Acc@1  83.50 ( 82.73)
The current update step is 5220
GPU_0_using curriculum 20 with window 20
Epoch: [174][20/30]	Time  1.661 ( 1.544)	Data  0.152 ( 0.069)	InnerLoop  0.658 ( 0.642)	Loss 4.6679e-01 (5.0355e-01)	Acc@1  84.52 ( 82.70)
The current update step is 5250
The current seed is 5043810806072457507
The current lr is: 0.0012
Testing Results:
 *   Acc@1 58.961
 *   Acc@1 58.848
 *   Acc@1 59.671
 *   Acc@1 60.047
 *   Acc@1 60.434
 *   Acc@1 60.846
 *   Acc@1 63.197
 *   Acc@1 63.291
 *   Acc@1 63.263
 *   Acc@1 63.514
 *   Acc@1 63.487
 *   Acc@1 63.497
Training for 300 epoch: 61.078947368421055
Training for 600 epoch: 61.46710526315789
Training for 1000 epoch: 61.96052631578947
Training for 300 epoch: 61.069583333333334
Training for 600 epoch: 61.78041666666667
Training for 1000 epoch: 62.17125
[[61.078947368421055, 61.46710526315789, 61.96052631578947], [61.069583333333334, 61.78041666666667, 62.17125]]
train loss 1.1547519383748373, epoch 174, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [175][20/30]	Time  1.516 ( 1.538)	Data  0.039 ( 0.050)	InnerLoop  0.636 ( 0.655)	Loss 4.8138e-01 (4.9363e-01)	Acc@1  83.37 ( 82.95)
The current update step is 5280
GPU_0_using curriculum 20 with window 20
Epoch: [176][20/30]	Time  1.624 ( 1.543)	Data  0.157 ( 0.067)	InnerLoop  0.636 ( 0.644)	Loss 5.0938e-01 (5.2070e-01)	Acc@1  81.32 ( 81.94)
The current update step is 5310
GPU_0_using curriculum 20 with window 20
Epoch: [177][20/30]	Time  1.519 ( 1.534)	Data  0.039 ( 0.056)	InnerLoop  0.641 ( 0.646)	Loss 5.0812e-01 (4.9956e-01)	Acc@1  82.74 ( 82.69)
The current update step is 5340
GPU_0_using curriculum 20 with window 20
Epoch: [178][20/30]	Time  1.502 ( 1.537)	Data  0.037 ( 0.044)	InnerLoop  0.634 ( 0.662)	Loss 4.4934e-01 (4.9489e-01)	Acc@1  85.11 ( 83.24)
The current update step is 5370
GPU_0_using curriculum 20 with window 20
Epoch: [179][20/30]	Time  1.511 ( 1.533)	Data  0.039 ( 0.068)	InnerLoop  0.639 ( 0.634)	Loss 4.6691e-01 (5.0070e-01)	Acc@1  84.42 ( 82.78)
The current update step is 5400
The current seed is 7428436520474417118
The current lr is: 0.0012
Testing Results:
 *   Acc@1 53.947
 *   Acc@1 54.253
 *   Acc@1 54.303
 *   Acc@1 54.148
 *   Acc@1 53.368
 *   Acc@1 53.486
 *   Acc@1 58.763
 *   Acc@1 58.944
 *   Acc@1 60.711
 *   Acc@1 60.307
 *   Acc@1 61.026
 *   Acc@1 60.697
Training for 300 epoch: 56.35526315789474
Training for 600 epoch: 57.506578947368425
Training for 1000 epoch: 57.19736842105263
Training for 300 epoch: 56.598749999999995
Training for 600 epoch: 57.2275
Training for 1000 epoch: 57.09125
[[56.35526315789474, 57.506578947368425, 57.19736842105263], [56.598749999999995, 57.2275, 57.09125]]
train loss 1.181515109761556, epoch 179, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [180][20/30]	Time  1.644 ( 1.543)	Data  0.037 ( 0.068)	InnerLoop  0.758 ( 0.641)	Loss 4.5936e-01 (4.9945e-01)	Acc@1  84.16 ( 82.80)
The current update step is 5430
GPU_0_using curriculum 20 with window 20
Epoch: [181][20/30]	Time  1.509 ( 1.538)	Data  0.041 ( 0.055)	InnerLoop  0.635 ( 0.648)	Loss 4.9559e-01 (4.9095e-01)	Acc@1  83.33 ( 82.77)
The current update step is 5460
GPU_0_using curriculum 20 with window 20
Epoch: [182][20/30]	Time  1.502 ( 1.541)	Data  0.041 ( 0.045)	InnerLoop  0.632 ( 0.661)	Loss 5.1302e-01 (4.9923e-01)	Acc@1  81.79 ( 82.78)
The current update step is 5490
GPU_0_using curriculum 20 with window 20
Epoch: [183][20/30]	Time  1.501 ( 1.540)	Data  0.037 ( 0.049)	InnerLoop  0.636 ( 0.656)	Loss 5.2169e-01 (5.0268e-01)	Acc@1  81.47 ( 82.73)
The current update step is 5520
GPU_0_using curriculum 20 with window 20
Epoch: [184][20/30]	Time  1.624 ( 1.544)	Data  0.152 ( 0.068)	InnerLoop  0.632 ( 0.641)	Loss 4.8098e-01 (5.0524e-01)	Acc@1  83.40 ( 82.41)
The current update step is 5550
The current seed is 13435431439030880274
The current lr is: 0.0012
Testing Results:
 *   Acc@1 41.895
 *   Acc@1 41.976
 *   Acc@1 44.000
 *   Acc@1 44.592
 *   Acc@1 46.566
 *   Acc@1 46.428
 *   Acc@1 68.145
 *   Acc@1 68.618
 *   Acc@1 68.105
 *   Acc@1 68.418
 *   Acc@1 66.868
 *   Acc@1 67.547
Training for 300 epoch: 55.01973684210526
Training for 600 epoch: 56.05263157894737
Training for 1000 epoch: 56.71710526315789
Training for 300 epoch: 55.29666666666667
Training for 600 epoch: 56.505
Training for 1000 epoch: 56.98708333333333
[[55.01973684210526, 56.05263157894737, 56.71710526315789], [55.29666666666667, 56.505, 56.98708333333333]]
train loss 0.766623933060964, epoch 184, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [185][20/30]	Time  1.518 ( 1.538)	Data  0.040 ( 0.050)	InnerLoop  0.647 ( 0.655)	Loss 4.5093e-01 (4.8009e-01)	Acc@1  84.30 ( 83.47)
The current update step is 5580
GPU_0_using curriculum 20 with window 20
Epoch: [186][20/30]	Time  1.616 ( 1.543)	Data  0.155 ( 0.068)	InnerLoop  0.634 ( 0.642)	Loss 4.6863e-01 (4.9937e-01)	Acc@1  83.35 ( 82.68)
The current update step is 5610
GPU_0_using curriculum 20 with window 20
Epoch: [187][20/30]	Time  1.497 ( 1.533)	Data  0.039 ( 0.056)	InnerLoop  0.629 ( 0.648)	Loss 5.4187e-01 (4.9064e-01)	Acc@1  81.81 ( 83.13)
The current update step is 5640
GPU_0_using curriculum 20 with window 20
Epoch: [188][20/30]	Time  1.538 ( 1.538)	Data  0.039 ( 0.044)	InnerLoop  0.641 ( 0.660)	Loss 5.3777e-01 (5.1240e-01)	Acc@1  81.30 ( 82.16)
The current update step is 5670
GPU_0_using curriculum 20 with window 20
Epoch: [189][20/30]	Time  1.537 ( 1.551)	Data  0.042 ( 0.071)	InnerLoop  0.649 ( 0.641)	Loss 6.0864e-01 (5.0540e-01)	Acc@1  79.05 ( 82.60)
The current update step is 5700
The current seed is 17009744698316555690
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.461
 *   Acc@1 65.203
 *   Acc@1 63.447
 *   Acc@1 64.106
 *   Acc@1 53.605
 *   Acc@1 53.645
 *   Acc@1 49.408
 *   Acc@1 48.952
 *   Acc@1 50.513
 *   Acc@1 49.984
 *   Acc@1 50.711
 *   Acc@1 50.360
Training for 300 epoch: 56.934210526315795
Training for 600 epoch: 56.98026315789474
Training for 1000 epoch: 52.15789473684211
Training for 300 epoch: 57.0775
Training for 600 epoch: 57.045
Training for 1000 epoch: 52.0025
[[56.934210526315795, 56.98026315789474, 52.15789473684211], [57.0775, 57.045, 52.0025]]
train loss 1.577735141690572, epoch 189, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [190][20/30]	Time  1.644 ( 1.562)	Data  0.042 ( 0.070)	InnerLoop  0.758 ( 0.651)	Loss 4.6072e-01 (4.9453e-01)	Acc@1  84.06 ( 82.86)
The current update step is 5730
GPU_0_using curriculum 20 with window 20
Epoch: [191][20/30]	Time  1.547 ( 1.556)	Data  0.037 ( 0.058)	InnerLoop  0.650 ( 0.656)	Loss 4.7967e-01 (5.0740e-01)	Acc@1  83.79 ( 82.77)
The current update step is 5760
GPU_0_using curriculum 20 with window 20
Epoch: [192][20/30]	Time  1.512 ( 1.539)	Data  0.038 ( 0.044)	InnerLoop  0.633 ( 0.661)	Loss 5.2764e-01 (4.8608e-01)	Acc@1  81.03 ( 83.08)
The current update step is 5790
GPU_0_using curriculum 20 with window 20
Epoch: [193][20/30]	Time  1.512 ( 1.539)	Data  0.040 ( 0.050)	InnerLoop  0.640 ( 0.653)	Loss 4.5931e-01 (4.9176e-01)	Acc@1  83.96 ( 83.10)
The current update step is 5820
GPU_0_using curriculum 20 with window 20
Epoch: [194][20/30]	Time  1.619 ( 1.545)	Data  0.155 ( 0.068)	InnerLoop  0.631 ( 0.643)	Loss 4.7271e-01 (4.7568e-01)	Acc@1  83.50 ( 83.77)
The current update step is 5850
The current seed is 10298026054248423928
The current lr is: 0.0012
Testing Results:
 *   Acc@1 36.474
 *   Acc@1 36.429
 *   Acc@1 50.645
 *   Acc@1 50.813
 *   Acc@1 49.066
 *   Acc@1 49.182
 *   Acc@1 47.316
 *   Acc@1 47.593
 *   Acc@1 44.763
 *   Acc@1 44.543
 *   Acc@1 43.855
 *   Acc@1 43.985
Training for 300 epoch: 41.89473684210526
Training for 600 epoch: 47.703947368421055
Training for 1000 epoch: 46.46052631578948
Training for 300 epoch: 42.01083333333334
Training for 600 epoch: 47.678333333333335
Training for 1000 epoch: 46.58333333333333
[[41.89473684210526, 47.703947368421055, 46.46052631578948], [42.01083333333334, 47.678333333333335, 46.58333333333333]]
train loss 1.9247829627990722, epoch 194, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [195][20/30]	Time  1.536 ( 1.546)	Data  0.036 ( 0.050)	InnerLoop  0.636 ( 0.658)	Loss 4.8702e-01 (5.0143e-01)	Acc@1  83.59 ( 82.73)
The current update step is 5880
GPU_0_using curriculum 20 with window 20
Epoch: [196][20/30]	Time  1.626 ( 1.547)	Data  0.158 ( 0.068)	InnerLoop  0.633 ( 0.645)	Loss 5.5546e-01 (5.0044e-01)	Acc@1  81.32 ( 82.89)
The current update step is 5910
GPU_0_using curriculum 20 with window 20
Epoch: [197][20/30]	Time  1.511 ( 1.535)	Data  0.039 ( 0.057)	InnerLoop  0.634 ( 0.645)	Loss 5.1153e-01 (4.9516e-01)	Acc@1  82.64 ( 82.86)
The current update step is 5940
GPU_0_using curriculum 20 with window 20
Epoch: [198][20/30]	Time  1.501 ( 1.537)	Data  0.037 ( 0.043)	InnerLoop  0.632 ( 0.661)	Loss 4.6686e-01 (5.0943e-01)	Acc@1  84.59 ( 82.18)
The current update step is 5970
GPU_0_using curriculum 20 with window 20
Epoch: [199][20/30]	Time  1.504 ( 1.536)	Data  0.040 ( 0.069)	InnerLoop  0.636 ( 0.634)	Loss 4.9411e-01 (4.9284e-01)	Acc@1  83.08 ( 83.07)
The current update step is 6000
The current seed is 14038675565277366212
The current lr is: 0.0012
Testing Results:
 *   Acc@1 45.132
 *   Acc@1 44.657
 *   Acc@1 56.342
 *   Acc@1 56.179
 *   Acc@1 57.408
 *   Acc@1 57.266
 *   Acc@1 54.934
 *   Acc@1 54.594
 *   Acc@1 57.684
 *   Acc@1 57.408
 *   Acc@1 59.921
 *   Acc@1 59.856
Training for 300 epoch: 50.0328947368421
Training for 600 epoch: 57.01315789473684
Training for 1000 epoch: 58.66447368421052
Training for 300 epoch: 49.62583333333333
Training for 600 epoch: 56.79375
Training for 1000 epoch: 58.560833333333335
[[50.0328947368421, 57.01315789473684, 58.66447368421052], [49.62583333333333, 56.79375, 58.560833333333335]]
train loss 1.2195605739593507, epoch 199, best loss 0.7306614384333293, best_epoch 149
GPU_0_using curriculum 20 with window 20
Epoch: [200][20/30]	Time  1.629 ( 1.538)	Data  0.037 ( 0.067)	InnerLoop  0.763 ( 0.641)	Loss 5.1253e-01 (5.0592e-01)	Acc@1  82.71 ( 82.63)
The current update step is 6030
GPU_0_using curriculum 20 with window 20
Epoch: [201][20/30]	Time  1.492 ( 1.531)	Data  0.040 ( 0.056)	InnerLoop  0.627 ( 0.646)	Loss 4.4073e-01 (5.0134e-01)	Acc@1  85.08 ( 82.95)
The current update step is 6060
GPU_0_using curriculum 20 with window 20
Epoch: [202][20/30]	Time  1.516 ( 1.534)	Data  0.045 ( 0.044)	InnerLoop  0.642 ( 0.658)	Loss 5.4854e-01 (4.9176e-01)	Acc@1  82.01 ( 83.26)
The current update step is 6090
GPU_0_using curriculum 20 with window 20
Epoch: [203][20/30]	Time  1.505 ( 1.533)	Data  0.038 ( 0.051)	InnerLoop  0.637 ( 0.652)	Loss 5.0265e-01 (4.8797e-01)	Acc@1  83.01 ( 83.44)
The current update step is 6120
GPU_0_using curriculum 20 with window 20
Epoch: [204][20/30]	Time  1.610 ( 1.540)	Data  0.156 ( 0.068)	InnerLoop  0.625 ( 0.640)	Loss 4.6312e-01 (4.7604e-01)	Acc@1  84.06 ( 83.72)
The current update step is 6150
The current seed is 16174049145606727791
The current lr is: 0.0012
Testing Results:
 *   Acc@1 45.026
 *   Acc@1 45.009
 *   Acc@1 47.434
 *   Acc@1 47.868
 *   Acc@1 49.039
 *   Acc@1 49.228
 *   Acc@1 69.276
 *   Acc@1 69.765
 *   Acc@1 69.118
 *   Acc@1 69.850
 *   Acc@1 69.974
 *   Acc@1 70.022
Training for 300 epoch: 57.151315789473685
Training for 600 epoch: 58.276315789473685
Training for 1000 epoch: 59.506578947368425
Training for 300 epoch: 57.38708333333334
Training for 600 epoch: 58.85916666666667
Training for 1000 epoch: 59.625
[[57.151315789473685, 58.276315789473685, 59.506578947368425], [57.38708333333334, 58.85916666666667, 59.625]]
train loss 0.6758205777168274, epoch 204, best loss 0.6758205777168274, best_epoch 204
GPU_0_using curriculum 20 with window 20
Epoch: [205][20/30]	Time  1.509 ( 1.536)	Data  0.036 ( 0.051)	InnerLoop  0.636 ( 0.653)	Loss 4.2331e-01 (4.8298e-01)	Acc@1  85.67 ( 83.60)
The current update step is 6180
GPU_0_using curriculum 20 with window 20
Epoch: [206][20/30]	Time  1.626 ( 1.548)	Data  0.154 ( 0.069)	InnerLoop  0.639 ( 0.646)	Loss 5.0859e-01 (4.8004e-01)	Acc@1  83.08 ( 83.76)
The current update step is 6210
GPU_0_using curriculum 20 with window 20
Epoch: [207][20/30]	Time  1.518 ( 1.544)	Data  0.039 ( 0.056)	InnerLoop  0.642 ( 0.656)	Loss 4.8415e-01 (4.8435e-01)	Acc@1  83.23 ( 83.29)
The current update step is 6240
GPU_0_using curriculum 20 with window 20
Epoch: [208][20/30]	Time  1.509 ( 1.544)	Data  0.039 ( 0.044)	InnerLoop  0.640 ( 0.665)	Loss 5.0142e-01 (4.7786e-01)	Acc@1  83.45 ( 83.68)
The current update step is 6270
GPU_0_using curriculum 20 with window 20
Epoch: [209][20/30]	Time  1.513 ( 1.544)	Data  0.042 ( 0.069)	InnerLoop  0.640 ( 0.643)	Loss 4.8479e-01 (4.9818e-01)	Acc@1  83.23 ( 82.88)
The current update step is 6300
The current seed is 3469276696584244382
The current lr is: 0.0012
Testing Results:
 *   Acc@1 54.816
 *   Acc@1 55.594
 *   Acc@1 53.395
 *   Acc@1 53.157
 *   Acc@1 52.592
 *   Acc@1 52.429
 *   Acc@1 39.697
 *   Acc@1 39.458
 *   Acc@1 46.237
 *   Acc@1 45.574
 *   Acc@1 46.789
 *   Acc@1 46.510
Training for 300 epoch: 47.256578947368425
Training for 600 epoch: 49.815789473684205
Training for 1000 epoch: 49.69078947368421
Training for 300 epoch: 47.52583333333334
Training for 600 epoch: 49.36541666666666
Training for 1000 epoch: 49.46958333333333
[[47.256578947368425, 49.815789473684205, 49.69078947368421], [47.52583333333334, 49.36541666666666, 49.46958333333333]]
train loss 1.8303835369110106, epoch 209, best loss 0.6758205777168274, best_epoch 204
GPU_0_using curriculum 20 with window 20
Epoch: [210][20/30]	Time  1.627 ( 1.550)	Data  0.040 ( 0.070)	InnerLoop  0.760 ( 0.648)	Loss 5.3283e-01 (4.8329e-01)	Acc@1  80.86 ( 83.49)
The current update step is 6330
GPU_0_using curriculum 20 with window 20
Epoch: [211][20/30]	Time  1.505 ( 1.549)	Data  0.038 ( 0.058)	InnerLoop  0.636 ( 0.656)	Loss 5.0828e-01 (4.9495e-01)	Acc@1  82.35 ( 82.95)
The current update step is 6360
GPU_0_using curriculum 20 with window 20
Epoch: [212][20/30]	Time  1.530 ( 1.555)	Data  0.045 ( 0.047)	InnerLoop  0.646 ( 0.670)	Loss 5.0891e-01 (4.8144e-01)	Acc@1  82.13 ( 83.53)
The current update step is 6390
GPU_0_using curriculum 20 with window 20
Epoch: [213][20/30]	Time  1.518 ( 1.553)	Data  0.038 ( 0.053)	InnerLoop  0.646 ( 0.661)	Loss 4.4573e-01 (4.9363e-01)	Acc@1  84.69 ( 82.97)
The current update step is 6420
GPU_0_using curriculum 20 with window 20
Epoch: [214][20/30]	Time  1.642 ( 1.559)	Data  0.161 ( 0.071)	InnerLoop  0.644 ( 0.649)	Loss 4.7630e-01 (5.0356e-01)	Acc@1  84.28 ( 82.58)
The current update step is 6450
The current seed is 16192579096749625303
The current lr is: 0.0012
Testing Results:
 *   Acc@1 61.092
 *   Acc@1 60.957
 *   Acc@1 59.566
 *   Acc@1 59.465
 *   Acc@1 64.368
 *   Acc@1 64.513
 *   Acc@1 74.803
 *   Acc@1 74.678
 *   Acc@1 74.408
 *   Acc@1 74.552
 *   Acc@1 74.329
 *   Acc@1 74.608
Training for 300 epoch: 67.94736842105263
Training for 600 epoch: 66.98684210526316
Training for 1000 epoch: 69.34868421052632
Training for 300 epoch: 67.8175
Training for 600 epoch: 67.00874999999999
Training for 1000 epoch: 69.56083333333333
[[67.94736842105263, 66.98684210526316, 69.34868421052632], [67.8175, 67.00874999999999, 69.56083333333333]]
train loss 0.7033368236223857, epoch 214, best loss 0.6758205777168274, best_epoch 204
GPU_0_using curriculum 20 with window 20
Epoch: [215][20/30]	Time  1.503 ( 1.549)	Data  0.038 ( 0.052)	InnerLoop  0.631 ( 0.660)	Loss 4.6744e-01 (4.9523e-01)	Acc@1  83.52 ( 82.78)
The current update step is 6480
GPU_0_using curriculum 20 with window 20
Epoch: [216][20/30]	Time  1.637 ( 1.546)	Data  0.155 ( 0.068)	InnerLoop  0.628 ( 0.643)	Loss 4.5020e-01 (4.9172e-01)	Acc@1  85.03 ( 82.78)
The current update step is 6510
GPU_0_using curriculum 20 with window 20
Epoch: [217][20/30]	Time  1.520 ( 1.540)	Data  0.040 ( 0.057)	InnerLoop  0.640 ( 0.649)	Loss 4.5709e-01 (5.0533e-01)	Acc@1  84.96 ( 82.86)
The current update step is 6540
GPU_0_using curriculum 20 with window 20
Epoch: [218][20/30]	Time  1.514 ( 1.546)	Data  0.040 ( 0.046)	InnerLoop  0.639 ( 0.667)	Loss 4.4287e-01 (4.8681e-01)	Acc@1  84.96 ( 83.16)
The current update step is 6570
GPU_0_using curriculum 20 with window 20
Epoch: [219][20/30]	Time  1.526 ( 1.550)	Data  0.039 ( 0.069)	InnerLoop  0.651 ( 0.644)	Loss 5.2828e-01 (5.0008e-01)	Acc@1  81.35 ( 82.46)
The current update step is 6600
The current seed is 8453242846744500939
The current lr is: 0.0012
Testing Results:
 *   Acc@1 77.776
 *   Acc@1 78.739
 *   Acc@1 77.171
 *   Acc@1 78.086
 *   Acc@1 77.053
 *   Acc@1 77.733
 *   Acc@1 56.618
 *   Acc@1 57.095
 *   Acc@1 49.724
 *   Acc@1 49.758
 *   Acc@1 49.842
 *   Acc@1 49.392
Training for 300 epoch: 67.19736842105263
Training for 600 epoch: 63.44736842105263
Training for 1000 epoch: 63.44736842105263
Training for 300 epoch: 67.91708333333332
Training for 600 epoch: 63.92166666666667
Training for 1000 epoch: 63.5625
[[67.19736842105263, 63.44736842105263, 63.44736842105263], [67.91708333333332, 63.92166666666667, 63.5625]]
train loss 1.3484741430282592, epoch 219, best loss 0.6758205777168274, best_epoch 204
GPU_0_using curriculum 20 with window 20
Epoch: [220][20/30]	Time  1.643 ( 1.548)	Data  0.040 ( 0.069)	InnerLoop  0.759 ( 0.645)	Loss 4.8087e-01 (4.9930e-01)	Acc@1  83.54 ( 82.67)
The current update step is 6630
GPU_0_using curriculum 20 with window 20
Epoch: [221][20/30]	Time  1.509 ( 1.542)	Data  0.039 ( 0.057)	InnerLoop  0.637 ( 0.652)	Loss 4.4236e-01 (5.0364e-01)	Acc@1  84.45 ( 82.63)
The current update step is 6660
GPU_0_using curriculum 20 with window 20
Epoch: [222][20/30]	Time  1.520 ( 1.538)	Data  0.040 ( 0.046)	InnerLoop  0.642 ( 0.664)	Loss 4.5380e-01 (4.8484e-01)	Acc@1  84.30 ( 83.09)
The current update step is 6690
GPU_0_using curriculum 20 with window 20
Epoch: [223][20/30]	Time  1.506 ( 1.533)	Data  0.039 ( 0.050)	InnerLoop  0.637 ( 0.655)	Loss 5.7546e-01 (4.8616e-01)	Acc@1  78.66 ( 83.25)
The current update step is 6720
GPU_0_using curriculum 20 with window 20
Epoch: [224][20/30]	Time  1.618 ( 1.543)	Data  0.156 ( 0.068)	InnerLoop  0.633 ( 0.646)	Loss 5.1601e-01 (4.8609e-01)	Acc@1  82.20 ( 83.29)
The current update step is 6750
The current seed is 9829957370115479515
The current lr is: 0.0012
Testing Results:
 *   Acc@1 68.447
 *   Acc@1 69.461
 *   Acc@1 68.763
 *   Acc@1 69.941
 *   Acc@1 69.947
 *   Acc@1 70.463
 *   Acc@1 45.250
 *   Acc@1 45.108
 *   Acc@1 51.066
 *   Acc@1 51.391
 *   Acc@1 51.618
 *   Acc@1 51.669
Training for 300 epoch: 56.848684210526315
Training for 600 epoch: 59.91447368421052
Training for 1000 epoch: 60.78289473684211
Training for 300 epoch: 57.28458333333333
Training for 600 epoch: 60.66583333333333
Training for 1000 epoch: 61.066250000000004
[[56.848684210526315, 59.91447368421052, 60.78289473684211], [57.28458333333333, 60.66583333333333, 61.066250000000004]]
train loss 1.3618285659154257, epoch 224, best loss 0.6758205777168274, best_epoch 204
GPU_0_using curriculum 20 with window 20
Epoch: [225][20/30]	Time  1.523 ( 1.535)	Data  0.038 ( 0.051)	InnerLoop  0.639 ( 0.655)	Loss 4.6353e-01 (4.6639e-01)	Acc@1  84.64 ( 83.96)
The current update step is 6780
GPU_0_using curriculum 20 with window 20
Epoch: [226][20/30]	Time  1.617 ( 1.542)	Data  0.155 ( 0.069)	InnerLoop  0.636 ( 0.644)	Loss 5.0932e-01 (4.9632e-01)	Acc@1  82.93 ( 82.40)
The current update step is 6810
GPU_0_using curriculum 20 with window 20
Epoch: [227][20/30]	Time  1.502 ( 1.540)	Data  0.039 ( 0.057)	InnerLoop  0.635 ( 0.651)	Loss 5.0199e-01 (4.7947e-01)	Acc@1  83.03 ( 83.65)
The current update step is 6840
GPU_0_using curriculum 20 with window 20
Epoch: [228][20/30]	Time  1.503 ( 1.533)	Data  0.042 ( 0.046)	InnerLoop  0.638 ( 0.660)	Loss 5.0269e-01 (4.9261e-01)	Acc@1  82.54 ( 82.89)
The current update step is 6870
GPU_0_using curriculum 20 with window 20
Epoch: [229][20/30]	Time  1.546 ( 1.545)	Data  0.040 ( 0.069)	InnerLoop  0.646 ( 0.644)	Loss 4.8954e-01 (4.8483e-01)	Acc@1  83.91 ( 83.36)
The current update step is 6900
The current seed is 5798871714983952440
The current lr is: 0.0012
Testing Results:
 *   Acc@1 56.224
 *   Acc@1 56.437
 *   Acc@1 74.776
 *   Acc@1 75.618
 *   Acc@1 74.592
 *   Acc@1 74.571
 *   Acc@1 65.579
 *   Acc@1 65.788
 *   Acc@1 57.079
 *   Acc@1 56.981
 *   Acc@1 60.974
 *   Acc@1 60.924
Training for 300 epoch: 60.901315789473685
Training for 600 epoch: 65.92763157894737
Training for 1000 epoch: 67.78289473684211
Training for 300 epoch: 61.1125
Training for 600 epoch: 66.29916666666668
Training for 1000 epoch: 67.7475
[[60.901315789473685, 65.92763157894737, 67.78289473684211], [61.1125, 66.29916666666668, 67.7475]]
train loss 1.0354767786026, epoch 229, best loss 0.6758205777168274, best_epoch 204
GPU_0_using curriculum 20 with window 20
Epoch: [230][20/30]	Time  1.680 ( 1.585)	Data  0.042 ( 0.073)	InnerLoop  0.780 ( 0.667)	Loss 4.7195e-01 (4.7956e-01)	Acc@1  82.08 ( 83.31)
The current update step is 6930
GPU_0_using curriculum 20 with window 20
Epoch: [231][20/30]	Time  1.578 ( 1.580)	Data  0.040 ( 0.059)	InnerLoop  0.694 ( 0.675)	Loss 4.4075e-01 (4.7991e-01)	Acc@1  84.40 ( 83.47)
The current update step is 6960
GPU_0_using curriculum 20 with window 20
Epoch: [232][20/30]	Time  1.547 ( 1.579)	Data  0.041 ( 0.048)	InnerLoop  0.659 ( 0.685)	Loss 5.7473e-01 (4.9311e-01)	Acc@1  77.27 ( 82.95)
The current update step is 6990
GPU_0_using curriculum 20 with window 20
Epoch: [233][20/30]	Time  1.534 ( 1.575)	Data  0.045 ( 0.054)	InnerLoop  0.650 ( 0.679)	Loss 4.6720e-01 (4.7336e-01)	Acc@1  84.16 ( 83.70)
The current update step is 7020
GPU_0_using curriculum 20 with window 20
Epoch: [234][20/30]	Time  1.646 ( 1.562)	Data  0.159 ( 0.071)	InnerLoop  0.655 ( 0.654)	Loss 4.8211e-01 (4.8702e-01)	Acc@1  83.25 ( 83.09)
The current update step is 7050
The current seed is 10234150073507774033
The current lr is: 0.0012
Testing Results:
 *   Acc@1 70.408
 *   Acc@1 70.845
 *   Acc@1 71.237
 *   Acc@1 71.885
 *   Acc@1 71.789
 *   Acc@1 71.916
 *   Acc@1 55.868
 *   Acc@1 56.063
 *   Acc@1 56.632
 *   Acc@1 57.447
 *   Acc@1 58.684
 *   Acc@1 59.116
Training for 300 epoch: 63.13815789473685
Training for 600 epoch: 63.934210526315795
Training for 1000 epoch: 65.23684210526315
Training for 300 epoch: 63.454166666666666
Training for 600 epoch: 64.66625
Training for 1000 epoch: 65.51583333333333
[[63.13815789473685, 63.934210526315795, 65.23684210526315], [63.454166666666666, 64.66625, 65.51583333333333]]
train loss 0.8783669528007507, epoch 234, best loss 0.6758205777168274, best_epoch 204
GPU_0_using curriculum 20 with window 20
Epoch: [235][20/30]	Time  1.521 ( 1.552)	Data  0.040 ( 0.051)	InnerLoop  0.642 ( 0.664)	Loss 6.1599e-01 (4.8345e-01)	Acc@1  78.81 ( 83.39)
The current update step is 7080
GPU_0_using curriculum 20 with window 20
Epoch: [236][20/30]	Time  1.631 ( 1.556)	Data  0.155 ( 0.070)	InnerLoop  0.639 ( 0.649)	Loss 5.1750e-01 (4.9806e-01)	Acc@1  80.69 ( 82.72)
The current update step is 7110
GPU_0_using curriculum 20 with window 20
Epoch: [237][20/30]	Time  1.526 ( 1.550)	Data  0.040 ( 0.057)	InnerLoop  0.643 ( 0.654)	Loss 4.4103e-01 (4.9678e-01)	Acc@1  85.55 ( 82.81)
The current update step is 7140
GPU_0_using curriculum 20 with window 20
Epoch: [238][20/30]	Time  1.526 ( 1.545)	Data  0.039 ( 0.045)	InnerLoop  0.653 ( 0.667)	Loss 4.7407e-01 (4.9787e-01)	Acc@1  82.67 ( 82.60)
The current update step is 7170
GPU_0_using curriculum 20 with window 20
Epoch: [239][20/30]	Time  1.506 ( 1.539)	Data  0.037 ( 0.069)	InnerLoop  0.638 ( 0.639)	Loss 5.8707e-01 (5.1668e-01)	Acc@1  81.13 ( 82.00)
The current update step is 7200
The current seed is 8409253144143050165
The current lr is: 0.0012
Testing Results:
 *   Acc@1 74.132
 *   Acc@1 74.329
 *   Acc@1 72.461
 *   Acc@1 73.385
 *   Acc@1 72.408
 *   Acc@1 72.802
 *   Acc@1 74.842
 *   Acc@1 75.167
 *   Acc@1 76.421
 *   Acc@1 76.613
 *   Acc@1 77.013
 *   Acc@1 77.079
Training for 300 epoch: 74.48684210526315
Training for 600 epoch: 74.44078947368422
Training for 1000 epoch: 74.71052631578948
Training for 300 epoch: 74.74791666666667
Training for 600 epoch: 74.99916666666667
Training for 1000 epoch: 74.94041666666666
[[74.48684210526315, 74.44078947368422, 74.71052631578948], [74.74791666666667, 74.99916666666667, 74.94041666666666]]
train loss 0.5412657897313435, epoch 239, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [240][20/30]	Time  1.639 ( 1.544)	Data  0.041 ( 0.068)	InnerLoop  0.751 ( 0.642)	Loss 5.3450e-01 (4.9973e-01)	Acc@1  80.66 ( 82.67)
The current update step is 7230
GPU_0_using curriculum 20 with window 20
Epoch: [241][20/30]	Time  1.526 ( 1.546)	Data  0.038 ( 0.057)	InnerLoop  0.639 ( 0.654)	Loss 6.0325e-01 (5.1697e-01)	Acc@1  79.00 ( 82.04)
The current update step is 7260
GPU_0_using curriculum 20 with window 20
Epoch: [242][20/30]	Time  1.526 ( 1.546)	Data  0.041 ( 0.046)	InnerLoop  0.656 ( 0.666)	Loss 4.5592e-01 (4.7271e-01)	Acc@1  84.08 ( 84.00)
The current update step is 7290
GPU_0_using curriculum 20 with window 20
Epoch: [243][20/30]	Time  1.522 ( 1.539)	Data  0.037 ( 0.051)	InnerLoop  0.644 ( 0.658)	Loss 4.4580e-01 (4.7319e-01)	Acc@1  84.81 ( 83.79)
The current update step is 7320
GPU_0_using curriculum 20 with window 20
Epoch: [244][20/30]	Time  1.631 ( 1.549)	Data  0.156 ( 0.069)	InnerLoop  0.637 ( 0.647)	Loss 4.4357e-01 (4.8125e-01)	Acc@1  84.50 ( 83.47)
The current update step is 7350
The current seed is 1979766006171278267
The current lr is: 0.0012
Testing Results:
 *   Acc@1 63.750
 *   Acc@1 63.959
 *   Acc@1 59.618
 *   Acc@1 60.580
 *   Acc@1 62.382
 *   Acc@1 63.112
 *   Acc@1 61.368
 *   Acc@1 60.934
 *   Acc@1 62.658
 *   Acc@1 62.763
 *   Acc@1 64.671
 *   Acc@1 64.217
Training for 300 epoch: 62.559210526315795
Training for 600 epoch: 61.13815789473684
Training for 1000 epoch: 63.526315789473685
Training for 300 epoch: 62.44666666666667
Training for 600 epoch: 61.67166666666667
Training for 1000 epoch: 63.66458333333333
[[62.559210526315795, 61.13815789473684, 63.526315789473685], [62.44666666666667, 61.67166666666667, 63.66458333333333]]
train loss 1.1526821630477906, epoch 244, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [245][20/30]	Time  1.552 ( 1.545)	Data  0.040 ( 0.050)	InnerLoop  0.642 ( 0.661)	Loss 4.4464e-01 (4.9884e-01)	Acc@1  85.11 ( 82.38)
The current update step is 7380
GPU_0_using curriculum 20 with window 20
Epoch: [246][20/30]	Time  1.620 ( 1.546)	Data  0.156 ( 0.069)	InnerLoop  0.633 ( 0.645)	Loss 5.2762e-01 (5.0094e-01)	Acc@1  82.71 ( 82.63)
The current update step is 7410
GPU_0_using curriculum 20 with window 20
Epoch: [247][20/30]	Time  1.507 ( 1.543)	Data  0.039 ( 0.056)	InnerLoop  0.637 ( 0.653)	Loss 4.5138e-01 (4.9811e-01)	Acc@1  84.01 ( 82.43)
The current update step is 7440
GPU_0_using curriculum 20 with window 20
Epoch: [248][20/30]	Time  1.514 ( 1.544)	Data  0.037 ( 0.044)	InnerLoop  0.645 ( 0.664)	Loss 5.5375e-01 (4.9369e-01)	Acc@1  80.13 ( 82.93)
The current update step is 7470
GPU_0_using curriculum 20 with window 20
Epoch: [249][20/30]	Time  1.522 ( 1.543)	Data  0.041 ( 0.069)	InnerLoop  0.651 ( 0.640)	Loss 5.3889e-01 (4.9034e-01)	Acc@1  81.20 ( 82.91)
The current update step is 7500
The current seed is 11173734460113111986
The current lr is: 0.0012
Testing Results:
 *   Acc@1 62.421
 *   Acc@1 62.273
 *   Acc@1 62.118
 *   Acc@1 61.781
 *   Acc@1 63.263
 *   Acc@1 62.973
 *   Acc@1 54.697
 *   Acc@1 55.623
 *   Acc@1 58.132
 *   Acc@1 58.426
 *   Acc@1 57.487
 *   Acc@1 58.278
Training for 300 epoch: 58.55921052631579
Training for 600 epoch: 60.125
Training for 1000 epoch: 60.375
Training for 300 epoch: 58.94833333333334
Training for 600 epoch: 60.10333333333334
Training for 1000 epoch: 60.62583333333333
[[58.55921052631579, 60.125, 60.375], [58.94833333333334, 60.10333333333334, 60.62583333333333]]
train loss 1.224976586341858, epoch 249, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [250][20/30]	Time  1.639 ( 1.554)	Data  0.042 ( 0.071)	InnerLoop  0.757 ( 0.648)	Loss 4.3481e-01 (4.8582e-01)	Acc@1  85.25 ( 83.42)
The current update step is 7530
GPU_0_using curriculum 20 with window 20
Epoch: [251][20/30]	Time  1.520 ( 1.544)	Data  0.037 ( 0.057)	InnerLoop  0.641 ( 0.653)	Loss 5.7953e-01 (4.9678e-01)	Acc@1  78.52 ( 82.57)
The current update step is 7560
GPU_0_using curriculum 20 with window 20
Epoch: [252][20/30]	Time  1.517 ( 1.543)	Data  0.039 ( 0.044)	InnerLoop  0.647 ( 0.666)	Loss 4.5237e-01 (4.9020e-01)	Acc@1  84.59 ( 82.97)
The current update step is 7590
GPU_0_using curriculum 20 with window 20
Epoch: [253][20/30]	Time  1.512 ( 1.544)	Data  0.041 ( 0.051)	InnerLoop  0.638 ( 0.658)	Loss 5.0371e-01 (4.9304e-01)	Acc@1  82.71 ( 82.82)
The current update step is 7620
GPU_0_using curriculum 20 with window 20
Epoch: [254][20/30]	Time  1.631 ( 1.550)	Data  0.156 ( 0.069)	InnerLoop  0.636 ( 0.647)	Loss 4.4942e-01 (4.8564e-01)	Acc@1  84.79 ( 83.21)
The current update step is 7650
The current seed is 16771014634007192302
The current lr is: 0.0012
Testing Results:
 *   Acc@1 42.697
 *   Acc@1 42.655
 *   Acc@1 45.868
 *   Acc@1 45.968
 *   Acc@1 48.513
 *   Acc@1 47.962
 *   Acc@1 53.355
 *   Acc@1 53.523
 *   Acc@1 64.105
 *   Acc@1 64.779
 *   Acc@1 66.026
 *   Acc@1 66.662
Training for 300 epoch: 48.026315789473685
Training for 600 epoch: 54.986842105263165
Training for 1000 epoch: 57.26973684210526
Training for 300 epoch: 48.088750000000005
Training for 600 epoch: 55.373333333333335
Training for 1000 epoch: 57.312083333333334
[[48.026315789473685, 54.986842105263165, 57.26973684210526], [48.088750000000005, 55.373333333333335, 57.312083333333334]]
train loss 0.6171858325322469, epoch 254, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [255][20/30]	Time  1.528 ( 1.543)	Data  0.037 ( 0.050)	InnerLoop  0.642 ( 0.659)	Loss 4.9018e-01 (5.0468e-01)	Acc@1  83.20 ( 82.52)
The current update step is 7680
GPU_0_using curriculum 20 with window 20
Epoch: [256][20/30]	Time  1.656 ( 1.555)	Data  0.163 ( 0.070)	InnerLoop  0.649 ( 0.649)	Loss 5.1484e-01 (5.0983e-01)	Acc@1  81.91 ( 82.17)
The current update step is 7710
GPU_0_using curriculum 20 with window 20
Epoch: [257][20/30]	Time  1.520 ( 1.545)	Data  0.040 ( 0.057)	InnerLoop  0.643 ( 0.654)	Loss 4.6370e-01 (4.8954e-01)	Acc@1  83.69 ( 82.96)
The current update step is 7740
GPU_0_using curriculum 20 with window 20
Epoch: [258][20/30]	Time  1.512 ( 1.549)	Data  0.038 ( 0.045)	InnerLoop  0.639 ( 0.667)	Loss 4.8119e-01 (4.8268e-01)	Acc@1  83.47 ( 83.10)
The current update step is 7770
GPU_0_using curriculum 20 with window 20
Epoch: [259][20/30]	Time  1.530 ( 1.542)	Data  0.038 ( 0.069)	InnerLoop  0.648 ( 0.639)	Loss 4.8617e-01 (4.8479e-01)	Acc@1  83.69 ( 83.17)
The current update step is 7800
The current seed is 1018927234852316391
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.395
 *   Acc@1 65.061
 *   Acc@1 44.079
 *   Acc@1 44.358
 *   Acc@1 44.316
 *   Acc@1 44.128
 *   Acc@1 53.684
 *   Acc@1 54.279
 *   Acc@1 52.132
 *   Acc@1 51.978
 *   Acc@1 52.750
 *   Acc@1 53.204
Training for 300 epoch: 59.03947368421052
Training for 600 epoch: 48.10526315789474
Training for 1000 epoch: 48.53289473684211
Training for 300 epoch: 59.67
Training for 600 epoch: 48.16791666666667
Training for 1000 epoch: 48.66625
[[59.03947368421052, 48.10526315789474, 48.53289473684211], [59.67, 48.16791666666667, 48.66625]]
train loss 1.4080409704844157, epoch 259, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [260][20/30]	Time  1.641 ( 1.553)	Data  0.036 ( 0.068)	InnerLoop  0.767 ( 0.647)	Loss 4.5222e-01 (5.1403e-01)	Acc@1  83.84 ( 82.22)
The current update step is 7830
GPU_0_using curriculum 20 with window 20
Epoch: [261][20/30]	Time  1.514 ( 1.546)	Data  0.040 ( 0.057)	InnerLoop  0.644 ( 0.653)	Loss 4.7428e-01 (4.7563e-01)	Acc@1  84.50 ( 83.69)
The current update step is 7860
GPU_0_using curriculum 20 with window 20
Epoch: [262][20/30]	Time  1.511 ( 1.542)	Data  0.039 ( 0.044)	InnerLoop  0.640 ( 0.666)	Loss 4.8549e-01 (4.6323e-01)	Acc@1  82.89 ( 83.76)
The current update step is 7890
GPU_0_using curriculum 20 with window 20
Epoch: [263][20/30]	Time  1.509 ( 1.539)	Data  0.038 ( 0.050)	InnerLoop  0.647 ( 0.658)	Loss 5.1562e-01 (5.0638e-01)	Acc@1  80.86 ( 82.36)
The current update step is 7920
GPU_0_using curriculum 20 with window 20
Epoch: [264][20/30]	Time  1.634 ( 1.551)	Data  0.158 ( 0.069)	InnerLoop  0.643 ( 0.649)	Loss 4.5327e-01 (4.9019e-01)	Acc@1  84.06 ( 83.03)
The current update step is 7950
The current seed is 13775629417907034937
The current lr is: 0.0012
Testing Results:
 *   Acc@1 69.250
 *   Acc@1 69.289
 *   Acc@1 69.658
 *   Acc@1 70.162
 *   Acc@1 70.592
 *   Acc@1 70.975
 *   Acc@1 52.987
 *   Acc@1 53.191
 *   Acc@1 57.026
 *   Acc@1 57.236
 *   Acc@1 58.421
 *   Acc@1 58.970
Training for 300 epoch: 61.118421052631575
Training for 600 epoch: 63.3421052631579
Training for 1000 epoch: 64.50657894736841
Training for 300 epoch: 61.24
Training for 600 epoch: 63.69875
Training for 1000 epoch: 64.9725
[[61.118421052631575, 63.3421052631579, 64.50657894736841], [61.24, 63.69875, 64.9725]]
train loss 1.1485815042495728, epoch 264, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [265][20/30]	Time  1.525 ( 1.550)	Data  0.037 ( 0.052)	InnerLoop  0.636 ( 0.661)	Loss 5.9351e-01 (5.0235e-01)	Acc@1  79.57 ( 82.57)
The current update step is 7980
GPU_0_using curriculum 20 with window 20
Epoch: [266][20/30]	Time  1.632 ( 1.553)	Data  0.156 ( 0.069)	InnerLoop  0.643 ( 0.648)	Loss 4.9919e-01 (4.9976e-01)	Acc@1  82.15 ( 82.52)
The current update step is 8010
GPU_0_using curriculum 20 with window 20
Epoch: [267][20/30]	Time  1.523 ( 1.543)	Data  0.039 ( 0.057)	InnerLoop  0.651 ( 0.652)	Loss 5.0375e-01 (4.9737e-01)	Acc@1  81.84 ( 82.60)
The current update step is 8040
GPU_0_using curriculum 20 with window 20
Epoch: [268][20/30]	Time  1.543 ( 1.566)	Data  0.041 ( 0.046)	InnerLoop  0.658 ( 0.677)	Loss 4.8096e-01 (4.8416e-01)	Acc@1  83.84 ( 83.39)
The current update step is 8070
GPU_0_using curriculum 20 with window 20
Epoch: [269][20/30]	Time  1.541 ( 1.560)	Data  0.040 ( 0.072)	InnerLoop  0.656 ( 0.649)	Loss 4.7678e-01 (4.7190e-01)	Acc@1  83.67 ( 83.60)
The current update step is 8100
The current seed is 2490937427465640474
The current lr is: 0.0012
Testing Results:
 *   Acc@1 65.408
 *   Acc@1 66.043
 *   Acc@1 54.526
 *   Acc@1 54.605
 *   Acc@1 50.711
 *   Acc@1 50.742
 *   Acc@1 57.434
 *   Acc@1 57.830
 *   Acc@1 59.303
 *   Acc@1 59.799
 *   Acc@1 60.724
 *   Acc@1 60.941
Training for 300 epoch: 61.421052631578945
Training for 600 epoch: 56.91447368421053
Training for 1000 epoch: 55.71710526315789
Training for 300 epoch: 61.93625
Training for 600 epoch: 57.202083333333334
Training for 1000 epoch: 55.84166666666667
[[61.421052631578945, 56.91447368421053, 55.71710526315789], [61.93625, 57.202083333333334, 55.84166666666667]]
train loss 1.200251173400879, epoch 269, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [270][20/30]	Time  1.653 ( 1.566)	Data  0.042 ( 0.072)	InnerLoop  0.775 ( 0.655)	Loss 4.9249e-01 (4.8134e-01)	Acc@1  83.35 ( 83.22)
The current update step is 8130
GPU_0_using curriculum 20 with window 20
Epoch: [271][20/30]	Time  1.529 ( 1.561)	Data  0.041 ( 0.060)	InnerLoop  0.650 ( 0.661)	Loss 4.6843e-01 (4.6942e-01)	Acc@1  83.37 ( 83.98)
The current update step is 8160
GPU_0_using curriculum 20 with window 20
Epoch: [272][20/30]	Time  1.515 ( 1.550)	Data  0.040 ( 0.046)	InnerLoop  0.644 ( 0.668)	Loss 4.9404e-01 (4.9692e-01)	Acc@1  82.23 ( 82.64)
The current update step is 8190
GPU_0_using curriculum 20 with window 20
Epoch: [273][20/30]	Time  1.525 ( 1.552)	Data  0.043 ( 0.052)	InnerLoop  0.645 ( 0.665)	Loss 4.7502e-01 (4.7917e-01)	Acc@1  83.91 ( 83.54)
The current update step is 8220
GPU_0_using curriculum 20 with window 20
Epoch: [274][20/30]	Time  1.655 ( 1.559)	Data  0.162 ( 0.071)	InnerLoop  0.647 ( 0.650)	Loss 4.5019e-01 (4.8656e-01)	Acc@1  84.45 ( 83.17)
The current update step is 8250
The current seed is 3867942463834832404
The current lr is: 0.0012
Testing Results:
 *   Acc@1 58.474
 *   Acc@1 58.434
 *   Acc@1 55.658
 *   Acc@1 55.406
 *   Acc@1 55.539
 *   Acc@1 55.433
 *   Acc@1 76.329
 *   Acc@1 76.176
 *   Acc@1 75.250
 *   Acc@1 75.698
 *   Acc@1 75.053
 *   Acc@1 75.455
Training for 300 epoch: 67.40131578947368
Training for 600 epoch: 65.45394736842105
Training for 1000 epoch: 65.29605263157895
Training for 300 epoch: 67.305
Training for 600 epoch: 65.55166666666668
Training for 1000 epoch: 65.44416666666666
[[67.40131578947368, 65.45394736842105, 65.29605263157895], [67.305, 65.55166666666668, 65.44416666666666]]
train loss 0.5982925820350647, epoch 274, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [275][20/30]	Time  1.520 ( 1.549)	Data  0.038 ( 0.050)	InnerLoop  0.641 ( 0.664)	Loss 5.3096e-01 (4.8337e-01)	Acc@1  81.42 ( 83.19)
The current update step is 8280
GPU_0_using curriculum 20 with window 20
Epoch: [276][20/30]	Time  1.630 ( 1.556)	Data  0.154 ( 0.069)	InnerLoop  0.645 ( 0.651)	Loss 5.0570e-01 (4.8805e-01)	Acc@1  82.37 ( 83.06)
The current update step is 8310
GPU_0_using curriculum 20 with window 20
Epoch: [277][20/30]	Time  1.525 ( 1.553)	Data  0.039 ( 0.056)	InnerLoop  0.648 ( 0.660)	Loss 5.0521e-01 (4.8542e-01)	Acc@1  81.96 ( 83.18)
The current update step is 8340
GPU_0_using curriculum 20 with window 20
Epoch: [278][20/30]	Time  1.529 ( 1.555)	Data  0.039 ( 0.044)	InnerLoop  0.654 ( 0.673)	Loss 5.8658e-01 (5.1412e-01)	Acc@1  78.32 ( 82.29)
The current update step is 8370
GPU_0_using curriculum 20 with window 20
Epoch: [279][20/30]	Time  1.524 ( 1.545)	Data  0.039 ( 0.069)	InnerLoop  0.650 ( 0.645)	Loss 4.9068e-01 (5.0782e-01)	Acc@1  83.03 ( 82.59)
The current update step is 8400
The current seed is 10352375173579645650
The current lr is: 0.0012
Testing Results:
 *   Acc@1 59.105
 *   Acc@1 59.156
 *   Acc@1 58.961
 *   Acc@1 58.899
 *   Acc@1 59.158
 *   Acc@1 59.017
 *   Acc@1 69.276
 *   Acc@1 69.315
 *   Acc@1 69.145
 *   Acc@1 69.289
 *   Acc@1 70.039
 *   Acc@1 69.372
Training for 300 epoch: 64.19078947368422
Training for 600 epoch: 64.05263157894737
Training for 1000 epoch: 64.59868421052632
Training for 300 epoch: 64.23541666666667
Training for 600 epoch: 64.09416666666667
Training for 1000 epoch: 64.19416666666666
[[64.19078947368422, 64.05263157894737, 64.59868421052632], [64.23541666666667, 64.09416666666667, 64.19416666666666]]
train loss 0.715980731455485, epoch 279, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [280][20/30]	Time  1.643 ( 1.556)	Data  0.040 ( 0.069)	InnerLoop  0.766 ( 0.652)	Loss 5.2223e-01 (4.9735e-01)	Acc@1  81.59 ( 83.01)
The current update step is 8430
GPU_0_using curriculum 20 with window 20
Epoch: [281][20/30]	Time  1.529 ( 1.553)	Data  0.041 ( 0.058)	InnerLoop  0.652 ( 0.658)	Loss 4.7752e-01 (5.0284e-01)	Acc@1  83.74 ( 82.49)
The current update step is 8460
GPU_0_using curriculum 20 with window 20
Epoch: [282][20/30]	Time  1.519 ( 1.546)	Data  0.043 ( 0.046)	InnerLoop  0.642 ( 0.668)	Loss 5.1261e-01 (4.8826e-01)	Acc@1  81.86 ( 83.18)
The current update step is 8490
GPU_0_using curriculum 20 with window 20
Epoch: [283][20/30]	Time  1.520 ( 1.546)	Data  0.037 ( 0.051)	InnerLoop  0.655 ( 0.663)	Loss 4.6012e-01 (5.0836e-01)	Acc@1  83.96 ( 82.26)
The current update step is 8520
GPU_0_using curriculum 20 with window 20
Epoch: [284][20/30]	Time  1.635 ( 1.552)	Data  0.156 ( 0.070)	InnerLoop  0.641 ( 0.652)	Loss 4.7809e-01 (4.8702e-01)	Acc@1  83.42 ( 83.05)
The current update step is 8550
The current seed is 2937434908431589278
The current lr is: 0.0012
Testing Results:
 *   Acc@1 58.276
 *   Acc@1 58.101
 *   Acc@1 61.526
 *   Acc@1 61.704
 *   Acc@1 63.079
 *   Acc@1 63.047
 *   Acc@1 74.158
 *   Acc@1 74.746
 *   Acc@1 75.829
 *   Acc@1 76.333
 *   Acc@1 76.211
 *   Acc@1 76.813
Training for 300 epoch: 66.21710526315789
Training for 600 epoch: 68.67763157894737
Training for 1000 epoch: 69.64473684210526
Training for 300 epoch: 66.42333333333333
Training for 600 epoch: 69.01875
Training for 1000 epoch: 69.93041666666667
[[66.21710526315789, 68.67763157894737, 69.64473684210526], [66.42333333333333, 69.01875, 69.93041666666667]]
train loss 0.5820674741744996, epoch 284, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [285][20/30]	Time  1.515 ( 1.536)	Data  0.038 ( 0.050)	InnerLoop  0.637 ( 0.651)	Loss 4.8870e-01 (4.9831e-01)	Acc@1  83.35 ( 82.56)
The current update step is 8580
GPU_0_using curriculum 20 with window 20
Epoch: [286][20/30]	Time  1.642 ( 1.541)	Data  0.158 ( 0.068)	InnerLoop  0.632 ( 0.640)	Loss 4.9917e-01 (4.9133e-01)	Acc@1  81.62 ( 82.97)
The current update step is 8610
GPU_0_using curriculum 20 with window 20
Epoch: [287][20/30]	Time  1.500 ( 1.542)	Data  0.038 ( 0.057)	InnerLoop  0.630 ( 0.648)	Loss 5.5083e-01 (5.0255e-01)	Acc@1  80.71 ( 82.38)
The current update step is 8640
GPU_0_using curriculum 20 with window 20
Epoch: [288][20/30]	Time  1.504 ( 1.540)	Data  0.039 ( 0.043)	InnerLoop  0.632 ( 0.662)	Loss 6.6881e-01 (4.7628e-01)	Acc@1  77.37 ( 83.66)
The current update step is 8670
GPU_0_using curriculum 20 with window 20
Epoch: [289][20/30]	Time  1.512 ( 1.542)	Data  0.040 ( 0.069)	InnerLoop  0.637 ( 0.637)	Loss 5.4745e-01 (4.8839e-01)	Acc@1  81.47 ( 83.20)
The current update step is 8700
The current seed is 11132277720831109862
The current lr is: 0.0012
Testing Results:
 *   Acc@1 77.697
 *   Acc@1 78.014
 *   Acc@1 77.171
 *   Acc@1 77.887
 *   Acc@1 77.171
 *   Acc@1 77.581
 *   Acc@1 73.671
 *   Acc@1 74.552
 *   Acc@1 73.171
 *   Acc@1 74.022
 *   Acc@1 51.461
 *   Acc@1 51.049
Training for 300 epoch: 75.68421052631578
Training for 600 epoch: 75.17105263157895
Training for 1000 epoch: 64.3157894736842
Training for 300 epoch: 76.28333333333333
Training for 600 epoch: 75.95458333333333
Training for 1000 epoch: 64.315
[[75.68421052631578, 75.17105263157895, 64.3157894736842], [76.28333333333333, 75.95458333333333, 64.315]]
train loss 1.199172404162089, epoch 289, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [290][20/30]	Time  1.617 ( 1.543)	Data  0.041 ( 0.068)	InnerLoop  0.742 ( 0.643)	Loss 4.5017e-01 (4.7247e-01)	Acc@1  84.11 ( 83.62)
The current update step is 8730
GPU_0_using curriculum 20 with window 20
Epoch: [291][20/30]	Time  1.505 ( 1.537)	Data  0.039 ( 0.056)	InnerLoop  0.635 ( 0.649)	Loss 4.8208e-01 (4.6604e-01)	Acc@1  83.01 ( 84.00)
The current update step is 8760
GPU_0_using curriculum 20 with window 20
Epoch: [292][20/30]	Time  1.497 ( 1.535)	Data  0.037 ( 0.044)	InnerLoop  0.627 ( 0.658)	Loss 4.8286e-01 (4.8231e-01)	Acc@1  83.67 ( 83.13)
The current update step is 8790
GPU_0_using curriculum 20 with window 20
Epoch: [293][20/30]	Time  1.538 ( 1.543)	Data  0.040 ( 0.051)	InnerLoop  0.644 ( 0.655)	Loss 4.6353e-01 (5.0705e-01)	Acc@1  84.16 ( 82.35)
The current update step is 8820
GPU_0_using curriculum 20 with window 20
Epoch: [294][20/30]	Time  1.620 ( 1.541)	Data  0.155 ( 0.068)	InnerLoop  0.636 ( 0.641)	Loss 4.2960e-01 (4.9661e-01)	Acc@1  85.16 ( 82.82)
The current update step is 8850
The current seed is 14542759863573390582
The current lr is: 0.0012
Testing Results:
 *   Acc@1 63.263
 *   Acc@1 63.888
 *   Acc@1 64.921
 *   Acc@1 65.059
 *   Acc@1 65.816
 *   Acc@1 66.305
 *   Acc@1 69.434
 *   Acc@1 70.070
 *   Acc@1 38.618
 *   Acc@1 38.765
 *   Acc@1 59.263
 *   Acc@1 59.392
Training for 300 epoch: 66.34868421052632
Training for 600 epoch: 51.76973684210526
Training for 1000 epoch: 62.53947368421052
Training for 300 epoch: 66.97916666666666
Training for 600 epoch: 51.912083333333335
Training for 1000 epoch: 62.848333333333336
[[66.34868421052632, 51.76973684210526, 62.53947368421052], [66.97916666666666, 51.912083333333335, 62.848333333333336]]
train loss 1.5741449319203695, epoch 294, best loss 0.5412657897313435, best_epoch 239
GPU_0_using curriculum 20 with window 20
Epoch: [295][20/30]	Time  1.529 ( 1.539)	Data  0.036 ( 0.051)	InnerLoop  0.646 ( 0.654)	Loss 5.1411e-01 (4.9715e-01)	Acc@1  81.98 ( 82.69)
The current update step is 8880
GPU_0_using curriculum 20 with window 20
Epoch: [296][20/30]	Time  1.630 ( 1.544)	Data  0.159 ( 0.069)	InnerLoop  0.639 ( 0.644)	Loss 4.6542e-01 (4.8931e-01)	Acc@1  83.81 ( 83.20)
The current update step is 8910
GPU_0_using curriculum 20 with window 20
Epoch: [297][20/30]	Time  1.508 ( 1.534)	Data  0.039 ( 0.056)	InnerLoop  0.632 ( 0.647)	Loss 4.8349e-01 (4.7986e-01)	Acc@1  82.79 ( 83.30)
The current update step is 8940
GPU_0_using curriculum 20 with window 20
Epoch: [298][20/30]	Time  1.516 ( 1.534)	Data  0.042 ( 0.045)	InnerLoop  0.639 ( 0.657)	Loss 4.8414e-01 (4.7703e-01)	Acc@1  83.28 ( 83.56)
The current update step is 8970
GPU_0_using curriculum 20 with window 20
Epoch: [299][20/30]	Time  1.510 ( 1.540)	Data  0.040 ( 0.070)	InnerLoop  0.634 ( 0.635)	Loss 4.6789e-01 (4.6610e-01)	Acc@1  84.40 ( 83.93)
The current update step is 9000
The current seed is 2886384755572814065
The current lr is: 0.0012
Testing Results:
 *   Acc@1 67.684
 *   Acc@1 67.905
 *   Acc@1 66.816
 *   Acc@1 67.460
 *   Acc@1 66.776
 *   Acc@1 66.911
 *   Acc@1 56.579
 *   Acc@1 56.640
 *   Acc@1 57.974
 *   Acc@1 57.707
 *   Acc@1 58.224
 *   Acc@1 58.218
Training for 300 epoch: 62.131578947368425
Training for 600 epoch: 62.39473684210526
Training for 1000 epoch: 62.5
Training for 300 epoch: 62.2725
Training for 600 epoch: 62.58333333333333
Training for 1000 epoch: 62.564166666666665
[[62.131578947368425, 62.39473684210526, 62.5], [62.2725, 62.58333333333333, 62.564166666666665]]
train loss 1.2550765727996827, epoch 299, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [300][20/30]	Time  1.644 ( 1.541)	Data  0.039 ( 0.069)	InnerLoop  0.750 ( 0.639)	Loss 5.1819e-01 (4.8642e-01)	Acc@1  81.91 ( 83.13)
The current update step is 9030
GPU_0_using curriculum 20 with window 20
Epoch: [301][20/30]	Time  1.529 ( 1.540)	Data  0.042 ( 0.058)	InnerLoop  0.631 ( 0.647)	Loss 4.6053e-01 (4.7859e-01)	Acc@1  83.84 ( 83.45)
The current update step is 9060
GPU_0_using curriculum 20 with window 20
Epoch: [302][20/30]	Time  1.509 ( 1.538)	Data  0.038 ( 0.044)	InnerLoop  0.636 ( 0.659)	Loss 4.7578e-01 (4.9617e-01)	Acc@1  84.67 ( 82.70)
The current update step is 9090
GPU_0_using curriculum 20 with window 20
Epoch: [303][20/30]	Time  1.522 ( 1.541)	Data  0.040 ( 0.052)	InnerLoop  0.644 ( 0.654)	Loss 5.3632e-01 (4.9620e-01)	Acc@1  80.35 ( 82.78)
The current update step is 9120
GPU_0_using curriculum 20 with window 20
Epoch: [304][20/30]	Time  1.637 ( 1.544)	Data  0.153 ( 0.068)	InnerLoop  0.643 ( 0.640)	Loss 5.0423e-01 (4.8655e-01)	Acc@1  82.06 ( 82.94)
The current update step is 9150
The current seed is 9993684372315828501
The current lr is: 0.0012
Testing Results:
 *   Acc@1 69.605
 *   Acc@1 69.677
 *   Acc@1 69.974
 *   Acc@1 69.953
 *   Acc@1 70.447
 *   Acc@1 70.108
 *   Acc@1 71.461
 *   Acc@1 72.450
 *   Acc@1 51.487
 *   Acc@1 51.036
 *   Acc@1 55.763
 *   Acc@1 55.848
Training for 300 epoch: 70.53289473684211
Training for 600 epoch: 60.73026315789474
Training for 1000 epoch: 63.10526315789474
Training for 300 epoch: 71.06333333333333
Training for 600 epoch: 60.49416666666667
Training for 1000 epoch: 62.97833333333334
[[70.53289473684211, 60.73026315789474, 63.10526315789474], [71.06333333333333, 60.49416666666667, 62.97833333333334]]
train loss 1.2990101895650228, epoch 304, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [305][20/30]	Time  1.509 ( 1.547)	Data  0.037 ( 0.052)	InnerLoop  0.642 ( 0.659)	Loss 4.5736e-01 (5.0111e-01)	Acc@1  84.57 ( 82.52)
The current update step is 9180
GPU_0_using curriculum 20 with window 20
Epoch: [306][20/30]	Time  1.644 ( 1.551)	Data  0.154 ( 0.069)	InnerLoop  0.641 ( 0.645)	Loss 4.7092e-01 (4.9321e-01)	Acc@1  83.98 ( 82.97)
The current update step is 9210
GPU_0_using curriculum 20 with window 20
Epoch: [307][20/30]	Time  1.511 ( 1.544)	Data  0.038 ( 0.058)	InnerLoop  0.643 ( 0.653)	Loss 4.3409e-01 (5.0699e-01)	Acc@1  85.28 ( 82.30)
The current update step is 9240
GPU_0_using curriculum 20 with window 20
Epoch: [308][20/30]	Time  1.520 ( 1.550)	Data  0.040 ( 0.046)	InnerLoop  0.644 ( 0.666)	Loss 5.2183e-01 (5.0436e-01)	Acc@1  82.59 ( 82.42)
The current update step is 9270
GPU_0_using curriculum 20 with window 20
Epoch: [309][20/30]	Time  1.539 ( 1.561)	Data  0.045 ( 0.073)	InnerLoop  0.648 ( 0.646)	Loss 4.8569e-01 (5.0156e-01)	Acc@1  84.01 ( 82.41)
The current update step is 9300
The current seed is 10816112549343792951
The current lr is: 0.0012
Testing Results:
 *   Acc@1 69.184
 *   Acc@1 69.895
 *   Acc@1 70.539
 *   Acc@1 71.017
 *   Acc@1 70.816
 *   Acc@1 71.475
 *   Acc@1 44.763
 *   Acc@1 44.758
 *   Acc@1 47.789
 *   Acc@1 47.442
 *   Acc@1 49.276
 *   Acc@1 48.899
Training for 300 epoch: 56.973684210526315
Training for 600 epoch: 59.16447368421052
Training for 1000 epoch: 60.046052631578945
Training for 300 epoch: 57.32666666666667
Training for 600 epoch: 59.22958333333334
Training for 1000 epoch: 60.187083333333334
[[56.973684210526315, 59.16447368421052, 60.046052631578945], [57.32666666666667, 59.22958333333334, 60.187083333333334]]
train loss 1.7544681135813396, epoch 309, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [310][20/30]	Time  1.674 ( 1.582)	Data  0.043 ( 0.073)	InnerLoop  0.778 ( 0.661)	Loss 4.9008e-01 (4.9017e-01)	Acc@1  82.50 ( 82.76)
The current update step is 9330
GPU_0_using curriculum 20 with window 20
Epoch: [311][20/30]	Time  1.545 ( 1.581)	Data  0.044 ( 0.062)	InnerLoop  0.651 ( 0.671)	Loss 4.9827e-01 (4.9634e-01)	Acc@1  83.01 ( 82.65)
The current update step is 9360
GPU_0_using curriculum 20 with window 20
Epoch: [312][20/30]	Time  1.544 ( 1.568)	Data  0.043 ( 0.048)	InnerLoop  0.655 ( 0.677)	Loss 4.5122e-01 (4.7811e-01)	Acc@1  84.20 ( 83.29)
The current update step is 9390
GPU_0_using curriculum 20 with window 20
Epoch: [313][20/30]	Time  1.519 ( 1.555)	Data  0.041 ( 0.052)	InnerLoop  0.635 ( 0.663)	Loss 4.6726e-01 (4.9081e-01)	Acc@1  83.25 ( 82.94)
The current update step is 9420
GPU_0_using curriculum 20 with window 20
Epoch: [314][20/30]	Time  1.641 ( 1.554)	Data  0.159 ( 0.070)	InnerLoop  0.650 ( 0.648)	Loss 4.9817e-01 (4.7769e-01)	Acc@1  82.25 ( 83.59)
The current update step is 9450
The current seed is 7191534144855547928
The current lr is: 0.0012
Testing Results:
 *   Acc@1 79.039
 *   Acc@1 78.921
 *   Acc@1 69.092
 *   Acc@1 68.211
 *   Acc@1 63.289
 *   Acc@1 63.068
 *   Acc@1 74.066
 *   Acc@1 74.966
 *   Acc@1 73.855
 *   Acc@1 74.809
 *   Acc@1 74.013
 *   Acc@1 74.860
Training for 300 epoch: 76.55263157894737
Training for 600 epoch: 71.47368421052632
Training for 1000 epoch: 68.65131578947368
Training for 300 epoch: 76.94333333333333
Training for 600 epoch: 71.50999999999999
Training for 1000 epoch: 68.96416666666667
[[76.55263157894737, 71.47368421052632, 68.65131578947368], [76.94333333333333, 71.50999999999999, 68.96416666666667]]
train loss 0.6487238483111064, epoch 314, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [315][20/30]	Time  1.531 ( 1.549)	Data  0.039 ( 0.050)	InnerLoop  0.637 ( 0.662)	Loss 5.0677e-01 (4.8537e-01)	Acc@1  81.84 ( 82.96)
The current update step is 9480
GPU_0_using curriculum 20 with window 20
Epoch: [316][20/30]	Time  1.641 ( 1.553)	Data  0.156 ( 0.069)	InnerLoop  0.646 ( 0.647)	Loss 4.4042e-01 (4.7728e-01)	Acc@1  84.74 ( 83.40)
The current update step is 9510
GPU_0_using curriculum 20 with window 20
Epoch: [317][20/30]	Time  1.523 ( 1.547)	Data  0.040 ( 0.056)	InnerLoop  0.649 ( 0.654)	Loss 7.8259e-01 (5.0488e-01)	Acc@1  70.58 ( 82.18)
The current update step is 9540
GPU_0_using curriculum 20 with window 20
Epoch: [318][20/30]	Time  1.512 ( 1.539)	Data  0.038 ( 0.044)	InnerLoop  0.645 ( 0.665)	Loss 5.5736e-01 (4.8903e-01)	Acc@1  81.59 ( 83.24)
The current update step is 9570
GPU_0_using curriculum 20 with window 20
Epoch: [319][20/30]	Time  1.506 ( 1.542)	Data  0.037 ( 0.069)	InnerLoop  0.639 ( 0.642)	Loss 5.0800e-01 (5.1123e-01)	Acc@1  82.25 ( 82.08)
The current update step is 9600
The current seed is 6879389294192114665
The current lr is: 0.0012
Testing Results:
 *   Acc@1 69.224
 *   Acc@1 69.205
 *   Acc@1 69.539
 *   Acc@1 69.595
 *   Acc@1 69.882
 *   Acc@1 69.900
 *   Acc@1 66.461
 *   Acc@1 65.689
 *   Acc@1 67.013
 *   Acc@1 66.175
 *   Acc@1 67.447
 *   Acc@1 66.707
Training for 300 epoch: 67.84210526315789
Training for 600 epoch: 68.27631578947367
Training for 1000 epoch: 68.66447368421052
Training for 300 epoch: 67.44708333333332
Training for 600 epoch: 67.88499999999999
Training for 1000 epoch: 68.30333333333334
[[67.84210526315789, 68.27631578947367, 68.66447368421052], [67.44708333333332, 67.88499999999999, 68.30333333333334]]
train loss 0.9732106142361959, epoch 319, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [320][20/30]	Time  1.636 ( 1.556)	Data  0.040 ( 0.069)	InnerLoop  0.763 ( 0.650)	Loss 4.8363e-01 (5.0695e-01)	Acc@1  83.35 ( 82.22)
The current update step is 9630
GPU_0_using curriculum 20 with window 20
Epoch: [321][20/30]	Time  1.513 ( 1.541)	Data  0.038 ( 0.056)	InnerLoop  0.637 ( 0.650)	Loss 5.0274e-01 (5.0548e-01)	Acc@1  83.23 ( 82.55)
The current update step is 9660
GPU_0_using curriculum 20 with window 20
Epoch: [322][20/30]	Time  1.513 ( 1.535)	Data  0.036 ( 0.044)	InnerLoop  0.638 ( 0.661)	Loss 5.5217e-01 (4.7694e-01)	Acc@1  80.66 ( 83.62)
The current update step is 9690
GPU_0_using curriculum 20 with window 20
Epoch: [323][20/30]	Time  1.520 ( 1.535)	Data  0.040 ( 0.050)	InnerLoop  0.646 ( 0.656)	Loss 4.9636e-01 (4.7832e-01)	Acc@1  82.89 ( 83.50)
The current update step is 9720
GPU_0_using curriculum 20 with window 20
Epoch: [324][20/30]	Time  1.647 ( 1.543)	Data  0.156 ( 0.069)	InnerLoop  0.646 ( 0.646)	Loss 4.5834e-01 (4.7555e-01)	Acc@1  83.62 ( 83.75)
The current update step is 9750
The current seed is 9743432629536045424
The current lr is: 0.0012
Testing Results:
 *   Acc@1 70.816
 *   Acc@1 71.511
 *   Acc@1 67.921
 *   Acc@1 67.541
 *   Acc@1 67.408
 *   Acc@1 67.583
 *   Acc@1 65.342
 *   Acc@1 64.632
 *   Acc@1 66.829
 *   Acc@1 66.751
 *   Acc@1 67.474
 *   Acc@1 67.463
Training for 300 epoch: 68.07894736842104
Training for 600 epoch: 67.375
Training for 1000 epoch: 67.44078947368422
Training for 300 epoch: 68.07166666666666
Training for 600 epoch: 67.14583333333334
Training for 1000 epoch: 67.52291666666667
[[68.07894736842104, 67.375, 67.44078947368422], [68.07166666666666, 67.14583333333334, 67.52291666666667]]
train loss 0.8051820414861043, epoch 324, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [325][20/30]	Time  1.506 ( 1.533)	Data  0.040 ( 0.051)	InnerLoop  0.637 ( 0.655)	Loss 5.5943e-01 (5.0371e-01)	Acc@1  79.59 ( 82.26)
The current update step is 9780
GPU_0_using curriculum 20 with window 20
Epoch: [326][20/30]	Time  1.628 ( 1.548)	Data  0.156 ( 0.068)	InnerLoop  0.646 ( 0.645)	Loss 4.2749e-01 (4.7784e-01)	Acc@1  85.08 ( 83.35)
The current update step is 9810
GPU_0_using curriculum 20 with window 20
Epoch: [327][20/30]	Time  1.513 ( 1.541)	Data  0.039 ( 0.057)	InnerLoop  0.643 ( 0.652)	Loss 4.3761e-01 (4.8044e-01)	Acc@1  84.38 ( 83.49)
The current update step is 9840
GPU_0_using curriculum 20 with window 20
Epoch: [328][20/30]	Time  1.506 ( 1.537)	Data  0.040 ( 0.044)	InnerLoop  0.637 ( 0.661)	Loss 4.4250e-01 (4.7816e-01)	Acc@1  84.33 ( 83.40)
The current update step is 9870
GPU_0_using curriculum 20 with window 20
Epoch: [329][20/30]	Time  1.519 ( 1.536)	Data  0.039 ( 0.069)	InnerLoop  0.633 ( 0.638)	Loss 4.7133e-01 (4.8784e-01)	Acc@1  83.45 ( 83.25)
The current update step is 9900
The current seed is 14184935557051931571
The current lr is: 0.0012
Testing Results:
 *   Acc@1 52.474
 *   Acc@1 51.703
 *   Acc@1 49.461
 *   Acc@1 49.016
 *   Acc@1 50.105
 *   Acc@1 49.702
 *   Acc@1 64.053
 *   Acc@1 65.078
 *   Acc@1 63.316
 *   Acc@1 64.190
 *   Acc@1 63.974
 *   Acc@1 64.092
Training for 300 epoch: 58.26315789473684
Training for 600 epoch: 56.38815789473684
Training for 1000 epoch: 57.03947368421053
Training for 300 epoch: 58.39041666666667
Training for 600 epoch: 56.602916666666665
Training for 1000 epoch: 56.89666666666667
[[58.26315789473684, 56.38815789473684, 57.03947368421053], [58.39041666666667, 56.602916666666665, 56.89666666666667]]
train loss 0.7972450319290161, epoch 329, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [330][20/30]	Time  1.618 ( 1.544)	Data  0.039 ( 0.069)	InnerLoop  0.753 ( 0.644)	Loss 4.7552e-01 (4.7617e-01)	Acc@1  83.81 ( 83.50)
The current update step is 9930
GPU_0_using curriculum 20 with window 20
Epoch: [331][20/30]	Time  1.515 ( 1.535)	Data  0.038 ( 0.057)	InnerLoop  0.641 ( 0.647)	Loss 4.7749e-01 (4.6729e-01)	Acc@1  83.52 ( 83.86)
The current update step is 9960
GPU_0_using curriculum 20 with window 20
Epoch: [332][20/30]	Time  1.537 ( 1.544)	Data  0.037 ( 0.044)	InnerLoop  0.642 ( 0.665)	Loss 4.9880e-01 (4.6930e-01)	Acc@1  82.20 ( 83.88)
The current update step is 9990
GPU_0_using curriculum 20 with window 20
Epoch: [333][20/30]	Time  1.509 ( 1.541)	Data  0.040 ( 0.051)	InnerLoop  0.640 ( 0.657)	Loss 4.4838e-01 (5.0298e-01)	Acc@1  83.33 ( 82.42)
The current update step is 10020
GPU_0_using curriculum 20 with window 20
Epoch: [334][20/30]	Time  1.628 ( 1.548)	Data  0.156 ( 0.069)	InnerLoop  0.641 ( 0.646)	Loss 4.5505e-01 (4.9102e-01)	Acc@1  84.38 ( 82.63)
The current update step is 10050
The current seed is 4250020898709210255
The current lr is: 0.0012
Testing Results:
 *   Acc@1 65.289
 *   Acc@1 65.510
 *   Acc@1 66.605
 *   Acc@1 66.817
 *   Acc@1 68.000
 *   Acc@1 67.935
 *   Acc@1 70.250
 *   Acc@1 70.543
 *   Acc@1 72.618
 *   Acc@1 72.783
 *   Acc@1 73.987
 *   Acc@1 74.208
Training for 300 epoch: 67.76973684210526
Training for 600 epoch: 69.61184210526315
Training for 1000 epoch: 70.99342105263159
Training for 300 epoch: 68.02666666666667
Training for 600 epoch: 69.80041666666666
Training for 1000 epoch: 71.07166666666666
[[67.76973684210526, 69.61184210526315, 70.99342105263159], [68.02666666666667, 69.80041666666666, 71.07166666666666]]
train loss 0.6681038252512614, epoch 334, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [335][20/30]	Time  1.507 ( 1.540)	Data  0.039 ( 0.051)	InnerLoop  0.639 ( 0.656)	Loss 4.6302e-01 (4.8799e-01)	Acc@1  84.50 ( 83.07)
The current update step is 10080
GPU_0_using curriculum 20 with window 20
Epoch: [336][20/30]	Time  1.626 ( 1.548)	Data  0.159 ( 0.070)	InnerLoop  0.632 ( 0.644)	Loss 4.5190e-01 (4.6567e-01)	Acc@1  85.72 ( 84.04)
The current update step is 10110
GPU_0_using curriculum 20 with window 20
Epoch: [337][20/30]	Time  1.521 ( 1.562)	Data  0.042 ( 0.059)	InnerLoop  0.645 ( 0.664)	Loss 4.6207e-01 (4.8943e-01)	Acc@1  84.20 ( 83.09)
The current update step is 10140
GPU_0_using curriculum 20 with window 20
Epoch: [338][20/30]	Time  1.535 ( 1.555)	Data  0.041 ( 0.046)	InnerLoop  0.645 ( 0.672)	Loss 4.7012e-01 (4.8014e-01)	Acc@1  83.86 ( 83.33)
The current update step is 10170
GPU_0_using curriculum 20 with window 20
Epoch: [339][20/30]	Time  1.516 ( 1.558)	Data  0.040 ( 0.072)	InnerLoop  0.638 ( 0.648)	Loss 4.5243e-01 (4.8476e-01)	Acc@1  84.33 ( 83.04)
The current update step is 10200
The current seed is 13431242005814774057
The current lr is: 0.0012
Testing Results:
 *   Acc@1 68.842
 *   Acc@1 69.285
 *   Acc@1 64.066
 *   Acc@1 63.629
 *   Acc@1 57.329
 *   Acc@1 57.321
 *   Acc@1 71.566
 *   Acc@1 72.460
 *   Acc@1 73.579
 *   Acc@1 74.175
 *   Acc@1 74.197
 *   Acc@1 74.797
Training for 300 epoch: 70.20394736842104
Training for 600 epoch: 68.82236842105263
Training for 1000 epoch: 65.76315789473685
Training for 300 epoch: 70.8725
Training for 600 epoch: 68.90208333333334
Training for 1000 epoch: 66.05916666666667
[[70.20394736842104, 68.82236842105263, 65.76315789473685], [70.8725, 68.90208333333334, 66.05916666666667]]
train loss 0.5924326608657837, epoch 339, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [340][20/30]	Time  1.650 ( 1.561)	Data  0.039 ( 0.071)	InnerLoop  0.772 ( 0.652)	Loss 5.8946e-01 (4.9861e-01)	Acc@1  79.61 ( 82.36)
The current update step is 10230
GPU_0_using curriculum 20 with window 20
Epoch: [341][20/30]	Time  1.503 ( 1.543)	Data  0.039 ( 0.057)	InnerLoop  0.637 ( 0.653)	Loss 4.4214e-01 (4.8727e-01)	Acc@1  85.18 ( 83.00)
The current update step is 10260
GPU_0_using curriculum 20 with window 20
Epoch: [342][20/30]	Time  1.527 ( 1.545)	Data  0.040 ( 0.046)	InnerLoop  0.633 ( 0.664)	Loss 4.7750e-01 (4.7970e-01)	Acc@1  83.42 ( 83.49)
The current update step is 10290
GPU_0_using curriculum 20 with window 20
Epoch: [343][20/30]	Time  1.504 ( 1.539)	Data  0.040 ( 0.050)	InnerLoop  0.634 ( 0.657)	Loss 4.5410e-01 (4.8602e-01)	Acc@1  83.74 ( 83.08)
The current update step is 10320
GPU_0_using curriculum 20 with window 20
Epoch: [344][20/30]	Time  1.653 ( 1.547)	Data  0.156 ( 0.069)	InnerLoop  0.632 ( 0.644)	Loss 5.0327e-01 (4.9320e-01)	Acc@1  82.08 ( 82.77)
The current update step is 10350
The current seed is 15913845185230718678
The current lr is: 0.0012
Testing Results:
 *   Acc@1 62.895
 *   Acc@1 63.468
 *   Acc@1 72.105
 *   Acc@1 72.430
 *   Acc@1 71.368
 *   Acc@1 71.644
 *   Acc@1 57.395
 *   Acc@1 57.427
 *   Acc@1 60.461
 *   Acc@1 60.848
 *   Acc@1 53.658
 *   Acc@1 52.872
Training for 300 epoch: 60.14473684210526
Training for 600 epoch: 66.28289473684211
Training for 1000 epoch: 62.513157894736835
Training for 300 epoch: 60.447500000000005
Training for 600 epoch: 66.63916666666667
Training for 1000 epoch: 62.25791666666667
[[60.14473684210526, 66.28289473684211, 62.513157894736835], [60.447500000000005, 66.63916666666667, 62.25791666666667]]
train loss 1.7535083662033082, epoch 344, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [345][20/30]	Time  1.513 ( 1.540)	Data  0.043 ( 0.050)	InnerLoop  0.638 ( 0.655)	Loss 5.3995e-01 (4.9606e-01)	Acc@1  81.03 ( 82.71)
The current update step is 10380
GPU_0_using curriculum 20 with window 20
Epoch: [346][20/30]	Time  1.613 ( 1.548)	Data  0.157 ( 0.069)	InnerLoop  0.629 ( 0.647)	Loss 4.5537e-01 (4.9097e-01)	Acc@1  84.11 ( 82.98)
The current update step is 10410
GPU_0_using curriculum 20 with window 20
Epoch: [347][20/30]	Time  1.508 ( 1.541)	Data  0.042 ( 0.057)	InnerLoop  0.634 ( 0.652)	Loss 5.0926e-01 (4.8873e-01)	Acc@1  81.57 ( 82.87)
The current update step is 10440
GPU_0_using curriculum 20 with window 20
Epoch: [348][20/30]	Time  1.506 ( 1.538)	Data  0.038 ( 0.044)	InnerLoop  0.636 ( 0.661)	Loss 4.3643e-01 (4.9379e-01)	Acc@1  85.06 ( 82.77)
The current update step is 10470
GPU_0_using curriculum 20 with window 20
Epoch: [349][20/30]	Time  1.507 ( 1.534)	Data  0.039 ( 0.069)	InnerLoop  0.643 ( 0.637)	Loss 5.1926e-01 (5.0303e-01)	Acc@1  82.01 ( 82.44)
The current update step is 10500
The current seed is 997884125304825406
The current lr is: 0.0012
Testing Results:
 *   Acc@1 72.237
 *   Acc@1 72.078
 *   Acc@1 70.211
 *   Acc@1 70.043
 *   Acc@1 69.645
 *   Acc@1 69.501
 *   Acc@1 64.237
 *   Acc@1 64.088
 *   Acc@1 65.092
 *   Acc@1 65.017
 *   Acc@1 64.658
 *   Acc@1 64.914
Training for 300 epoch: 68.23684210526316
Training for 600 epoch: 67.65131578947368
Training for 1000 epoch: 67.15131578947368
Training for 300 epoch: 68.08291666666668
Training for 600 epoch: 67.53
Training for 1000 epoch: 67.20750000000001
[[68.23684210526316, 67.65131578947368, 67.15131578947368], [68.08291666666668, 67.53, 67.20750000000001]]
train loss 1.3348487751642863, epoch 349, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [350][20/30]	Time  1.641 ( 1.543)	Data  0.041 ( 0.069)	InnerLoop  0.770 ( 0.643)	Loss 5.0715e-01 (4.9892e-01)	Acc@1  80.74 ( 82.55)
The current update step is 10530
GPU_0_using curriculum 20 with window 20
Epoch: [351][20/30]	Time  1.532 ( 1.545)	Data  0.043 ( 0.058)	InnerLoop  0.657 ( 0.653)	Loss 4.3331e-01 (5.0164e-01)	Acc@1  85.13 ( 82.57)
The current update step is 10560
GPU_0_using curriculum 20 with window 20
Epoch: [352][20/30]	Time  1.510 ( 1.545)	Data  0.039 ( 0.045)	InnerLoop  0.638 ( 0.665)	Loss 4.5766e-01 (5.0756e-01)	Acc@1  84.20 ( 82.22)
The current update step is 10590
GPU_0_using curriculum 20 with window 20
Epoch: [353][20/30]	Time  1.538 ( 1.542)	Data  0.041 ( 0.051)	InnerLoop  0.653 ( 0.658)	Loss 5.0108e-01 (4.7128e-01)	Acc@1  82.25 ( 83.48)
The current update step is 10620
GPU_0_using curriculum 20 with window 20
Epoch: [354][20/30]	Time  1.633 ( 1.548)	Data  0.160 ( 0.069)	InnerLoop  0.637 ( 0.645)	Loss 4.7429e-01 (4.8694e-01)	Acc@1  83.08 ( 83.24)
The current update step is 10650
The current seed is 4357012137755262483
The current lr is: 0.0012
Testing Results:
 *   Acc@1 52.250
 *   Acc@1 52.722
 *   Acc@1 51.211
 *   Acc@1 51.758
 *   Acc@1 49.158
 *   Acc@1 48.998
 *   Acc@1 63.895
 *   Acc@1 64.549
 *   Acc@1 76.197
 *   Acc@1 76.722
 *   Acc@1 74.750
 *   Acc@1 75.297
Training for 300 epoch: 58.07236842105263
Training for 600 epoch: 63.703947368421055
Training for 1000 epoch: 61.953947368421055
Training for 300 epoch: 58.635416666666664
Training for 600 epoch: 64.24
Training for 1000 epoch: 62.147083333333335
[[58.07236842105263, 63.703947368421055, 61.953947368421055], [58.635416666666664, 64.24, 62.147083333333335]]
train loss 0.6790795224825541, epoch 354, best loss 0.5412657897313435, best_epoch 299
GPU_0_using curriculum 20 with window 20
Epoch: [355][20/30]	Time  1.504 ( 1.543)	Data  0.041 ( 0.051)	InnerLoop  0.636 ( 0.657)	Loss 5.7582e-01 (4.9961e-01)	Acc@1  79.20 ( 82.52)
The current update step is 10680
GPU_0_using curriculum 20 with window 20
Epoch: [356][20/30]	Time  1.613 ( 1.547)	Data  0.155 ( 0.069)	InnerLoop  0.630 ( 0.645)	Loss 5.4775e-01 (4.9119e-01)	Acc@1  80.88 ( 82.86)
The current update step is 10710
GPU_0_using curriculum 20 with window 20
Epoch: [357][20/30]	Time  1.516 ( 1.536)	Data  0.042 ( 0.057)	InnerLoop  0.637 ( 0.648)	Loss 4.3795e-01 (4.9620e-01)	Acc@1  85.33 ( 82.77)
The current update step is 10740
GPU_0_using curriculum 20 with window 20
Epoch: [358][20/30]	Time  1.509 ( 1.534)	Data  0.039 ( 0.044)	InnerLoop  0.645 ( 0.661)	Loss 4.4495e-01 (4.9288e-01)	Acc@1  84.94 ( 82.72)
The current update step is 10770
GPU_0_using curriculum 20 with window 20
Epoch: [359][20/30]	Time  1.503 ( 1.534)	Data  0.039 ( 0.069)	InnerLoop  0.633 ( 0.635)	Loss 5.4280e-01 (4.7757e-01)	Acc@1  79.93 ( 83.36)
The current update step is 10800
The current seed is 11725490734443880985
The current lr is: 0.0012
Testing Results:
 *   Acc@1 73.105
 *   Acc@1 73.672
 *   Acc@1 73.816
 *   Acc@1 74.483
 *   Acc@1 74.684
 *   Acc@1 74.708
 *   Acc@1 59.961
 *   Acc@1 60.443
 *   Acc@1 60.803
 *   Acc@1 61.839
 *   Acc@1 63.355
 *   Acc@1 63.964
Training for 300 epoch: 66.53289473684211
Training for 600 epoch: 67.30921052631578
Training for 1000 epoch: 69.01973684210526
Training for 300 epoch: 67.0575
Training for 600 epoch: 68.16083333333333
Training for 1000 epoch: 69.33624999999999
[[66.53289473684211, 67.30921052631578, 69.01973684210526], [67.0575, 68.16083333333333, 69.33624999999999]]
train loss 1.0090703271865844, epoch 359, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [360][20/30]	Time  1.674 ( 1.554)	Data  0.040 ( 0.071)	InnerLoop  0.771 ( 0.648)	Loss 4.4287e-01 (4.7787e-01)	Acc@1  83.69 ( 83.48)
The current update step is 10830
GPU_0_using curriculum 20 with window 20
Epoch: [361][20/30]	Time  1.535 ( 1.554)	Data  0.041 ( 0.059)	InnerLoop  0.655 ( 0.658)	Loss 4.4613e-01 (4.9141e-01)	Acc@1  84.38 ( 82.76)
The current update step is 10860
GPU_0_using curriculum 20 with window 20
Epoch: [362][20/30]	Time  1.535 ( 1.559)	Data  0.041 ( 0.046)	InnerLoop  0.653 ( 0.672)	Loss 4.9776e-01 (5.1173e-01)	Acc@1  81.88 ( 82.09)
The current update step is 10890
GPU_0_using curriculum 20 with window 20
Epoch: [363][20/30]	Time  1.517 ( 1.559)	Data  0.039 ( 0.052)	InnerLoop  0.645 ( 0.663)	Loss 4.4846e-01 (4.7407e-01)	Acc@1  84.81 ( 83.71)
The current update step is 10920
GPU_0_using curriculum 20 with window 20
Epoch: [364][20/30]	Time  1.650 ( 1.565)	Data  0.161 ( 0.071)	InnerLoop  0.642 ( 0.654)	Loss 4.8760e-01 (5.0194e-01)	Acc@1  83.03 ( 82.40)
The current update step is 10950
The current seed is 17007465058247376196
The current lr is: 0.0012
Testing Results:
 *   Acc@1 73.000
 *   Acc@1 73.381
 *   Acc@1 73.579
 *   Acc@1 73.867
 *   Acc@1 53.474
 *   Acc@1 52.811
 *   Acc@1 72.974
 *   Acc@1 72.959
 *   Acc@1 75.803
 *   Acc@1 76.147
 *   Acc@1 76.184
 *   Acc@1 76.561
Training for 300 epoch: 72.98684210526315
Training for 600 epoch: 74.69078947368422
Training for 1000 epoch: 64.82894736842105
Training for 300 epoch: 73.16999999999999
Training for 600 epoch: 75.00708333333333
Training for 1000 epoch: 64.68583333333333
[[72.98684210526315, 74.69078947368422, 64.82894736842105], [73.16999999999999, 75.00708333333333, 64.68583333333333]]
train loss 0.7935154822985331, epoch 364, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [365][20/30]	Time  1.518 ( 1.544)	Data  0.036 ( 0.050)	InnerLoop  0.637 ( 0.658)	Loss 5.1329e-01 (4.8769e-01)	Acc@1  81.49 ( 82.95)
The current update step is 10980
GPU_0_using curriculum 20 with window 20
Epoch: [366][20/30]	Time  1.632 ( 1.549)	Data  0.162 ( 0.070)	InnerLoop  0.637 ( 0.645)	Loss 4.8035e-01 (4.8402e-01)	Acc@1  83.08 ( 82.88)
The current update step is 11010
GPU_0_using curriculum 20 with window 20
Epoch: [367][20/30]	Time  1.506 ( 1.541)	Data  0.039 ( 0.058)	InnerLoop  0.637 ( 0.649)	Loss 4.6234e-01 (4.8865e-01)	Acc@1  83.91 ( 83.21)
The current update step is 11040
GPU_0_using curriculum 20 with window 20
Epoch: [368][20/30]	Time  1.506 ( 1.540)	Data  0.040 ( 0.045)	InnerLoop  0.640 ( 0.663)	Loss 4.6360e-01 (4.9342e-01)	Acc@1  84.16 ( 83.02)
The current update step is 11070
GPU_0_using curriculum 20 with window 20
Epoch: [369][20/30]	Time  1.518 ( 1.538)	Data  0.040 ( 0.071)	InnerLoop  0.633 ( 0.640)	Loss 5.8996e-01 (4.9573e-01)	Acc@1  77.73 ( 82.72)
The current update step is 11100
The current seed is 18000764037718767741
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.105
 *   Acc@1 63.882
 *   Acc@1 69.658
 *   Acc@1 69.785
 *   Acc@1 70.105
 *   Acc@1 69.970
 *   Acc@1 56.908
 *   Acc@1 56.209
 *   Acc@1 61.908
 *   Acc@1 61.548
 *   Acc@1 64.908
 *   Acc@1 64.651
Training for 300 epoch: 60.506578947368425
Training for 600 epoch: 65.78289473684211
Training for 1000 epoch: 67.50657894736842
Training for 300 epoch: 60.04541666666667
Training for 600 epoch: 65.66666666666666
Training for 1000 epoch: 67.31041666666667
[[60.506578947368425, 65.78289473684211, 67.50657894736842], [60.04541666666667, 65.66666666666666, 67.31041666666667]]
train loss 1.0365834626515706, epoch 369, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [370][20/30]	Time  1.627 ( 1.550)	Data  0.043 ( 0.070)	InnerLoop  0.753 ( 0.647)	Loss 5.3490e-01 (4.8456e-01)	Acc@1  80.81 ( 82.94)
The current update step is 11130
GPU_0_using curriculum 20 with window 20
Epoch: [371][20/30]	Time  1.500 ( 1.541)	Data  0.042 ( 0.057)	InnerLoop  0.633 ( 0.651)	Loss 5.5619e-01 (5.0343e-01)	Acc@1  81.86 ( 82.71)
The current update step is 11160
GPU_0_using curriculum 20 with window 20
Epoch: [372][20/30]	Time  1.505 ( 1.537)	Data  0.038 ( 0.044)	InnerLoop  0.641 ( 0.662)	Loss 5.3357e-01 (5.0983e-01)	Acc@1  81.49 ( 82.04)
The current update step is 11190
GPU_0_using curriculum 20 with window 20
Epoch: [373][20/30]	Time  1.508 ( 1.539)	Data  0.039 ( 0.051)	InnerLoop  0.636 ( 0.655)	Loss 4.8018e-01 (5.1527e-01)	Acc@1  83.86 ( 82.11)
The current update step is 11220
GPU_0_using curriculum 20 with window 20
Epoch: [374][20/30]	Time  1.657 ( 1.550)	Data  0.157 ( 0.070)	InnerLoop  0.642 ( 0.644)	Loss 5.2898e-01 (5.1979e-01)	Acc@1  82.06 ( 81.74)
The current update step is 11250
The current seed is 10387879761572835785
The current lr is: 0.0012
Testing Results:
 *   Acc@1 52.737
 *   Acc@1 51.608
 *   Acc@1 72.487
 *   Acc@1 72.459
 *   Acc@1 73.224
 *   Acc@1 73.548
 *   Acc@1 73.092
 *   Acc@1 72.846
 *   Acc@1 73.382
 *   Acc@1 73.552
 *   Acc@1 73.500
 *   Acc@1 74.146
Training for 300 epoch: 62.91447368421052
Training for 600 epoch: 72.9342105263158
Training for 1000 epoch: 73.36184210526315
Training for 300 epoch: 62.22708333333333
Training for 600 epoch: 73.00541666666666
Training for 1000 epoch: 73.84708333333333
[[62.91447368421052, 72.9342105263158, 73.36184210526315], [62.22708333333333, 73.00541666666666, 73.84708333333333]]
train loss 0.7144852551142374, epoch 374, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [375][20/30]	Time  1.524 ( 1.543)	Data  0.037 ( 0.051)	InnerLoop  0.640 ( 0.657)	Loss 4.9995e-01 (4.7339e-01)	Acc@1  82.96 ( 83.81)
The current update step is 11280
GPU_0_using curriculum 20 with window 20
Epoch: [376][20/30]	Time  1.628 ( 1.547)	Data  0.163 ( 0.069)	InnerLoop  0.637 ( 0.647)	Loss 5.2086e-01 (5.0510e-01)	Acc@1  81.76 ( 82.50)
The current update step is 11310
GPU_0_using curriculum 20 with window 20
Epoch: [377][20/30]	Time  1.509 ( 1.537)	Data  0.039 ( 0.057)	InnerLoop  0.639 ( 0.650)	Loss 4.4007e-01 (4.9360e-01)	Acc@1  85.62 ( 83.00)
The current update step is 11340
GPU_0_using curriculum 20 with window 20
Epoch: [378][20/30]	Time  1.529 ( 1.539)	Data  0.040 ( 0.045)	InnerLoop  0.657 ( 0.664)	Loss 4.6347e-01 (4.8203e-01)	Acc@1  84.30 ( 83.28)
The current update step is 11370
GPU_0_using curriculum 20 with window 20
Epoch: [379][20/30]	Time  1.512 ( 1.540)	Data  0.041 ( 0.070)	InnerLoop  0.634 ( 0.638)	Loss 4.7937e-01 (4.9436e-01)	Acc@1  82.50 ( 82.65)
The current update step is 11400
The current seed is 2026584163045796541
The current lr is: 0.0012
Testing Results:
 *   Acc@1 61.697
 *   Acc@1 61.760
 *   Acc@1 62.434
 *   Acc@1 62.199
 *   Acc@1 63.158
 *   Acc@1 63.123
 *   Acc@1 64.395
 *   Acc@1 64.118
 *   Acc@1 64.382
 *   Acc@1 64.497
 *   Acc@1 64.118
 *   Acc@1 64.732
Training for 300 epoch: 63.046052631578945
Training for 600 epoch: 63.40789473684211
Training for 1000 epoch: 63.638157894736835
Training for 300 epoch: 62.939166666666665
Training for 600 epoch: 63.34791666666666
Training for 1000 epoch: 63.927083333333336
[[63.046052631578945, 63.40789473684211, 63.638157894736835], [62.939166666666665, 63.34791666666666, 63.927083333333336]]
train loss 0.9722135725021362, epoch 379, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [380][20/30]	Time  1.648 ( 1.543)	Data  0.039 ( 0.069)	InnerLoop  0.770 ( 0.645)	Loss 4.9349e-01 (4.8174e-01)	Acc@1  82.23 ( 83.35)
The current update step is 11430
GPU_0_using curriculum 20 with window 20
Epoch: [381][20/30]	Time  1.525 ( 1.539)	Data  0.040 ( 0.058)	InnerLoop  0.635 ( 0.651)	Loss 5.6507e-01 (4.8573e-01)	Acc@1  81.13 ( 82.91)
The current update step is 11460
GPU_0_using curriculum 20 with window 20
Epoch: [382][20/30]	Time  1.525 ( 1.550)	Data  0.040 ( 0.046)	InnerLoop  0.648 ( 0.669)	Loss 5.4721e-01 (4.7914e-01)	Acc@1  81.32 ( 83.28)
The current update step is 11490
GPU_0_using curriculum 20 with window 20
Epoch: [383][20/30]	Time  1.519 ( 1.561)	Data  0.043 ( 0.054)	InnerLoop  0.642 ( 0.667)	Loss 4.4391e-01 (5.0553e-01)	Acc@1  84.25 ( 82.02)
The current update step is 11520
GPU_0_using curriculum 20 with window 20
Epoch: [384][20/30]	Time  1.654 ( 1.565)	Data  0.168 ( 0.073)	InnerLoop  0.646 ( 0.654)	Loss 4.9635e-01 (4.9646e-01)	Acc@1  82.79 ( 82.54)
The current update step is 11550
The current seed is 2482434435782575744
The current lr is: 0.0012
Testing Results:
 *   Acc@1 62.395
 *   Acc@1 62.483
 *   Acc@1 41.250
 *   Acc@1 41.319
 *   Acc@1 64.553
 *   Acc@1 64.208
 *   Acc@1 67.724
 *   Acc@1 67.368
 *   Acc@1 68.961
 *   Acc@1 68.763
 *   Acc@1 69.553
 *   Acc@1 69.586
Training for 300 epoch: 65.05921052631578
Training for 600 epoch: 55.10526315789474
Training for 1000 epoch: 67.05263157894737
Training for 300 epoch: 64.92583333333334
Training for 600 epoch: 55.041250000000005
Training for 1000 epoch: 66.89708333333333
[[65.05921052631578, 55.10526315789474, 67.05263157894737], [64.92583333333334, 55.041250000000005, 66.89708333333333]]
train loss 1.0358452679951986, epoch 384, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [385][20/30]	Time  1.543 ( 1.558)	Data  0.040 ( 0.053)	InnerLoop  0.656 ( 0.667)	Loss 4.5813e-01 (4.9769e-01)	Acc@1  84.91 ( 82.94)
The current update step is 11580
GPU_0_using curriculum 20 with window 20
Epoch: [386][20/30]	Time  1.641 ( 1.564)	Data  0.159 ( 0.071)	InnerLoop  0.637 ( 0.654)	Loss 5.1858e-01 (4.9655e-01)	Acc@1  82.23 ( 82.35)
The current update step is 11610
GPU_0_using curriculum 20 with window 20
Epoch: [387][20/30]	Time  1.523 ( 1.544)	Data  0.039 ( 0.057)	InnerLoop  0.635 ( 0.655)	Loss 4.7780e-01 (5.2240e-01)	Acc@1  84.40 ( 81.39)
The current update step is 11640
GPU_0_using curriculum 20 with window 20
Epoch: [388][20/30]	Time  1.506 ( 1.551)	Data  0.039 ( 0.046)	InnerLoop  0.637 ( 0.671)	Loss 4.7699e-01 (4.9639e-01)	Acc@1  84.67 ( 82.55)
The current update step is 11670
GPU_0_using curriculum 20 with window 20
Epoch: [389][20/30]	Time  1.548 ( 1.541)	Data  0.039 ( 0.069)	InnerLoop  0.636 ( 0.639)	Loss 5.8415e-01 (4.9163e-01)	Acc@1  78.20 ( 82.82)
The current update step is 11700
The current seed is 8767269236801685280
The current lr is: 0.0012
Testing Results:
 *   Acc@1 75.105
 *   Acc@1 75.214
 *   Acc@1 75.211
 *   Acc@1 75.235
 *   Acc@1 75.303
 *   Acc@1 75.421
 *   Acc@1 65.776
 *   Acc@1 65.669
 *   Acc@1 57.197
 *   Acc@1 56.994
 *   Acc@1 54.263
 *   Acc@1 53.756
Training for 300 epoch: 70.44078947368422
Training for 600 epoch: 66.20394736842105
Training for 1000 epoch: 64.78289473684211
Training for 300 epoch: 70.44166666666666
Training for 600 epoch: 66.11458333333333
Training for 1000 epoch: 64.58833333333334
[[70.44078947368422, 66.20394736842105, 64.78289473684211], [70.44166666666666, 66.11458333333333, 64.58833333333334]]
train loss 1.8349074621836345, epoch 389, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [390][20/30]	Time  1.631 ( 1.548)	Data  0.038 ( 0.069)	InnerLoop  0.755 ( 0.645)	Loss 4.3393e-01 (4.9380e-01)	Acc@1  84.67 ( 82.77)
The current update step is 11730
GPU_0_using curriculum 20 with window 20
Epoch: [391][20/30]	Time  1.511 ( 1.538)	Data  0.038 ( 0.056)	InnerLoop  0.644 ( 0.652)	Loss 4.3590e-01 (4.8669e-01)	Acc@1  84.74 ( 83.13)
The current update step is 11760
GPU_0_using curriculum 20 with window 20
Epoch: [392][20/30]	Time  1.512 ( 1.543)	Data  0.040 ( 0.045)	InnerLoop  0.641 ( 0.666)	Loss 5.7043e-01 (4.9937e-01)	Acc@1  80.52 ( 82.89)
The current update step is 11790
GPU_0_using curriculum 20 with window 20
Epoch: [393][20/30]	Time  1.504 ( 1.545)	Data  0.040 ( 0.050)	InnerLoop  0.637 ( 0.661)	Loss 4.8703e-01 (5.1415e-01)	Acc@1  82.81 ( 82.29)
The current update step is 11820
GPU_0_using curriculum 20 with window 20
Epoch: [394][20/30]	Time  1.620 ( 1.544)	Data  0.160 ( 0.069)	InnerLoop  0.635 ( 0.646)	Loss 4.4822e-01 (5.0976e-01)	Acc@1  84.20 ( 82.27)
The current update step is 11850
The current seed is 6598034453287005520
The current lr is: 0.0012
Testing Results:
 *   Acc@1 76.592
 *   Acc@1 76.649
 *   Acc@1 76.816
 *   Acc@1 76.948
 *   Acc@1 76.921
 *   Acc@1 77.082
 *   Acc@1 71.947
 *   Acc@1 72.170
 *   Acc@1 72.013
 *   Acc@1 72.342
 *   Acc@1 72.237
 *   Acc@1 72.560
Training for 300 epoch: 74.26973684210526
Training for 600 epoch: 74.41447368421052
Training for 1000 epoch: 74.57894736842105
Training for 300 epoch: 74.40958333333333
Training for 600 epoch: 74.64458333333334
Training for 1000 epoch: 74.82124999999999
[[74.26973684210526, 74.41447368421052, 74.57894736842105], [74.40958333333333, 74.64458333333334, 74.82124999999999]]
train loss 0.7615278981208802, epoch 394, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [395][20/30]	Time  1.500 ( 1.538)	Data  0.038 ( 0.051)	InnerLoop  0.633 ( 0.657)	Loss 5.0507e-01 (5.1975e-01)	Acc@1  81.47 ( 81.76)
The current update step is 11880
GPU_0_using curriculum 20 with window 20
Epoch: [396][20/30]	Time  1.629 ( 1.546)	Data  0.159 ( 0.069)	InnerLoop  0.640 ( 0.644)	Loss 4.9598e-01 (4.8643e-01)	Acc@1  82.23 ( 83.12)
The current update step is 11910
GPU_0_using curriculum 20 with window 20
Epoch: [397][20/30]	Time  1.515 ( 1.534)	Data  0.040 ( 0.058)	InnerLoop  0.646 ( 0.650)	Loss 4.3307e-01 (4.9592e-01)	Acc@1  84.62 ( 82.89)
The current update step is 11940
GPU_0_using curriculum 20 with window 20
Epoch: [398][20/30]	Time  1.519 ( 1.542)	Data  0.043 ( 0.046)	InnerLoop  0.639 ( 0.665)	Loss 4.1098e-01 (4.7277e-01)	Acc@1  85.60 ( 83.70)
The current update step is 11970
GPU_0_using curriculum 20 with window 20
Epoch: [399][20/30]	Time  1.503 ( 1.537)	Data  0.038 ( 0.070)	InnerLoop  0.642 ( 0.638)	Loss 4.5776e-01 (4.7816e-01)	Acc@1  84.06 ( 83.49)
The current update step is 12000
The current seed is 17660788896760035224
The current lr is: 0.0012
Testing Results:
 *   Acc@1 62.118
 *   Acc@1 62.545
 *   Acc@1 70.895
 *   Acc@1 71.166
 *   Acc@1 70.724
 *   Acc@1 70.911
 *   Acc@1 69.711
 *   Acc@1 70.118
 *   Acc@1 69.421
 *   Acc@1 69.987
 *   Acc@1 70.368
 *   Acc@1 70.761
Training for 300 epoch: 65.91447368421053
Training for 600 epoch: 70.15789473684211
Training for 1000 epoch: 70.54605263157895
Training for 300 epoch: 66.33125000000001
Training for 600 epoch: 70.57625
Training for 1000 epoch: 70.83583333333334
[[65.91447368421053, 70.15789473684211, 70.54605263157895], [66.33125000000001, 70.57625, 70.83583333333334]]
train loss 0.7612981339772542, epoch 399, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [400][20/30]	Time  1.627 ( 1.545)	Data  0.038 ( 0.070)	InnerLoop  0.764 ( 0.643)	Loss 4.5135e-01 (4.8894e-01)	Acc@1  84.42 ( 83.14)
The current update step is 12030
GPU_0_using curriculum 20 with window 20
Epoch: [401][20/30]	Time  1.513 ( 1.538)	Data  0.038 ( 0.057)	InnerLoop  0.631 ( 0.650)	Loss 4.9678e-01 (4.7101e-01)	Acc@1  80.86 ( 83.48)
The current update step is 12060
GPU_0_using curriculum 20 with window 20
Epoch: [402][20/30]	Time  1.505 ( 1.538)	Data  0.039 ( 0.045)	InnerLoop  0.633 ( 0.663)	Loss 4.8692e-01 (4.7638e-01)	Acc@1  82.67 ( 83.46)
The current update step is 12090
GPU_0_using curriculum 20 with window 20
Epoch: [403][20/30]	Time  1.513 ( 1.539)	Data  0.039 ( 0.051)	InnerLoop  0.647 ( 0.656)	Loss 4.5091e-01 (4.6521e-01)	Acc@1  84.18 ( 84.00)
The current update step is 12120
GPU_0_using curriculum 20 with window 20
Epoch: [404][20/30]	Time  1.640 ( 1.544)	Data  0.154 ( 0.069)	InnerLoop  0.648 ( 0.641)	Loss 4.9049e-01 (4.7144e-01)	Acc@1  82.93 ( 83.57)
The current update step is 12150
The current seed is 5696462049640658258
The current lr is: 0.0012
Testing Results:
 *   Acc@1 78.079
 *   Acc@1 78.300
 *   Acc@1 77.882
 *   Acc@1 77.956
 *   Acc@1 77.789
 *   Acc@1 77.803
 *   Acc@1 77.526
 *   Acc@1 77.519
 *   Acc@1 77.566
 *   Acc@1 77.757
 *   Acc@1 77.289
 *   Acc@1 77.644
Training for 300 epoch: 77.80263157894737
Training for 600 epoch: 77.72368421052632
Training for 1000 epoch: 77.53947368421052
Training for 300 epoch: 77.90958333333333
Training for 600 epoch: 77.85666666666665
Training for 1000 epoch: 77.72375
[[77.80263157894737, 77.72368421052632, 77.53947368421052], [77.90958333333333, 77.85666666666665, 77.72375]]
train loss 0.7053262560844421, epoch 404, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [405][20/30]	Time  1.498 ( 1.532)	Data  0.040 ( 0.051)	InnerLoop  0.633 ( 0.653)	Loss 4.5105e-01 (4.7660e-01)	Acc@1  84.25 ( 83.61)
The current update step is 12180
GPU_0_using curriculum 20 with window 20
Epoch: [406][20/30]	Time  1.632 ( 1.555)	Data  0.160 ( 0.069)	InnerLoop  0.640 ( 0.649)	Loss 5.0424e-01 (4.7170e-01)	Acc@1  82.76 ( 83.58)
The current update step is 12210
GPU_0_using curriculum 20 with window 20
Epoch: [407][20/30]	Time  1.525 ( 1.545)	Data  0.044 ( 0.058)	InnerLoop  0.635 ( 0.653)	Loss 4.3087e-01 (4.9213e-01)	Acc@1  85.23 ( 82.65)
The current update step is 12240
GPU_0_using curriculum 20 with window 20
Epoch: [408][20/30]	Time  1.507 ( 1.540)	Data  0.043 ( 0.046)	InnerLoop  0.639 ( 0.665)	Loss 4.3876e-01 (4.8152e-01)	Acc@1  84.52 ( 83.14)
The current update step is 12270
GPU_0_using curriculum 20 with window 20
Epoch: [409][20/30]	Time  1.515 ( 1.535)	Data  0.039 ( 0.069)	InnerLoop  0.645 ( 0.638)	Loss 4.7217e-01 (5.2324e-01)	Acc@1  84.23 ( 81.93)
The current update step is 12300
The current seed is 3170316196998107421
The current lr is: 0.0012
Testing Results:
 *   Acc@1 67.184
 *   Acc@1 67.779
 *   Acc@1 68.947
 *   Acc@1 69.118
 *   Acc@1 69.092
 *   Acc@1 69.552
 *   Acc@1 59.658
 *   Acc@1 58.996
 *   Acc@1 57.553
 *   Acc@1 57.212
 *   Acc@1 56.408
 *   Acc@1 56.691
Training for 300 epoch: 63.421052631578945
Training for 600 epoch: 63.25
Training for 1000 epoch: 62.75
Training for 300 epoch: 63.3875
Training for 600 epoch: 63.165000000000006
Training for 1000 epoch: 63.121249999999996
[[63.421052631578945, 63.25, 62.75], [63.3875, 63.165000000000006, 63.121249999999996]]
train loss 1.2416497379938762, epoch 409, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [410][20/30]	Time  1.622 ( 1.543)	Data  0.038 ( 0.069)	InnerLoop  0.751 ( 0.643)	Loss 4.7244e-01 (4.8278e-01)	Acc@1  83.74 ( 83.55)
The current update step is 12330
GPU_0_using curriculum 20 with window 20
Epoch: [411][20/30]	Time  1.500 ( 1.540)	Data  0.040 ( 0.057)	InnerLoop  0.632 ( 0.651)	Loss 4.5860e-01 (5.0117e-01)	Acc@1  84.40 ( 82.41)
The current update step is 12360
GPU_0_using curriculum 20 with window 20
Epoch: [412][20/30]	Time  1.525 ( 1.536)	Data  0.038 ( 0.045)	InnerLoop  0.639 ( 0.661)	Loss 4.7183e-01 (4.8215e-01)	Acc@1  83.86 ( 83.22)
The current update step is 12390
GPU_0_using curriculum 20 with window 20
Epoch: [413][20/30]	Time  1.506 ( 1.535)	Data  0.038 ( 0.051)	InnerLoop  0.634 ( 0.655)	Loss 4.6675e-01 (4.8066e-01)	Acc@1  83.28 ( 83.23)
The current update step is 12420
GPU_0_using curriculum 20 with window 20
Epoch: [414][20/30]	Time  1.619 ( 1.545)	Data  0.157 ( 0.068)	InnerLoop  0.634 ( 0.647)	Loss 4.6045e-01 (4.9564e-01)	Acc@1  83.79 ( 82.69)
The current update step is 12450
The current seed is 6277166413455323496
The current lr is: 0.0012
Testing Results:
 *   Acc@1 63.355
 *   Acc@1 63.174
 *   Acc@1 64.553
 *   Acc@1 64.025
 *   Acc@1 64.500
 *   Acc@1 64.170
 *   Acc@1 57.342
 *   Acc@1 57.363
 *   Acc@1 55.158
 *   Acc@1 54.819
 *   Acc@1 60.513
 *   Acc@1 60.725
Training for 300 epoch: 60.348684210526315
Training for 600 epoch: 59.85526315789474
Training for 1000 epoch: 62.506578947368425
Training for 300 epoch: 60.26875
Training for 600 epoch: 59.42208333333333
Training for 1000 epoch: 62.447500000000005
[[60.348684210526315, 59.85526315789474, 62.506578947368425], [60.26875, 59.42208333333333, 62.447500000000005]]
train loss 1.5473323952992757, epoch 414, best loss 0.5412657897313435, best_epoch 359
GPU_0_using curriculum 20 with window 20
Epoch: [415][20/30]	Time  1.512 ( 1.544)	Data  0.039 ( 0.051)	InnerLoop  0.642 ( 0.660)	Loss 4.5859e-01 (4.7388e-01)	Acc@1  84.86 ( 83.84)
The current update step is 12480
GPU_0_using curriculum 20 with window 20
Epoch: [416][20/30]	Time  1.647 ( 1.545)	Data  0.161 ( 0.069)	InnerLoop  0.638 ( 0.646)	Loss 4.6370e-01 (4.8130e-01)	Acc@1  83.33 ( 83.31)
The current update step is 12510
GPU_0_using curriculum 20 with window 20
Epoch: [417][20/30]	Time  1.507 ( 1.540)	Data  0.039 ( 0.057)	InnerLoop  0.635 ( 0.652)	Loss 5.6392e-01 (4.8636e-01)	Acc@1  78.32 ( 82.86)
The current update step is 12540
GPU_0_using curriculum 20 with window 20
Epoch: [418][20/30]	Time  1.500 ( 1.535)	Data  0.039 ( 0.044)	InnerLoop  0.635 ( 0.659)	Loss 4.8848e-01 (4.7271e-01)	Acc@1  83.03 ( 83.69)
The current update step is 12570
GPU_0_using curriculum 20 with window 20
Epoch: [419][20/30]	Time  1.516 ( 1.533)	Data  0.040 ( 0.068)	InnerLoop  0.636 ( 0.632)	Loss 4.7628e-01 (4.8704e-01)	Acc@1  82.93 ( 82.71)
The current update step is 12600
The current seed is 8419643153950770257
The current lr is: 0.0012
Testing Results:
 *   Acc@1 71.211
 *   Acc@1 70.808
 *   Acc@1 54.987
 *   Acc@1 54.758
 *   Acc@1 62.829
 *   Acc@1 62.635
 *   Acc@1 67.355
 *   Acc@1 67.953
 *   Acc@1 68.316
 *   Acc@1 68.505
 *   Acc@1 68.000
 *   Acc@1 68.401
Training for 300 epoch: 69.28289473684211
Training for 600 epoch: 61.651315789473685
Training for 1000 epoch: 65.41447368421052
Training for 300 epoch: 69.38083333333333
Training for 600 epoch: 61.63166666666666
Training for 1000 epoch: 65.51791666666666
[[69.28289473684211, 61.651315789473685, 65.41447368421052], [69.38083333333333, 61.63166666666666, 65.51791666666666]]
train loss 0.9775944102287293, epoch 419, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [420][20/30]	Time  1.604 ( 1.537)	Data  0.035 ( 0.068)	InnerLoop  0.746 ( 0.638)	Loss 4.4322e-01 (4.8808e-01)	Acc@1  83.25 ( 83.02)
The current update step is 12630
GPU_0_using curriculum 20 with window 20
Epoch: [421][20/30]	Time  1.526 ( 1.539)	Data  0.040 ( 0.057)	InnerLoop  0.645 ( 0.647)	Loss 5.4682e-01 (4.7560e-01)	Acc@1  81.32 ( 83.46)
The current update step is 12660
GPU_0_using curriculum 20 with window 20
Epoch: [422][20/30]	Time  1.544 ( 1.535)	Data  0.053 ( 0.045)	InnerLoop  0.642 ( 0.658)	Loss 4.3932e-01 (4.6660e-01)	Acc@1  85.38 ( 83.85)
The current update step is 12690
GPU_0_using curriculum 20 with window 20
Epoch: [423][20/30]	Time  1.518 ( 1.535)	Data  0.044 ( 0.051)	InnerLoop  0.632 ( 0.654)	Loss 4.7845e-01 (4.7942e-01)	Acc@1  83.28 ( 83.25)
The current update step is 12720
GPU_0_using curriculum 20 with window 20
Epoch: [424][20/30]	Time  1.638 ( 1.544)	Data  0.159 ( 0.069)	InnerLoop  0.638 ( 0.644)	Loss 5.1329e-01 (4.7665e-01)	Acc@1  82.03 ( 83.29)
The current update step is 12750
The current seed is 3862990633832602497
The current lr is: 0.0012
Testing Results:
 *   Acc@1 72.961
 *   Acc@1 72.694
 *   Acc@1 72.684
 *   Acc@1 72.703
 *   Acc@1 72.961
 *   Acc@1 72.903
 *   Acc@1 71.618
 *   Acc@1 72.426
 *   Acc@1 71.487
 *   Acc@1 72.020
 *   Acc@1 71.237
 *   Acc@1 71.899
Training for 300 epoch: 72.28947368421052
Training for 600 epoch: 72.08552631578948
Training for 1000 epoch: 72.09868421052633
Training for 300 epoch: 72.56
Training for 600 epoch: 72.36166666666666
Training for 1000 epoch: 72.40083333333334
[[72.28947368421052, 72.08552631578948, 72.09868421052633], [72.56, 72.36166666666666, 72.40083333333334]]
train loss 0.8692138592720031, epoch 424, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [425][20/30]	Time  1.504 ( 1.533)	Data  0.037 ( 0.050)	InnerLoop  0.630 ( 0.652)	Loss 5.3401e-01 (4.8001e-01)	Acc@1  81.20 ( 83.46)
The current update step is 12780
GPU_0_using curriculum 20 with window 20
Epoch: [426][20/30]	Time  1.631 ( 1.540)	Data  0.162 ( 0.069)	InnerLoop  0.634 ( 0.640)	Loss 5.2893e-01 (4.8219e-01)	Acc@1  81.52 ( 82.99)
The current update step is 12810
GPU_0_using curriculum 20 with window 20
Epoch: [427][20/30]	Time  1.494 ( 1.533)	Data  0.039 ( 0.056)	InnerLoop  0.628 ( 0.645)	Loss 4.5393e-01 (4.7410e-01)	Acc@1  84.03 ( 83.49)
The current update step is 12840
GPU_0_using curriculum 20 with window 20
Epoch: [428][20/30]	Time  1.519 ( 1.531)	Data  0.043 ( 0.045)	InnerLoop  0.633 ( 0.656)	Loss 6.4076e-01 (4.7472e-01)	Acc@1  77.59 ( 83.63)
The current update step is 12870
GPU_0_using curriculum 20 with window 20
Epoch: [429][20/30]	Time  1.547 ( 1.547)	Data  0.040 ( 0.070)	InnerLoop  0.645 ( 0.639)	Loss 4.8486e-01 (4.6843e-01)	Acc@1  82.35 ( 83.66)
The current update step is 12900
The current seed is 1003279985521041160
The current lr is: 0.0012
Testing Results:
 *   Acc@1 66.421
 *   Acc@1 66.620
 *   Acc@1 63.526
 *   Acc@1 63.577
 *   Acc@1 64.487
 *   Acc@1 64.478
 *   Acc@1 59.158
 *   Acc@1 58.976
 *   Acc@1 58.079
 *   Acc@1 58.383
 *   Acc@1 57.382
 *   Acc@1 57.954
Training for 300 epoch: 62.78947368421052
Training for 600 epoch: 60.80263157894737
Training for 1000 epoch: 60.934210526315795
Training for 300 epoch: 62.797916666666666
Training for 600 epoch: 60.97958333333334
Training for 1000 epoch: 61.21625
[[62.78947368421052, 60.80263157894737, 60.934210526315795], [62.797916666666666, 60.97958333333334, 61.21625]]
train loss 1.3267635494232177, epoch 429, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [430][20/30]	Time  1.671 ( 1.570)	Data  0.041 ( 0.071)	InnerLoop  0.769 ( 0.654)	Loss 5.9310e-01 (4.8471e-01)	Acc@1  78.96 ( 83.11)
The current update step is 12930
GPU_0_using curriculum 20 with window 20
Epoch: [431][20/30]	Time  1.527 ( 1.562)	Data  0.041 ( 0.058)	InnerLoop  0.646 ( 0.656)	Loss 5.6100e-01 (4.7496e-01)	Acc@1  78.03 ( 83.47)
The current update step is 12960
GPU_0_using curriculum 20 with window 20
Epoch: [432][20/30]	Time  1.514 ( 1.554)	Data  0.039 ( 0.046)	InnerLoop  0.643 ( 0.667)	Loss 5.4606e-01 (4.8156e-01)	Acc@1  81.96 ( 83.16)
The current update step is 12990
GPU_0_using curriculum 20 with window 20
Epoch: [433][20/30]	Time  1.522 ( 1.553)	Data  0.040 ( 0.051)	InnerLoop  0.644 ( 0.661)	Loss 4.3807e-01 (4.6733e-01)	Acc@1  83.86 ( 83.56)
The current update step is 13020
GPU_0_using curriculum 20 with window 20
Epoch: [434][20/30]	Time  1.644 ( 1.548)	Data  0.165 ( 0.070)	InnerLoop  0.646 ( 0.644)	Loss 6.0112e-01 (4.9965e-01)	Acc@1  79.93 ( 82.41)
The current update step is 13050
The current seed is 17622171195131179644
The current lr is: 0.0012
Testing Results:
 *   Acc@1 43.408
 *   Acc@1 43.029
 *   Acc@1 56.750
 *   Acc@1 56.191
 *   Acc@1 47.237
 *   Acc@1 48.034
 *   Acc@1 62.684
 *   Acc@1 63.104
 *   Acc@1 64.632
 *   Acc@1 65.108
 *   Acc@1 63.263
 *   Acc@1 63.097
Training for 300 epoch: 53.046052631578945
Training for 600 epoch: 60.69078947368421
Training for 1000 epoch: 55.25
Training for 300 epoch: 53.06666666666666
Training for 600 epoch: 60.64958333333333
Training for 1000 epoch: 55.56583333333333
[[53.046052631578945, 60.69078947368421, 55.25], [53.06666666666666, 60.64958333333333, 55.56583333333333]]
train loss 1.4843061774571737, epoch 434, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [435][20/30]	Time  1.509 ( 1.545)	Data  0.040 ( 0.051)	InnerLoop  0.639 ( 0.659)	Loss 5.0161e-01 (5.0551e-01)	Acc@1  82.35 ( 82.28)
The current update step is 13080
GPU_0_using curriculum 20 with window 20
Epoch: [436][20/30]	Time  1.629 ( 1.548)	Data  0.166 ( 0.070)	InnerLoop  0.634 ( 0.645)	Loss 4.4713e-01 (4.7294e-01)	Acc@1  84.69 ( 83.49)
The current update step is 13110
GPU_0_using curriculum 20 with window 20
Epoch: [437][20/30]	Time  1.508 ( 1.545)	Data  0.039 ( 0.057)	InnerLoop  0.639 ( 0.654)	Loss 4.7387e-01 (4.7628e-01)	Acc@1  82.86 ( 83.24)
The current update step is 13140
GPU_0_using curriculum 20 with window 20
Epoch: [438][20/30]	Time  1.519 ( 1.545)	Data  0.039 ( 0.044)	InnerLoop  0.643 ( 0.668)	Loss 5.5873e-01 (4.7375e-01)	Acc@1  79.61 ( 83.63)
The current update step is 13170
GPU_0_using curriculum 20 with window 20
Epoch: [439][20/30]	Time  1.536 ( 1.548)	Data  0.042 ( 0.071)	InnerLoop  0.638 ( 0.641)	Loss 4.8386e-01 (4.8728e-01)	Acc@1  83.35 ( 82.85)
The current update step is 13200
The current seed is 163253816589829733
The current lr is: 0.0012
Testing Results:
 *   Acc@1 71.303
 *   Acc@1 71.431
 *   Acc@1 70.329
 *   Acc@1 70.476
 *   Acc@1 70.487
 *   Acc@1 70.403
 *   Acc@1 66.237
 *   Acc@1 66.677
 *   Acc@1 63.829
 *   Acc@1 63.392
 *   Acc@1 63.487
 *   Acc@1 63.176
Training for 300 epoch: 68.76973684210526
Training for 600 epoch: 67.07894736842105
Training for 1000 epoch: 66.98684210526316
Training for 300 epoch: 69.05375000000001
Training for 600 epoch: 66.93416666666667
Training for 1000 epoch: 66.78916666666667
[[68.76973684210526, 67.07894736842105, 66.98684210526316], [69.05375000000001, 66.93416666666667, 66.78916666666667]]
train loss 1.4301852845509846, epoch 439, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [440][20/30]	Time  1.644 ( 1.559)	Data  0.037 ( 0.070)	InnerLoop  0.761 ( 0.649)	Loss 4.4935e-01 (4.6878e-01)	Acc@1  84.55 ( 83.64)
The current update step is 13230
GPU_0_using curriculum 20 with window 20
Epoch: [441][20/30]	Time  1.516 ( 1.552)	Data  0.038 ( 0.056)	InnerLoop  0.639 ( 0.656)	Loss 4.3588e-01 (4.7781e-01)	Acc@1  85.18 ( 83.32)
The current update step is 13260
GPU_0_using curriculum 20 with window 20
Epoch: [442][20/30]	Time  1.519 ( 1.548)	Data  0.036 ( 0.046)	InnerLoop  0.645 ( 0.668)	Loss 4.4989e-01 (4.7771e-01)	Acc@1  84.20 ( 83.40)
The current update step is 13290
GPU_0_using curriculum 20 with window 20
Epoch: [443][20/30]	Time  1.503 ( 1.549)	Data  0.037 ( 0.051)	InnerLoop  0.634 ( 0.661)	Loss 5.2667e-01 (4.9197e-01)	Acc@1  80.88 ( 82.39)
The current update step is 13320
GPU_0_using curriculum 20 with window 20
Epoch: [444][20/30]	Time  1.647 ( 1.569)	Data  0.161 ( 0.072)	InnerLoop  0.648 ( 0.655)	Loss 4.6446e-01 (4.7867e-01)	Acc@1  83.54 ( 83.11)
The current update step is 13350
The current seed is 9920495480404489151
The current lr is: 0.0012
Testing Results:
 *   Acc@1 53.671
 *   Acc@1 53.768
 *   Acc@1 54.342
 *   Acc@1 53.998
 *   Acc@1 54.855
 *   Acc@1 54.449
 *   Acc@1 75.289
 *   Acc@1 75.839
 *   Acc@1 75.395
 *   Acc@1 75.416
 *   Acc@1 75.250
 *   Acc@1 75.349
Training for 300 epoch: 64.48026315789474
Training for 600 epoch: 64.86842105263158
Training for 1000 epoch: 65.05263157894737
Training for 300 epoch: 64.80375000000001
Training for 600 epoch: 64.70666666666668
Training for 1000 epoch: 64.89916666666666
[[64.48026315789474, 64.86842105263158, 65.05263157894737], [64.80375000000001, 64.70666666666668, 64.89916666666666]]
train loss 0.8388248110453288, epoch 444, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [445][20/30]	Time  1.538 ( 1.572)	Data  0.037 ( 0.053)	InnerLoop  0.661 ( 0.673)	Loss 5.9954e-01 (4.9066e-01)	Acc@1  79.76 ( 83.06)
The current update step is 13380
GPU_0_using curriculum 20 with window 20
Epoch: [446][20/30]	Time  1.664 ( 1.575)	Data  0.166 ( 0.072)	InnerLoop  0.643 ( 0.658)	Loss 4.5940e-01 (5.0009e-01)	Acc@1  83.76 ( 82.60)
The current update step is 13410
GPU_0_using curriculum 20 with window 20
Epoch: [447][20/30]	Time  1.522 ( 1.564)	Data  0.039 ( 0.059)	InnerLoop  0.643 ( 0.666)	Loss 4.5347e-01 (4.8519e-01)	Acc@1  84.40 ( 82.98)
The current update step is 13440
GPU_0_using curriculum 20 with window 20
Epoch: [448][20/30]	Time  1.525 ( 1.552)	Data  0.042 ( 0.046)	InnerLoop  0.643 ( 0.670)	Loss 5.8718e-01 (4.9810e-01)	Acc@1  79.91 ( 82.62)
The current update step is 13470
GPU_0_using curriculum 20 with window 20
Epoch: [449][20/30]	Time  1.518 ( 1.550)	Data  0.040 ( 0.071)	InnerLoop  0.645 ( 0.639)	Loss 4.3841e-01 (4.7586e-01)	Acc@1  84.52 ( 83.37)
The current update step is 13500
The current seed is 5864543824318133048
The current lr is: 0.0012
Testing Results:
 *   Acc@1 75.553
 *   Acc@1 74.868
 *   Acc@1 75.434
 *   Acc@1 75.183
 *   Acc@1 74.645
 *   Acc@1 74.460
 *   Acc@1 71.447
 *   Acc@1 70.988
 *   Acc@1 72.553
 *   Acc@1 72.076
 *   Acc@1 72.237
 *   Acc@1 72.319
Training for 300 epoch: 73.5
Training for 600 epoch: 73.99342105263159
Training for 1000 epoch: 73.44078947368422
Training for 300 epoch: 72.92791666666668
Training for 600 epoch: 73.62958333333333
Training for 1000 epoch: 73.38958333333332
[[73.5, 73.99342105263159, 73.44078947368422], [72.92791666666668, 73.62958333333333, 73.38958333333332]]
train loss 0.8041432099342346, epoch 449, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [450][20/30]	Time  1.664 ( 1.552)	Data  0.040 ( 0.069)	InnerLoop  0.767 ( 0.649)	Loss 4.7181e-01 (4.7896e-01)	Acc@1  83.25 ( 83.41)
The current update step is 13530
GPU_0_using curriculum 20 with window 20
Epoch: [451][20/30]	Time  1.532 ( 1.547)	Data  0.039 ( 0.056)	InnerLoop  0.641 ( 0.655)	Loss 4.5602e-01 (4.6840e-01)	Acc@1  84.62 ( 84.03)
The current update step is 13560
GPU_0_using curriculum 20 with window 20
Epoch: [452][20/30]	Time  1.555 ( 1.545)	Data  0.040 ( 0.045)	InnerLoop  0.661 ( 0.668)	Loss 5.4129e-01 (4.6659e-01)	Acc@1  80.88 ( 83.93)
The current update step is 13590
GPU_0_using curriculum 20 with window 20
Epoch: [453][20/30]	Time  1.532 ( 1.546)	Data  0.046 ( 0.051)	InnerLoop  0.653 ( 0.661)	Loss 4.5390e-01 (4.7067e-01)	Acc@1  84.13 ( 83.38)
The current update step is 13620
GPU_0_using curriculum 20 with window 20
Epoch: [454][20/30]	Time  1.660 ( 1.556)	Data  0.162 ( 0.070)	InnerLoop  0.650 ( 0.649)	Loss 4.2640e-01 (4.6917e-01)	Acc@1  84.74 ( 83.51)
The current update step is 13650
The current seed is 14026202130470493945
The current lr is: 0.0012
Testing Results:
 *   Acc@1 61.882
 *   Acc@1 62.585
 *   Acc@1 66.145
 *   Acc@1 66.483
 *   Acc@1 57.684
 *   Acc@1 58.044
 *   Acc@1 62.158
 *   Acc@1 62.599
 *   Acc@1 62.171
 *   Acc@1 61.903
 *   Acc@1 61.447
 *   Acc@1 61.724
Training for 300 epoch: 62.01973684210526
Training for 600 epoch: 64.15789473684211
Training for 1000 epoch: 59.565789473684205
Training for 300 epoch: 62.592083333333335
Training for 600 epoch: 64.1925
Training for 1000 epoch: 59.88416666666667
[[62.01973684210526, 64.15789473684211, 59.565789473684205], [62.592083333333335, 64.1925, 59.88416666666667]]
train loss 1.41962970161438, epoch 454, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [455][20/30]	Time  1.525 ( 1.543)	Data  0.041 ( 0.051)	InnerLoop  0.653 ( 0.659)	Loss 5.8006e-01 (4.9150e-01)	Acc@1  81.01 ( 82.97)
The current update step is 13680
GPU_0_using curriculum 20 with window 20
Epoch: [456][20/30]	Time  1.648 ( 1.551)	Data  0.157 ( 0.069)	InnerLoop  0.643 ( 0.647)	Loss 5.0459e-01 (4.9000e-01)	Acc@1  82.30 ( 83.06)
The current update step is 13710
GPU_0_using curriculum 20 with window 20
Epoch: [457][20/30]	Time  1.533 ( 1.551)	Data  0.038 ( 0.057)	InnerLoop  0.647 ( 0.658)	Loss 4.5951e-01 (4.6436e-01)	Acc@1  84.40 ( 83.97)
The current update step is 13740
GPU_0_using curriculum 20 with window 20
Epoch: [458][20/30]	Time  1.520 ( 1.551)	Data  0.039 ( 0.045)	InnerLoop  0.646 ( 0.670)	Loss 4.4744e-01 (4.7775e-01)	Acc@1  84.40 ( 83.37)
The current update step is 13770
GPU_0_using curriculum 20 with window 20
Epoch: [459][20/30]	Time  1.495 ( 1.549)	Data  0.038 ( 0.069)	InnerLoop  0.631 ( 0.642)	Loss 4.6487e-01 (4.7225e-01)	Acc@1  83.47 ( 83.56)
The current update step is 13800
The current seed is 15711106584243886861
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.789
 *   Acc@1 65.575
 *   Acc@1 63.329
 *   Acc@1 63.333
 *   Acc@1 64.013
 *   Acc@1 63.994
 *   Acc@1 68.789
 *   Acc@1 69.408
 *   Acc@1 66.289
 *   Acc@1 67.228
 *   Acc@1 51.000
 *   Acc@1 50.952
Training for 300 epoch: 66.78947368421052
Training for 600 epoch: 64.80921052631578
Training for 1000 epoch: 57.50657894736842
Training for 300 epoch: 67.49166666666667
Training for 600 epoch: 65.28041666666667
Training for 1000 epoch: 57.47291666666666
[[66.78947368421052, 64.80921052631578, 57.50657894736842], [67.49166666666667, 65.28041666666667, 57.47291666666666]]
train loss 2.2540828156789146, epoch 459, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [460][20/30]	Time  1.661 ( 1.555)	Data  0.039 ( 0.070)	InnerLoop  0.760 ( 0.650)	Loss 5.4807e-01 (4.8265e-01)	Acc@1  79.74 ( 83.22)
The current update step is 13830
GPU_0_using curriculum 20 with window 20
Epoch: [461][20/30]	Time  1.521 ( 1.549)	Data  0.041 ( 0.057)	InnerLoop  0.649 ( 0.657)	Loss 4.5332e-01 (4.7341e-01)	Acc@1  84.74 ( 83.43)
The current update step is 13860
GPU_0_using curriculum 20 with window 20
Epoch: [462][20/30]	Time  1.512 ( 1.541)	Data  0.039 ( 0.045)	InnerLoop  0.640 ( 0.664)	Loss 4.8751e-01 (4.7937e-01)	Acc@1  82.96 ( 83.32)
The current update step is 13890
GPU_0_using curriculum 20 with window 20
Epoch: [463][20/30]	Time  1.508 ( 1.548)	Data  0.041 ( 0.051)	InnerLoop  0.639 ( 0.661)	Loss 5.7257e-01 (4.7014e-01)	Acc@1  79.35 ( 83.92)
The current update step is 13920
GPU_0_using curriculum 20 with window 20
Epoch: [464][20/30]	Time  1.618 ( 1.548)	Data  0.158 ( 0.068)	InnerLoop  0.635 ( 0.646)	Loss 4.8197e-01 (4.8768e-01)	Acc@1  82.84 ( 83.09)
The current update step is 13950
The current seed is 3073058345259848664
The current lr is: 0.0012
Testing Results:
 *   Acc@1 63.566
 *   Acc@1 63.453
 *   Acc@1 60.618
 *   Acc@1 60.864
 *   Acc@1 57.882
 *   Acc@1 58.209
 *   Acc@1 67.447
 *   Acc@1 67.346
 *   Acc@1 67.947
 *   Acc@1 68.155
 *   Acc@1 68.789
 *   Acc@1 68.713
Training for 300 epoch: 65.50657894736842
Training for 600 epoch: 64.28289473684211
Training for 1000 epoch: 63.335526315789465
Training for 300 epoch: 65.39916666666667
Training for 600 epoch: 64.50958333333334
Training for 1000 epoch: 63.46125000000001
[[65.50657894736842, 64.28289473684211, 63.335526315789465], [65.39916666666667, 64.50958333333334, 63.46125000000001]]
train loss 0.9666850415547689, epoch 464, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [465][20/30]	Time  1.524 ( 1.536)	Data  0.037 ( 0.050)	InnerLoop  0.643 ( 0.656)	Loss 4.9653e-01 (4.7690e-01)	Acc@1  82.54 ( 83.38)
The current update step is 13980
GPU_0_using curriculum 20 with window 20
Epoch: [466][20/30]	Time  1.622 ( 1.542)	Data  0.158 ( 0.069)	InnerLoop  0.638 ( 0.644)	Loss 4.6120e-01 (4.6928e-01)	Acc@1  83.84 ( 83.75)
The current update step is 14010
GPU_0_using curriculum 20 with window 20
Epoch: [467][20/30]	Time  1.501 ( 1.536)	Data  0.040 ( 0.056)	InnerLoop  0.636 ( 0.650)	Loss 5.6489e-01 (4.7759e-01)	Acc@1  78.30 ( 83.42)
The current update step is 14040
GPU_0_using curriculum 20 with window 20
Epoch: [468][20/30]	Time  1.516 ( 1.540)	Data  0.038 ( 0.046)	InnerLoop  0.633 ( 0.663)	Loss 4.2839e-01 (4.7212e-01)	Acc@1  85.38 ( 83.44)
The current update step is 14070
GPU_0_using curriculum 20 with window 20
Epoch: [469][20/30]	Time  1.504 ( 1.537)	Data  0.042 ( 0.069)	InnerLoop  0.637 ( 0.639)	Loss 4.2248e-01 (4.7272e-01)	Acc@1  85.21 ( 83.68)
The current update step is 14100
The current seed is 14074441076877384024
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.961
 *   Acc@1 64.752
 *   Acc@1 64.329
 *   Acc@1 64.203
 *   Acc@1 63.776
 *   Acc@1 63.632
 *   Acc@1 57.974
 *   Acc@1 58.032
 *   Acc@1 59.737
 *   Acc@1 59.914
 *   Acc@1 48.855
 *   Acc@1 48.219
Training for 300 epoch: 61.4671052631579
Training for 600 epoch: 62.03289473684211
Training for 1000 epoch: 56.31578947368421
Training for 300 epoch: 61.39208333333333
Training for 600 epoch: 62.05875
Training for 1000 epoch: 55.92541666666666
[[61.4671052631579, 62.03289473684211, 56.31578947368421], [61.39208333333333, 62.05875, 55.92541666666666]]
train loss 1.3310961954752605, epoch 469, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [470][20/30]	Time  1.625 ( 1.540)	Data  0.044 ( 0.070)	InnerLoop  0.752 ( 0.642)	Loss 4.2759e-01 (4.7392e-01)	Acc@1  84.72 ( 83.43)
The current update step is 14130
GPU_0_using curriculum 20 with window 20
Epoch: [471][20/30]	Time  1.510 ( 1.536)	Data  0.039 ( 0.058)	InnerLoop  0.642 ( 0.647)	Loss 4.5586e-01 (4.7775e-01)	Acc@1  83.86 ( 83.17)
The current update step is 14160
GPU_0_using curriculum 20 with window 20
Epoch: [472][20/30]	Time  1.503 ( 1.538)	Data  0.037 ( 0.043)	InnerLoop  0.643 ( 0.664)	Loss 4.4315e-01 (4.7271e-01)	Acc@1  84.72 ( 83.51)
The current update step is 14190
GPU_0_using curriculum 20 with window 20
Epoch: [473][20/30]	Time  1.514 ( 1.541)	Data  0.037 ( 0.052)	InnerLoop  0.646 ( 0.659)	Loss 4.7230e-01 (4.8087e-01)	Acc@1  83.86 ( 83.29)
The current update step is 14220
GPU_0_using curriculum 20 with window 20
Epoch: [474][20/30]	Time  1.630 ( 1.546)	Data  0.157 ( 0.069)	InnerLoop  0.644 ( 0.648)	Loss 4.1279e-01 (4.5760e-01)	Acc@1  85.79 ( 84.06)
The current update step is 14250
The current seed is 2070322378879288130
The current lr is: 0.0012
Testing Results:
 *   Acc@1 56.408
 *   Acc@1 56.571
 *   Acc@1 66.434
 *   Acc@1 66.374
 *   Acc@1 55.316
 *   Acc@1 55.390
 *   Acc@1 73.132
 *   Acc@1 73.822
 *   Acc@1 74.197
 *   Acc@1 74.895
 *   Acc@1 74.776
 *   Acc@1 75.302
Training for 300 epoch: 64.76973684210526
Training for 600 epoch: 70.31578947368422
Training for 1000 epoch: 65.04605263157895
Training for 300 epoch: 65.19625
Training for 600 epoch: 70.63458333333332
Training for 1000 epoch: 65.34583333333333
[[64.76973684210526, 70.31578947368422, 65.04605263157895], [65.19625, 70.63458333333332, 65.34583333333333]]
train loss 0.8693178550402323, epoch 474, best loss 0.5412657897313435, best_epoch 419
GPU_0_using curriculum 20 with window 20
Epoch: [475][20/30]	Time  1.511 ( 1.548)	Data  0.037 ( 0.051)	InnerLoop  0.645 ( 0.663)	Loss 4.9231e-01 (4.7285e-01)	Acc@1  83.18 ( 83.71)
The current update step is 14280
GPU_0_using curriculum 20 with window 20
Epoch: [476][20/30]	Time  1.643 ( 1.547)	Data  0.160 ( 0.069)	InnerLoop  0.639 ( 0.646)	Loss 4.4468e-01 (4.6444e-01)	Acc@1  84.52 ( 83.84)
The current update step is 14310
GPU_0_using curriculum 20 with window 20
Epoch: [477][20/30]	Time  1.526 ( 1.547)	Data  0.039 ( 0.058)	InnerLoop  0.642 ( 0.654)	Loss 4.5950e-01 (4.6857e-01)	Acc@1  84.62 ( 84.04)
The current update step is 14340
GPU_0_using curriculum 20 with window 20
Epoch: [478][20/30]	Time  1.516 ( 1.547)	Data  0.042 ( 0.045)	InnerLoop  0.647 ( 0.667)	Loss 4.1546e-01 (4.6240e-01)	Acc@1  85.11 ( 83.86)
The current update step is 14370
GPU_0_using curriculum 20 with window 20
Epoch: [479][20/30]	Time  1.510 ( 1.545)	Data  0.040 ( 0.070)	InnerLoop  0.642 ( 0.639)	Loss 4.6483e-01 (4.5468e-01)	Acc@1  83.94 ( 84.18)
The current update step is 14400
The current seed is 8174854713610579234
The current lr is: 0.0012
Testing Results:
 *   Acc@1 61.921
 *   Acc@1 62.248
 *   Acc@1 65.566
 *   Acc@1 65.193
 *   Acc@1 66.526
 *   Acc@1 66.370
 *   Acc@1 74.250
 *   Acc@1 74.310
 *   Acc@1 76.250
 *   Acc@1 76.635
 *   Acc@1 76.184
 *   Acc@1 76.623
Training for 300 epoch: 68.08552631578948
Training for 600 epoch: 70.90789473684211
Training for 1000 epoch: 71.35526315789474
Training for 300 epoch: 68.27875
Training for 600 epoch: 70.91416666666666
Training for 1000 epoch: 71.49666666666667
[[68.08552631578948, 70.90789473684211, 71.35526315789474], [68.27875, 70.91416666666666, 71.49666666666667]]
train loss 0.6864264377276103, epoch 479, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [480][20/30]	Time  1.632 ( 1.558)	Data  0.040 ( 0.070)	InnerLoop  0.762 ( 0.652)	Loss 4.3068e-01 (4.7415e-01)	Acc@1  85.13 ( 83.46)
The current update step is 14430
GPU_0_using curriculum 20 with window 20
Epoch: [481][20/30]	Time  1.506 ( 1.544)	Data  0.038 ( 0.056)	InnerLoop  0.638 ( 0.653)	Loss 5.7999e-01 (4.6197e-01)	Acc@1  80.52 ( 84.07)
The current update step is 14460
GPU_0_using curriculum 20 with window 20
Epoch: [482][20/30]	Time  1.521 ( 1.547)	Data  0.039 ( 0.044)	InnerLoop  0.654 ( 0.668)	Loss 4.3500e-01 (4.7361e-01)	Acc@1  84.45 ( 83.54)
The current update step is 14490
GPU_0_using curriculum 20 with window 20
Epoch: [483][20/30]	Time  1.518 ( 1.549)	Data  0.040 ( 0.051)	InnerLoop  0.639 ( 0.661)	Loss 4.0962e-01 (4.7660e-01)	Acc@1  86.06 ( 83.38)
The current update step is 14520
GPU_0_using curriculum 20 with window 20
Epoch: [484][20/30]	Time  1.682 ( 1.575)	Data  0.164 ( 0.073)	InnerLoop  0.662 ( 0.658)	Loss 5.1424e-01 (4.8138e-01)	Acc@1  81.15 ( 83.12)
The current update step is 14550
The current seed is 8372388619969592983
The current lr is: 0.0012
Testing Results:
 *   Acc@1 77.066
 *   Acc@1 76.793
 *   Acc@1 60.342
 *   Acc@1 60.134
 *   Acc@1 71.382
 *   Acc@1 71.819
 *   Acc@1 69.618
 *   Acc@1 69.580
 *   Acc@1 71.250
 *   Acc@1 71.132
 *   Acc@1 72.158
 *   Acc@1 72.238
Training for 300 epoch: 73.34210526315789
Training for 600 epoch: 65.79605263157895
Training for 1000 epoch: 71.76973684210526
Training for 300 epoch: 73.18625
Training for 600 epoch: 65.63333333333333
Training for 1000 epoch: 72.02875
[[73.34210526315789, 65.79605263157895, 71.76973684210526], [73.18625, 65.63333333333333, 72.02875]]
train loss 0.958712808450063, epoch 484, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [485][20/30]	Time  1.534 ( 1.563)	Data  0.038 ( 0.053)	InnerLoop  0.653 ( 0.669)	Loss 4.7516e-01 (4.6228e-01)	Acc@1  83.89 ( 84.03)
The current update step is 14580
GPU_0_using curriculum 20 with window 20
Epoch: [486][20/30]	Time  1.653 ( 1.561)	Data  0.161 ( 0.071)	InnerLoop  0.644 ( 0.652)	Loss 4.3182e-01 (4.6189e-01)	Acc@1  84.81 ( 83.85)
The current update step is 14610
GPU_0_using curriculum 20 with window 20
Epoch: [487][20/30]	Time  1.525 ( 1.562)	Data  0.042 ( 0.059)	InnerLoop  0.642 ( 0.664)	Loss 4.5416e-01 (4.8449e-01)	Acc@1  83.86 ( 83.27)
The current update step is 14640
GPU_0_using curriculum 20 with window 20
Epoch: [488][20/30]	Time  1.546 ( 1.552)	Data  0.040 ( 0.046)	InnerLoop  0.657 ( 0.670)	Loss 4.6463e-01 (4.6816e-01)	Acc@1  83.86 ( 83.45)
The current update step is 14670
GPU_0_using curriculum 20 with window 20
Epoch: [489][20/30]	Time  1.517 ( 1.547)	Data  0.042 ( 0.071)	InnerLoop  0.637 ( 0.639)	Loss 4.1656e-01 (4.5592e-01)	Acc@1  85.01 ( 84.16)
The current update step is 14700
The current seed is 17672927572498221463
The current lr is: 0.0012
Testing Results:
 *   Acc@1 62.197
 *   Acc@1 62.034
 *   Acc@1 62.026
 *   Acc@1 61.571
 *   Acc@1 61.408
 *   Acc@1 60.963
 *   Acc@1 72.066
 *   Acc@1 72.257
 *   Acc@1 71.197
 *   Acc@1 71.158
 *   Acc@1 71.921
 *   Acc@1 72.139
Training for 300 epoch: 67.13157894736841
Training for 600 epoch: 66.61184210526315
Training for 1000 epoch: 66.66447368421052
Training for 300 epoch: 67.14541666666666
Training for 600 epoch: 66.36458333333333
Training for 1000 epoch: 66.55125
[[67.13157894736841, 66.61184210526315, 66.66447368421052], [67.14541666666666, 66.36458333333333, 66.55125]]
train loss 0.7732704073588054, epoch 489, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [490][20/30]	Time  1.642 ( 1.551)	Data  0.040 ( 0.070)	InnerLoop  0.767 ( 0.647)	Loss 4.2749e-01 (4.9231e-01)	Acc@1  85.57 ( 82.92)
The current update step is 14730
GPU_0_using curriculum 20 with window 20
Epoch: [491][20/30]	Time  1.523 ( 1.542)	Data  0.046 ( 0.057)	InnerLoop  0.641 ( 0.650)	Loss 4.3152e-01 (4.8271e-01)	Acc@1  85.82 ( 83.18)
The current update step is 14760
GPU_0_using curriculum 20 with window 20
Epoch: [492][20/30]	Time  1.511 ( 1.540)	Data  0.037 ( 0.044)	InnerLoop  0.643 ( 0.663)	Loss 4.5042e-01 (4.7155e-01)	Acc@1  84.67 ( 83.59)
The current update step is 14790
GPU_0_using curriculum 20 with window 20
Epoch: [493][20/30]	Time  1.513 ( 1.541)	Data  0.041 ( 0.051)	InnerLoop  0.631 ( 0.656)	Loss 4.4694e-01 (4.8894e-01)	Acc@1  84.11 ( 82.94)
The current update step is 14820
GPU_0_using curriculum 20 with window 20
Epoch: [494][20/30]	Time  1.647 ( 1.547)	Data  0.165 ( 0.069)	InnerLoop  0.638 ( 0.644)	Loss 4.2704e-01 (4.6957e-01)	Acc@1  85.18 ( 83.85)
The current update step is 14850
The current seed is 10952610010827307005
The current lr is: 0.0012
Testing Results:
 *   Acc@1 59.605
 *   Acc@1 60.040
 *   Acc@1 60.197
 *   Acc@1 60.448
 *   Acc@1 60.908
 *   Acc@1 61.286
 *   Acc@1 53.237
 *   Acc@1 53.322
 *   Acc@1 61.632
 *   Acc@1 61.808
 *   Acc@1 62.132
 *   Acc@1 62.266
Training for 300 epoch: 56.421052631578945
Training for 600 epoch: 60.91447368421052
Training for 1000 epoch: 61.51973684210526
Training for 300 epoch: 56.68125
Training for 600 epoch: 61.12833333333333
Training for 1000 epoch: 61.77583333333334
[[56.421052631578945, 60.91447368421052, 61.51973684210526], [56.68125, 61.12833333333333, 61.77583333333334]]
train loss 1.3263193617502849, epoch 494, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [495][20/30]	Time  1.518 ( 1.540)	Data  0.043 ( 0.051)	InnerLoop  0.637 ( 0.657)	Loss 5.5166e-01 (4.6605e-01)	Acc@1  80.32 ( 83.93)
The current update step is 14880
GPU_0_using curriculum 20 with window 20
Epoch: [496][20/30]	Time  1.632 ( 1.548)	Data  0.160 ( 0.070)	InnerLoop  0.636 ( 0.645)	Loss 4.7074e-01 (4.8502e-01)	Acc@1  84.38 ( 83.04)
The current update step is 14910
GPU_0_using curriculum 20 with window 20
Epoch: [497][20/30]	Time  1.509 ( 1.540)	Data  0.039 ( 0.057)	InnerLoop  0.634 ( 0.648)	Loss 4.2603e-01 (4.8184e-01)	Acc@1  85.23 ( 83.08)
The current update step is 14940
GPU_0_using curriculum 20 with window 20
Epoch: [498][20/30]	Time  1.510 ( 1.537)	Data  0.039 ( 0.045)	InnerLoop  0.641 ( 0.660)	Loss 5.0107e-01 (4.8497e-01)	Acc@1  83.35 ( 83.12)
The current update step is 14970
GPU_0_using curriculum 20 with window 20
Epoch: [499][20/30]	Time  1.503 ( 1.542)	Data  0.038 ( 0.069)	InnerLoop  0.634 ( 0.638)	Loss 4.4156e-01 (4.6426e-01)	Acc@1  84.62 ( 83.98)
The current update step is 15000
The current seed is 89942272562352867
The current lr is: 0.0012
Testing Results:
 *   Acc@1 63.118
 *   Acc@1 63.737
 *   Acc@1 60.329
 *   Acc@1 60.551
 *   Acc@1 59.750
 *   Acc@1 60.353
 *   Acc@1 71.092
 *   Acc@1 71.422
 *   Acc@1 71.803
 *   Acc@1 71.971
 *   Acc@1 71.855
 *   Acc@1 72.241
Training for 300 epoch: 67.10526315789474
Training for 600 epoch: 66.06578947368422
Training for 1000 epoch: 65.80263157894737
Training for 300 epoch: 67.57916666666667
Training for 600 epoch: 66.26083333333334
Training for 1000 epoch: 66.29708333333333
[[67.10526315789474, 66.06578947368422, 65.80263157894737], [67.57916666666667, 66.26083333333334, 66.29708333333333]]
train loss 1.0977841288248698, epoch 499, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [500][20/30]	Time  1.630 ( 1.548)	Data  0.039 ( 0.068)	InnerLoop  0.756 ( 0.647)	Loss 4.3687e-01 (4.8990e-01)	Acc@1  85.18 ( 82.94)
The current update step is 15030
GPU_0_using curriculum 20 with window 20
Epoch: [501][20/30]	Time  1.510 ( 1.540)	Data  0.038 ( 0.056)	InnerLoop  0.645 ( 0.652)	Loss 4.9715e-01 (4.8422e-01)	Acc@1  83.40 ( 83.04)
The current update step is 15060
GPU_0_using curriculum 20 with window 20
Epoch: [502][20/30]	Time  1.505 ( 1.545)	Data  0.038 ( 0.044)	InnerLoop  0.636 ( 0.665)	Loss 5.0870e-01 (5.1058e-01)	Acc@1  81.98 ( 82.13)
The current update step is 15090
GPU_0_using curriculum 20 with window 20
Epoch: [503][20/30]	Time  1.515 ( 1.538)	Data  0.039 ( 0.050)	InnerLoop  0.640 ( 0.657)	Loss 4.5984e-01 (4.7162e-01)	Acc@1  84.38 ( 83.69)
The current update step is 15120
GPU_0_using curriculum 20 with window 20
Epoch: [504][20/30]	Time  1.633 ( 1.549)	Data  0.156 ( 0.069)	InnerLoop  0.645 ( 0.647)	Loss 4.8762e-01 (4.6272e-01)	Acc@1  82.93 ( 83.93)
The current update step is 15150
The current seed is 2832788500757575291
The current lr is: 0.0012
Testing Results:
 *   Acc@1 46.368
 *   Acc@1 47.203
 *   Acc@1 61.934
 *   Acc@1 62.538
 *   Acc@1 64.079
 *   Acc@1 64.487
 *   Acc@1 59.224
 *   Acc@1 59.182
 *   Acc@1 61.211
 *   Acc@1 61.183
 *   Acc@1 61.829
 *   Acc@1 62.048
Training for 300 epoch: 52.796052631578945
Training for 600 epoch: 61.57236842105263
Training for 1000 epoch: 62.953947368421055
Training for 300 epoch: 53.192499999999995
Training for 600 epoch: 61.860416666666666
Training for 1000 epoch: 63.2675
[[52.796052631578945, 61.57236842105263, 62.953947368421055], [53.192499999999995, 61.860416666666666, 63.2675]]
train loss 1.2978500602722167, epoch 504, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [505][20/30]	Time  1.511 ( 1.544)	Data  0.039 ( 0.051)	InnerLoop  0.642 ( 0.657)	Loss 4.6333e-01 (4.7321e-01)	Acc@1  84.67 ( 83.69)
The current update step is 15180
GPU_0_using curriculum 20 with window 20
Epoch: [506][20/30]	Time  1.674 ( 1.550)	Data  0.156 ( 0.069)	InnerLoop  0.669 ( 0.644)	Loss 4.5045e-01 (4.6498e-01)	Acc@1  83.33 ( 83.85)
The current update step is 15210
GPU_0_using curriculum 20 with window 20
Epoch: [507][20/30]	Time  1.520 ( 1.544)	Data  0.041 ( 0.057)	InnerLoop  0.646 ( 0.655)	Loss 4.0531e-01 (4.6760e-01)	Acc@1  86.13 ( 83.88)
The current update step is 15240
GPU_0_using curriculum 20 with window 20
Epoch: [508][20/30]	Time  1.502 ( 1.543)	Data  0.037 ( 0.045)	InnerLoop  0.636 ( 0.664)	Loss 4.6472e-01 (4.8511e-01)	Acc@1  83.64 ( 83.32)
The current update step is 15270
GPU_0_using curriculum 20 with window 20
Epoch: [509][20/30]	Time  1.516 ( 1.537)	Data  0.039 ( 0.070)	InnerLoop  0.643 ( 0.635)	Loss 4.5374e-01 (4.6450e-01)	Acc@1  84.55 ( 83.99)
The current update step is 15300
The current seed is 3722104678739314179
The current lr is: 0.0012
Testing Results:
 *   Acc@1 55.645
 *   Acc@1 56.054
 *   Acc@1 66.882
 *   Acc@1 66.872
 *   Acc@1 46.408
 *   Acc@1 46.101
 *   Acc@1 62.632
 *   Acc@1 62.797
 *   Acc@1 66.855
 *   Acc@1 68.030
 *   Acc@1 66.276
 *   Acc@1 67.463
Training for 300 epoch: 59.138157894736835
Training for 600 epoch: 66.86842105263159
Training for 1000 epoch: 56.34210526315789
Training for 300 epoch: 59.42541666666666
Training for 600 epoch: 67.45083333333334
Training for 1000 epoch: 56.781666666666666
[[59.138157894736835, 66.86842105263159, 56.34210526315789], [59.42541666666666, 67.45083333333334, 56.781666666666666]]
train loss 1.2872170614242553, epoch 509, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [510][20/30]	Time  1.623 ( 1.543)	Data  0.040 ( 0.069)	InnerLoop  0.753 ( 0.643)	Loss 4.4332e-01 (4.7798e-01)	Acc@1  84.74 ( 83.41)
The current update step is 15330
GPU_0_using curriculum 20 with window 20
Epoch: [511][20/30]	Time  1.507 ( 1.538)	Data  0.040 ( 0.056)	InnerLoop  0.633 ( 0.649)	Loss 4.5341e-01 (4.7370e-01)	Acc@1  84.13 ( 83.51)
The current update step is 15360
GPU_0_using curriculum 20 with window 20
Epoch: [512][20/30]	Time  1.514 ( 1.539)	Data  0.038 ( 0.045)	InnerLoop  0.645 ( 0.663)	Loss 4.3079e-01 (4.5949e-01)	Acc@1  85.30 ( 83.95)
The current update step is 15390
GPU_0_using curriculum 20 with window 20
Epoch: [513][20/30]	Time  1.507 ( 1.536)	Data  0.040 ( 0.051)	InnerLoop  0.636 ( 0.654)	Loss 4.5059e-01 (4.8522e-01)	Acc@1  83.86 ( 83.18)
The current update step is 15420
GPU_0_using curriculum 20 with window 20
Epoch: [514][20/30]	Time  1.631 ( 1.541)	Data  0.156 ( 0.068)	InnerLoop  0.639 ( 0.642)	Loss 4.9815e-01 (4.5650e-01)	Acc@1  82.96 ( 84.28)
The current update step is 15450
The current seed is 641851268981621318
The current lr is: 0.0012
Testing Results:
 *   Acc@1 72.013
 *   Acc@1 73.375
 *   Acc@1 45.487
 *   Acc@1 45.615
 *   Acc@1 47.737
 *   Acc@1 47.626
 *   Acc@1 71.447
 *   Acc@1 71.552
 *   Acc@1 56.684
 *   Acc@1 56.648
 *   Acc@1 53.803
 *   Acc@1 53.926
Training for 300 epoch: 71.73026315789474
Training for 600 epoch: 51.08552631578947
Training for 1000 epoch: 50.76973684210526
Training for 300 epoch: 72.46375
Training for 600 epoch: 51.13125
Training for 1000 epoch: 50.77583333333334
[[71.73026315789474, 51.08552631578947, 50.76973684210526], [72.46375, 51.13125, 50.77583333333334]]
train loss 1.486605169169108, epoch 514, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [515][20/30]	Time  1.519 ( 1.548)	Data  0.039 ( 0.052)	InnerLoop  0.645 ( 0.658)	Loss 4.4999e-01 (4.6481e-01)	Acc@1  83.84 ( 83.62)
The current update step is 15480
GPU_0_using curriculum 20 with window 20
Epoch: [516][20/30]	Time  1.640 ( 1.548)	Data  0.162 ( 0.069)	InnerLoop  0.641 ( 0.644)	Loss 4.9548e-01 (4.7861e-01)	Acc@1  81.64 ( 83.24)
The current update step is 15510
GPU_0_using curriculum 20 with window 20
Epoch: [517][20/30]	Time  1.508 ( 1.538)	Data  0.039 ( 0.058)	InnerLoop  0.636 ( 0.647)	Loss 4.6922e-01 (4.6797e-01)	Acc@1  83.69 ( 83.84)
The current update step is 15540
GPU_0_using curriculum 20 with window 20
Epoch: [518][20/30]	Time  1.530 ( 1.552)	Data  0.043 ( 0.045)	InnerLoop  0.645 ( 0.668)	Loss 5.0114e-01 (4.7835e-01)	Acc@1  82.06 ( 83.24)
The current update step is 15570
GPU_0_using curriculum 20 with window 20
Epoch: [519][20/30]	Time  1.521 ( 1.554)	Data  0.043 ( 0.071)	InnerLoop  0.642 ( 0.643)	Loss 4.3845e-01 (4.8000e-01)	Acc@1  85.18 ( 83.28)
The current update step is 15600
The current seed is 16610306369404146342
The current lr is: 0.0012
Testing Results:
 *   Acc@1 56.711
 *   Acc@1 57.262
 *   Acc@1 48.539
 *   Acc@1 48.358
 *   Acc@1 64.158
 *   Acc@1 64.828
 *   Acc@1 63.566
 *   Acc@1 65.227
 *   Acc@1 64.355
 *   Acc@1 65.297
 *   Acc@1 64.079
 *   Acc@1 65.237
Training for 300 epoch: 60.13815789473684
Training for 600 epoch: 56.44736842105263
Training for 1000 epoch: 64.11842105263159
Training for 300 epoch: 61.24416666666667
Training for 600 epoch: 56.8275
Training for 1000 epoch: 65.03208333333333
[[60.13815789473684, 56.44736842105263, 64.11842105263159], [61.24416666666667, 56.8275, 65.03208333333333]]
train loss 1.1341104608535766, epoch 519, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [520][20/30]	Time  1.652 ( 1.570)	Data  0.038 ( 0.072)	InnerLoop  0.777 ( 0.654)	Loss 4.9639e-01 (4.8563e-01)	Acc@1  82.86 ( 83.16)
The current update step is 15630
GPU_0_using curriculum 20 with window 20
Epoch: [521][20/30]	Time  1.523 ( 1.556)	Data  0.041 ( 0.060)	InnerLoop  0.644 ( 0.658)	Loss 4.5844e-01 (4.8392e-01)	Acc@1  84.57 ( 83.22)
The current update step is 15660
GPU_0_using curriculum 20 with window 20
Epoch: [522][20/30]	Time  1.506 ( 1.552)	Data  0.042 ( 0.046)	InnerLoop  0.633 ( 0.669)	Loss 4.6234e-01 (4.8085e-01)	Acc@1  84.40 ( 83.03)
The current update step is 15690
GPU_0_using curriculum 20 with window 20
Epoch: [523][20/30]	Time  1.523 ( 1.544)	Data  0.040 ( 0.051)	InnerLoop  0.641 ( 0.658)	Loss 4.6579e-01 (4.6307e-01)	Acc@1  82.96 ( 83.80)
The current update step is 15720
GPU_0_using curriculum 20 with window 20
Epoch: [524][20/30]	Time  1.648 ( 1.555)	Data  0.160 ( 0.069)	InnerLoop  0.644 ( 0.648)	Loss 4.7171e-01 (4.6664e-01)	Acc@1  83.96 ( 84.01)
The current update step is 15750
The current seed is 16452074405490834517
The current lr is: 0.0012
Testing Results:
 *   Acc@1 58.605
 *   Acc@1 58.987
 *   Acc@1 58.158
 *   Acc@1 58.462
 *   Acc@1 57.237
 *   Acc@1 57.704
 *   Acc@1 66.934
 *   Acc@1 67.008
 *   Acc@1 68.461
 *   Acc@1 68.737
 *   Acc@1 69.026
 *   Acc@1 68.803
Training for 300 epoch: 62.76973684210527
Training for 600 epoch: 63.309210526315795
Training for 1000 epoch: 63.131578947368425
Training for 300 epoch: 62.9975
Training for 600 epoch: 63.59958333333333
Training for 1000 epoch: 63.25375
[[62.76973684210527, 63.309210526315795, 63.131578947368425], [62.9975, 63.59958333333333, 63.25375]]
train loss 1.1262686508178712, epoch 524, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [525][20/30]	Time  1.537 ( 1.546)	Data  0.038 ( 0.050)	InnerLoop  0.639 ( 0.658)	Loss 6.5650e-01 (4.8315e-01)	Acc@1  77.39 ( 83.35)
The current update step is 15780
GPU_0_using curriculum 20 with window 20
Epoch: [526][20/30]	Time  1.629 ( 1.545)	Data  0.158 ( 0.069)	InnerLoop  0.641 ( 0.646)	Loss 4.4619e-01 (4.7658e-01)	Acc@1  84.62 ( 83.54)
The current update step is 15810
GPU_0_using curriculum 20 with window 20
Epoch: [527][20/30]	Time  1.508 ( 1.549)	Data  0.039 ( 0.057)	InnerLoop  0.640 ( 0.658)	Loss 4.5500e-01 (4.8453e-01)	Acc@1  84.84 ( 83.32)
The current update step is 15840
GPU_0_using curriculum 20 with window 20
Epoch: [528][20/30]	Time  1.538 ( 1.550)	Data  0.038 ( 0.045)	InnerLoop  0.664 ( 0.668)	Loss 4.1869e-01 (4.5781e-01)	Acc@1  85.28 ( 84.12)
The current update step is 15870
GPU_0_using curriculum 20 with window 20
Epoch: [529][20/30]	Time  1.519 ( 1.543)	Data  0.037 ( 0.069)	InnerLoop  0.637 ( 0.640)	Loss 4.3252e-01 (4.6528e-01)	Acc@1  84.94 ( 83.70)
The current update step is 15900
The current seed is 11503978940317389141
The current lr is: 0.0012
Testing Results:
 *   Acc@1 64.908
 *   Acc@1 65.034
 *   Acc@1 65.118
 *   Acc@1 65.336
 *   Acc@1 65.053
 *   Acc@1 65.522
 *   Acc@1 60.434
 *   Acc@1 60.598
 *   Acc@1 60.816
 *   Acc@1 61.428
 *   Acc@1 59.105
 *   Acc@1 59.124
Training for 300 epoch: 62.671052631578945
Training for 600 epoch: 62.96710526315789
Training for 1000 epoch: 62.078947368421055
Training for 300 epoch: 62.81625
Training for 600 epoch: 63.38166666666666
Training for 1000 epoch: 62.32333333333333
[[62.671052631578945, 62.96710526315789, 62.078947368421055], [62.81625, 63.38166666666666, 62.32333333333333]]
train loss 1.59987086353302, epoch 529, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [530][20/30]	Time  1.623 ( 1.537)	Data  0.043 ( 0.070)	InnerLoop  0.756 ( 0.641)	Loss 4.2358e-01 (4.8915e-01)	Acc@1  86.08 ( 82.75)
The current update step is 15930
GPU_0_using curriculum 20 with window 20
Epoch: [531][20/30]	Time  1.498 ( 1.533)	Data  0.042 ( 0.057)	InnerLoop  0.631 ( 0.647)	Loss 4.6064e-01 (5.0010e-01)	Acc@1  84.03 ( 82.74)
The current update step is 15960
GPU_0_using curriculum 20 with window 20
Epoch: [532][20/30]	Time  1.516 ( 1.534)	Data  0.038 ( 0.044)	InnerLoop  0.633 ( 0.660)	Loss 4.2853e-01 (4.6871e-01)	Acc@1  85.69 ( 83.79)
The current update step is 15990
GPU_0_using curriculum 20 with window 20
Epoch: [533][20/30]	Time  1.506 ( 1.537)	Data  0.037 ( 0.051)	InnerLoop  0.630 ( 0.653)	Loss 4.5461e-01 (4.6027e-01)	Acc@1  83.84 ( 84.08)
The current update step is 16020
GPU_0_using curriculum 20 with window 20
Epoch: [534][20/30]	Time  1.614 ( 1.548)	Data  0.156 ( 0.070)	InnerLoop  0.630 ( 0.643)	Loss 4.2136e-01 (4.6580e-01)	Acc@1  84.84 ( 83.72)
The current update step is 16050
The current seed is 1633697704313787956
The current lr is: 0.0012
Testing Results:
 *   Acc@1 62.105
 *   Acc@1 63.359
 *   Acc@1 61.211
 *   Acc@1 61.437
 *   Acc@1 62.053
 *   Acc@1 62.271
 *   Acc@1 67.895
 *   Acc@1 68.243
 *   Acc@1 68.421
 *   Acc@1 68.840
 *   Acc@1 69.303
 *   Acc@1 68.948
Training for 300 epoch: 65.0
Training for 600 epoch: 64.8157894736842
Training for 1000 epoch: 65.67763157894737
Training for 300 epoch: 65.80125000000001
Training for 600 epoch: 65.13833333333334
Training for 1000 epoch: 65.60916666666667
[[65.0, 64.8157894736842, 65.67763157894737], [65.80125000000001, 65.13833333333334, 65.60916666666667]]
train loss 0.9866510429382325, epoch 534, best loss 0.5412657897313435, best_epoch 479
GPU_0_using curriculum 20 with window 20
Epoch: [535][20/30]	Time  1.510 ( 1.540)	Data  0.036 ( 0.050)	InnerLoop  0.633 ( 0.654)	Loss 4.2444e-01 (4.7137e-01)	Acc@1  84.96 ( 83.47)
The current update step is 16080
GPU_0_using curriculum 20 with window 20
Epoch: [536][20/30]	Time  1.633 ( 1.546)	Data  0.159 ( 0.070)	InnerLoop  0.633 ( 0.643)	Loss 4.4954e-01 (4.5514e-01)	Acc@1  84.91 ( 84.25)
The current update step is 16110
GPU_0_using curriculum 20 with window 20
Epoch: [537][20/30]	Time  1.503 ( 1.534)	Data  0.038 ( 0.057)	InnerLoop  0.634 ( 0.648)	Loss 4.8285e-01 (4.5447e-01)	Acc@1  82.74 ( 84.29)
The current update step is 16140
GPU_0_using curriculum 20 with window 20
Epoch: [538][20/30]	Time  1.529 ( 1.532)	Data  0.038 ( 0.044)	InnerLoop  0.637 ( 0.658)	Loss 4.2660e-01 (4.7380e-01)	Acc@1  85.94 ( 83.60)
The current update step is 16170
GPU_0_using curriculum 20 with window 20
Epoch: [539][20/30]	Time  1.509 ( 1.535)	Data  0.040 ( 0.069)	InnerLoop  0.638 ( 0.636)	Loss 4.8510e-01 (4.8468e-01)	Acc@1  83.67 ( 83.05)
The current update step is 16200
The current seed is 1628128388700320584
The current lr is: 0.0012
Testing Results:
 *   Acc@1 55.026
 *   Acc@1 55.773
 *   Acc@1 40.974
 *   Acc@1 40.808
 *   Acc@1 41.434
 *   Acc@1 41.538
 *   Acc@1 64.737
 *   Acc@1 65.573
 *   Acc@1 65.711
 *   Acc@1 66.507
 *   Acc@1 57.013
 *   Acc@1 56.547
Training for 300 epoch: 59.881578947368425
Training for 600 epoch: 53.3421052631579
Training for 1000 epoch: 49.223684210526315
Training for 300 epoch: 60.672916666666666
Training for 600 epoch: 53.6575
Training for 1000 epoch: 49.04291666666667
[[59.881578947368425, 53.3421052631579, 49.223684210526315], [60.672916666666666, 53.6575, 49.04291666666667]]
train loss 1.3925398926417032, epoch 539, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [540][20/30]	Time  1.635 ( 1.546)	Data  0.041 ( 0.070)	InnerLoop  0.752 ( 0.641)	Loss 4.8348e-01 (4.8934e-01)	Acc@1  83.62 ( 83.11)
The current update step is 16230
GPU_0_using curriculum 20 with window 20
Epoch: [541][20/30]	Time  1.506 ( 1.533)	Data  0.039 ( 0.057)	InnerLoop  0.631 ( 0.646)	Loss 4.3556e-01 (4.5432e-01)	Acc@1  84.59 ( 84.32)
The current update step is 16260
GPU_0_using curriculum 20 with window 20
Epoch: [542][20/30]	Time  1.542 ( 1.535)	Data  0.039 ( 0.045)	InnerLoop  0.639 ( 0.660)	Loss 4.3572e-01 (4.6731e-01)	Acc@1  84.96 ( 83.38)
The current update step is 16290
GPU_0_using curriculum 20 with window 20
Epoch: [543][20/30]	Time  1.524 ( 1.536)	Data  0.042 ( 0.050)	InnerLoop  0.634 ( 0.655)	Loss 4.2488e-01 (4.6416e-01)	Acc@1  85.21 ( 83.92)
The current update step is 16320
GPU_0_using curriculum 20 with window 20
Epoch: [544][20/30]	Time  1.614 ( 1.536)	Data  0.157 ( 0.068)	InnerLoop  0.628 ( 0.641)	Loss 4.6989e-01 (4.7170e-01)	Acc@1  84.16 ( 83.52)
The current update step is 16350
The current seed is 4720188654815869513
The current lr is: 0.0012
Testing Results:
 *   Acc@1 68.395
 *   Acc@1 68.162
 *   Acc@1 69.184
 *   Acc@1 68.630
 *   Acc@1 68.092
 *   Acc@1 68.080
 *   Acc@1 77.526
 *   Acc@1 78.429
 *   Acc@1 77.842
 *   Acc@1 78.948
 *   Acc@1 78.303
 *   Acc@1 78.958
Training for 300 epoch: 72.96052631578948
Training for 600 epoch: 73.51315789473685
Training for 1000 epoch: 73.19736842105263
Training for 300 epoch: 73.29583333333332
Training for 600 epoch: 73.78916666666666
Training for 1000 epoch: 73.51916666666666
[[72.96052631578948, 73.51315789473685, 73.19736842105263], [73.29583333333332, 73.78916666666666, 73.51916666666666]]
train loss 0.6313805020650228, epoch 544, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [545][20/30]	Time  1.500 ( 1.534)	Data  0.040 ( 0.051)	InnerLoop  0.634 ( 0.654)	Loss 4.8419e-01 (4.8299e-01)	Acc@1  83.64 ( 83.27)
The current update step is 16380
GPU_0_using curriculum 20 with window 20
Epoch: [546][20/30]	Time  1.623 ( 1.538)	Data  0.159 ( 0.069)	InnerLoop  0.635 ( 0.642)	Loss 4.8353e-01 (4.7081e-01)	Acc@1  83.40 ( 83.56)
The current update step is 16410
GPU_0_using curriculum 20 with window 20
Epoch: [547][20/30]	Time  1.520 ( 1.543)	Data  0.038 ( 0.057)	InnerLoop  0.637 ( 0.651)	Loss 4.9913e-01 (4.7085e-01)	Acc@1  81.45 ( 83.62)
The current update step is 16440
GPU_0_using curriculum 20 with window 20
Epoch: [548][20/30]	Time  1.516 ( 1.539)	Data  0.040 ( 0.045)	InnerLoop  0.650 ( 0.661)	Loss 4.7401e-01 (4.7106e-01)	Acc@1  84.01 ( 83.74)
The current update step is 16470
GPU_0_using curriculum 20 with window 20
Epoch: [549][20/30]	Time  1.505 ( 1.536)	Data  0.039 ( 0.070)	InnerLoop  0.641 ( 0.634)	Loss 4.4855e-01 (4.8699e-01)	Acc@1  84.72 ( 82.97)
The current update step is 16500
The current seed is 16849443292704307490
The current lr is: 0.0012
Testing Results:
 *   Acc@1 45.855
 *   Acc@1 46.728
 *   Acc@1 57.500
 *   Acc@1 58.125
 *   Acc@1 66.289
 *   Acc@1 66.992
 *   Acc@1 54.395
 *   Acc@1 54.991
 *   Acc@1 53.132
 *   Acc@1 53.873
 *   Acc@1 53.855
 *   Acc@1 53.877
Training for 300 epoch: 50.125
Training for 600 epoch: 55.315789473684205
Training for 1000 epoch: 60.07236842105263
Training for 300 epoch: 50.85958333333333
Training for 600 epoch: 55.99875
Training for 1000 epoch: 60.43458333333333
[[50.125, 55.315789473684205, 60.07236842105263], [50.85958333333333, 55.99875, 60.43458333333333]]
train loss 1.7092947253545125, epoch 549, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [550][20/30]	Time  1.637 ( 1.546)	Data  0.042 ( 0.069)	InnerLoop  0.767 ( 0.645)	Loss 4.8145e-01 (4.6274e-01)	Acc@1  82.23 ( 84.12)
The current update step is 16530
GPU_0_using curriculum 20 with window 20
Epoch: [551][20/30]	Time  1.514 ( 1.539)	Data  0.039 ( 0.058)	InnerLoop  0.647 ( 0.653)	Loss 4.8500e-01 (4.5870e-01)	Acc@1  83.76 ( 84.16)
The current update step is 16560
GPU_0_using curriculum 20 with window 20
Epoch: [552][20/30]	Time  1.506 ( 1.540)	Data  0.040 ( 0.045)	InnerLoop  0.637 ( 0.663)	Loss 4.2108e-01 (4.6569e-01)	Acc@1  85.18 ( 83.90)
The current update step is 16590
GPU_0_using curriculum 20 with window 20
Epoch: [553][20/30]	Time  1.506 ( 1.532)	Data  0.039 ( 0.050)	InnerLoop  0.636 ( 0.656)	Loss 4.0522e-01 (4.7219e-01)	Acc@1  85.84 ( 83.70)
The current update step is 16620
GPU_0_using curriculum 20 with window 20
Epoch: [554][20/30]	Time  1.627 ( 1.541)	Data  0.161 ( 0.069)	InnerLoop  0.633 ( 0.643)	Loss 4.7136e-01 (4.6668e-01)	Acc@1  84.01 ( 84.17)
The current update step is 16650
The current seed is 3250993038777760946
The current lr is: 0.0012
Testing Results:
 *   Acc@1 57.316
 *   Acc@1 58.213
 *   Acc@1 57.789
 *   Acc@1 58.303
 *   Acc@1 58.079
 *   Acc@1 58.668
 *   Acc@1 71.355
 *   Acc@1 71.738
 *   Acc@1 72.079
 *   Acc@1 72.452
 *   Acc@1 72.829
 *   Acc@1 73.425
Training for 300 epoch: 64.33552631578948
Training for 600 epoch: 64.9342105263158
Training for 1000 epoch: 65.45394736842105
Training for 300 epoch: 64.97583333333333
Training for 600 epoch: 65.3775
Training for 1000 epoch: 66.04666666666667
[[64.33552631578948, 64.9342105263158, 65.45394736842105], [64.97583333333333, 65.3775, 66.04666666666667]]
train loss 0.8269340822219848, epoch 554, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [555][20/30]	Time  1.507 ( 1.543)	Data  0.039 ( 0.051)	InnerLoop  0.631 ( 0.659)	Loss 4.5792e-01 (4.5505e-01)	Acc@1  84.08 ( 84.32)
The current update step is 16680
GPU_0_using curriculum 20 with window 20
Epoch: [556][20/30]	Time  1.670 ( 1.551)	Data  0.162 ( 0.070)	InnerLoop  0.657 ( 0.648)	Loss 4.4270e-01 (4.6038e-01)	Acc@1  84.38 ( 84.08)
The current update step is 16710
GPU_0_using curriculum 20 with window 20
Epoch: [557][20/30]	Time  1.517 ( 1.558)	Data  0.040 ( 0.059)	InnerLoop  0.643 ( 0.659)	Loss 4.1539e-01 (4.5956e-01)	Acc@1  86.06 ( 84.15)
The current update step is 16740
GPU_0_using curriculum 20 with window 20
Epoch: [558][20/30]	Time  1.518 ( 1.552)	Data  0.042 ( 0.046)	InnerLoop  0.639 ( 0.671)	Loss 5.0192e-01 (4.6589e-01)	Acc@1  81.69 ( 83.67)
The current update step is 16770
GPU_0_using curriculum 20 with window 20
Epoch: [559][20/30]	Time  1.525 ( 1.558)	Data  0.041 ( 0.072)	InnerLoop  0.649 ( 0.647)	Loss 4.3513e-01 (4.5872e-01)	Acc@1  84.91 ( 83.92)
The current update step is 16800
The current seed is 7331700499286361391
The current lr is: 0.0012
Testing Results:
 *   Acc@1 61.382
 *   Acc@1 62.500
 *   Acc@1 60.539
 *   Acc@1 61.306
 *   Acc@1 53.868
 *   Acc@1 54.352
 *   Acc@1 62.553
 *   Acc@1 62.930
 *   Acc@1 58.737
 *   Acc@1 58.547
 *   Acc@1 66.000
 *   Acc@1 65.733
Training for 300 epoch: 61.96710526315789
Training for 600 epoch: 59.63815789473684
Training for 1000 epoch: 59.934210526315795
Training for 300 epoch: 62.715
Training for 600 epoch: 59.92666666666666
Training for 1000 epoch: 60.042500000000004
[[61.96710526315789, 59.63815789473684, 59.934210526315795], [62.715, 59.92666666666666, 60.042500000000004]]
train loss 1.213426693280538, epoch 559, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [560][20/30]	Time  1.632 ( 1.559)	Data  0.038 ( 0.071)	InnerLoop  0.761 ( 0.652)	Loss 4.1091e-01 (4.5964e-01)	Acc@1  85.55 ( 83.78)
The current update step is 16830
GPU_0_using curriculum 20 with window 20
Epoch: [561][20/30]	Time  1.512 ( 1.543)	Data  0.042 ( 0.058)	InnerLoop  0.637 ( 0.653)	Loss 4.6904e-01 (4.6147e-01)	Acc@1  82.89 ( 83.95)
The current update step is 16860
GPU_0_using curriculum 20 with window 20
Epoch: [562][20/30]	Time  1.523 ( 1.545)	Data  0.044 ( 0.047)	InnerLoop  0.642 ( 0.667)	Loss 3.9614e-01 (4.6529e-01)	Acc@1  86.04 ( 83.66)
The current update step is 16890
GPU_0_using curriculum 20 with window 20
Epoch: [563][20/30]	Time  1.518 ( 1.541)	Data  0.039 ( 0.050)	InnerLoop  0.635 ( 0.658)	Loss 4.0462e-01 (4.5387e-01)	Acc@1  85.69 ( 84.24)
The current update step is 16920
GPU_0_using curriculum 20 with window 20
Epoch: [564][20/30]	Time  1.641 ( 1.543)	Data  0.153 ( 0.069)	InnerLoop  0.645 ( 0.644)	Loss 4.3948e-01 (4.6618e-01)	Acc@1  85.25 ( 83.88)
The current update step is 16950
The current seed is 653703245466705475
The current lr is: 0.0012
Testing Results:
 *   Acc@1 54.842
 *   Acc@1 55.138
 *   Acc@1 56.776
 *   Acc@1 56.708
 *   Acc@1 57.316
 *   Acc@1 57.716
 *   Acc@1 55.039
 *   Acc@1 54.806
 *   Acc@1 66.526
 *   Acc@1 67.298
 *   Acc@1 64.092
 *   Acc@1 64.347
Training for 300 epoch: 54.94078947368421
Training for 600 epoch: 61.651315789473685
Training for 1000 epoch: 60.703947368421055
Training for 300 epoch: 54.97208333333333
Training for 600 epoch: 62.002916666666664
Training for 1000 epoch: 61.03125
[[54.94078947368421, 61.651315789473685, 60.703947368421055], [54.97208333333333, 62.002916666666664, 61.03125]]
train loss 1.3085339853286744, epoch 564, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [565][20/30]	Time  1.514 ( 1.538)	Data  0.037 ( 0.051)	InnerLoop  0.635 ( 0.657)	Loss 4.1605e-01 (4.5717e-01)	Acc@1  86.08 ( 84.25)
The current update step is 16980
GPU_0_using curriculum 20 with window 20
Epoch: [566][20/30]	Time  1.634 ( 1.547)	Data  0.159 ( 0.069)	InnerLoop  0.634 ( 0.646)	Loss 4.7308e-01 (4.6699e-01)	Acc@1  83.74 ( 84.05)
The current update step is 17010
GPU_0_using curriculum 20 with window 20
Epoch: [567][20/30]	Time  1.504 ( 1.544)	Data  0.037 ( 0.057)	InnerLoop  0.638 ( 0.655)	Loss 4.2809e-01 (4.9339e-01)	Acc@1  85.38 ( 82.83)
The current update step is 17040
GPU_0_using curriculum 20 with window 20
Epoch: [568][20/30]	Time  1.501 ( 1.544)	Data  0.040 ( 0.044)	InnerLoop  0.636 ( 0.664)	Loss 4.2925e-01 (4.5223e-01)	Acc@1  85.38 ( 84.25)
The current update step is 17070
GPU_0_using curriculum 20 with window 20
Epoch: [569][20/30]	Time  1.505 ( 1.544)	Data  0.038 ( 0.069)	InnerLoop  0.639 ( 0.641)	Loss 4.5770e-01 (4.5925e-01)	Acc@1  84.20 ( 84.02)
The current update step is 17100
The current seed is 7381596180840395948
The current lr is: 0.0012
Testing Results:
 *   Acc@1 66.316
 *   Acc@1 67.089
 *   Acc@1 66.039
 *   Acc@1 67.434
 *   Acc@1 65.803
 *   Acc@1 67.198
 *   Acc@1 71.803
 *   Acc@1 72.238
 *   Acc@1 61.605
 *   Acc@1 61.943
 *   Acc@1 58.013
 *   Acc@1 58.394
Training for 300 epoch: 69.05921052631578
Training for 600 epoch: 63.82236842105263
Training for 1000 epoch: 61.90789473684211
Training for 300 epoch: 69.66375
Training for 600 epoch: 64.68875
Training for 1000 epoch: 62.795833333333334
[[69.05921052631578, 63.82236842105263, 61.90789473684211], [69.66375, 64.68875, 62.795833333333334]]
train loss 1.5252196832656861, epoch 569, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [570][20/30]	Time  1.663 ( 1.547)	Data  0.040 ( 0.069)	InnerLoop  0.767 ( 0.645)	Loss 4.2415e-01 (4.5938e-01)	Acc@1  85.94 ( 84.13)
The current update step is 17130
GPU_0_using curriculum 20 with window 20
Epoch: [571][20/30]	Time  1.551 ( 1.536)	Data  0.037 ( 0.057)	InnerLoop  0.649 ( 0.649)	Loss 5.0204e-01 (4.6075e-01)	Acc@1  82.42 ( 84.06)
The current update step is 17160
GPU_0_using curriculum 20 with window 20
Epoch: [572][20/30]	Time  1.504 ( 1.538)	Data  0.037 ( 0.044)	InnerLoop  0.641 ( 0.664)	Loss 4.4126e-01 (4.7269e-01)	Acc@1  84.30 ( 83.47)
The current update step is 17190
GPU_0_using curriculum 20 with window 20
Epoch: [573][20/30]	Time  1.531 ( 1.544)	Data  0.039 ( 0.051)	InnerLoop  0.664 ( 0.661)	Loss 4.4029e-01 (4.6398e-01)	Acc@1  84.79 ( 84.01)
The current update step is 17220
GPU_0_using curriculum 20 with window 20
Epoch: [574][20/30]	Time  1.645 ( 1.563)	Data  0.162 ( 0.071)	InnerLoop  0.645 ( 0.654)	Loss 5.8345e-01 (4.8830e-01)	Acc@1  78.32 ( 82.73)
The current update step is 17250
The current seed is 10404245038838256006
The current lr is: 0.0012
Testing Results:
 *   Acc@1 71.066
 *   Acc@1 71.853
 *   Acc@1 43.711
 *   Acc@1 43.591
 *   Acc@1 55.132
 *   Acc@1 55.967
 *   Acc@1 52.355
 *   Acc@1 52.535
 *   Acc@1 54.618
 *   Acc@1 54.670
 *   Acc@1 55.658
 *   Acc@1 56.166
Training for 300 epoch: 61.71052631578947
Training for 600 epoch: 49.16447368421053
Training for 1000 epoch: 55.39473684210526
Training for 300 epoch: 62.19416666666667
Training for 600 epoch: 49.13041666666667
Training for 1000 epoch: 56.06625
[[61.71052631578947, 49.16447368421053, 55.39473684210526], [62.19416666666667, 49.13041666666667, 56.06625]]
train loss 1.52605479221344, epoch 574, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [575][20/30]	Time  1.535 ( 1.561)	Data  0.041 ( 0.052)	InnerLoop  0.648 ( 0.667)	Loss 4.7668e-01 (4.6787e-01)	Acc@1  83.37 ( 83.55)
The current update step is 17280
GPU_0_using curriculum 20 with window 20
Epoch: [576][20/30]	Time  1.628 ( 1.556)	Data  0.159 ( 0.070)	InnerLoop  0.635 ( 0.649)	Loss 4.7329e-01 (4.6710e-01)	Acc@1  82.81 ( 83.66)
The current update step is 17310
GPU_0_using curriculum 20 with window 20
Epoch: [577][20/30]	Time  1.518 ( 1.535)	Data  0.038 ( 0.057)	InnerLoop  0.640 ( 0.650)	Loss 4.5011e-01 (4.7691e-01)	Acc@1  83.45 ( 83.30)
The current update step is 17340
GPU_0_using curriculum 20 with window 20
Epoch: [578][20/30]	Time  1.492 ( 1.537)	Data  0.037 ( 0.044)	InnerLoop  0.626 ( 0.664)	Loss 4.6773e-01 (4.8522e-01)	Acc@1  84.16 ( 83.22)
The current update step is 17370
GPU_0_using curriculum 20 with window 20
Epoch: [579][20/30]	Time  1.503 ( 1.538)	Data  0.039 ( 0.069)	InnerLoop  0.638 ( 0.640)	Loss 5.3381e-01 (4.7021e-01)	Acc@1  82.03 ( 83.90)
The current update step is 17400
The current seed is 8418411547934438925
The current lr is: 0.0012
Testing Results:
 *   Acc@1 57.066
 *   Acc@1 57.443
 *   Acc@1 66.711
 *   Acc@1 67.421
 *   Acc@1 65.934
 *   Acc@1 67.013
 *   Acc@1 69.671
 *   Acc@1 70.621
 *   Acc@1 58.250
 *   Acc@1 59.227
 *   Acc@1 58.605
 *   Acc@1 59.453
Training for 300 epoch: 63.368421052631575
Training for 600 epoch: 62.48026315789474
Training for 1000 epoch: 62.26973684210527
Training for 300 epoch: 64.03208333333333
Training for 600 epoch: 63.323750000000004
Training for 1000 epoch: 63.233333333333334
[[63.368421052631575, 62.48026315789474, 62.26973684210527], [64.03208333333333, 63.323750000000004, 63.233333333333334]]
train loss 1.5143824522654215, epoch 579, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [580][20/30]	Time  1.619 ( 1.544)	Data  0.039 ( 0.069)	InnerLoop  0.755 ( 0.644)	Loss 5.0751e-01 (4.7516e-01)	Acc@1  82.28 ( 83.23)
The current update step is 17430
GPU_0_using curriculum 20 with window 20
Epoch: [581][20/30]	Time  1.520 ( 1.542)	Data  0.044 ( 0.057)	InnerLoop  0.640 ( 0.651)	Loss 4.6569e-01 (4.6639e-01)	Acc@1  84.25 ( 83.81)
The current update step is 17460
GPU_0_using curriculum 20 with window 20
Epoch: [582][20/30]	Time  1.510 ( 1.541)	Data  0.039 ( 0.045)	InnerLoop  0.640 ( 0.664)	Loss 4.2659e-01 (4.6677e-01)	Acc@1  85.01 ( 83.85)
The current update step is 17490
GPU_0_using curriculum 20 with window 20
Epoch: [583][20/30]	Time  1.514 ( 1.551)	Data  0.038 ( 0.051)	InnerLoop  0.642 ( 0.663)	Loss 4.3313e-01 (4.6108e-01)	Acc@1  85.13 ( 84.02)
The current update step is 17520
GPU_0_using curriculum 20 with window 20
Epoch: [584][20/30]	Time  1.629 ( 1.553)	Data  0.155 ( 0.069)	InnerLoop  0.639 ( 0.649)	Loss 4.5344e-01 (4.5644e-01)	Acc@1  83.81 ( 84.18)
The current update step is 17550
The current seed is 11414207427851197676
The current lr is: 0.0012
Testing Results:
 *   Acc@1 66.553
 *   Acc@1 66.937
 *   Acc@1 67.382
 *   Acc@1 67.685
 *   Acc@1 67.895
 *   Acc@1 68.036
 *   Acc@1 60.684
 *   Acc@1 61.948
 *   Acc@1 58.987
 *   Acc@1 59.447
 *   Acc@1 60.066
 *   Acc@1 60.721
Training for 300 epoch: 63.618421052631575
Training for 600 epoch: 63.184210526315795
Training for 1000 epoch: 63.98026315789474
Training for 300 epoch: 64.4425
Training for 600 epoch: 63.56625
Training for 1000 epoch: 64.37833333333333
[[63.618421052631575, 63.184210526315795, 63.98026315789474], [64.4425, 63.56625, 64.37833333333333]]
train loss 1.3543458901723227, epoch 584, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [585][20/30]	Time  1.518 ( 1.545)	Data  0.040 ( 0.050)	InnerLoop  0.648 ( 0.663)	Loss 5.6274e-01 (4.7112e-01)	Acc@1  78.86 ( 83.59)
The current update step is 17580
GPU_0_using curriculum 20 with window 20
Epoch: [586][20/30]	Time  1.644 ( 1.551)	Data  0.160 ( 0.068)	InnerLoop  0.642 ( 0.647)	Loss 5.2922e-01 (4.8331e-01)	Acc@1  82.35 ( 83.25)
The current update step is 17610
GPU_0_using curriculum 20 with window 20
Epoch: [587][20/30]	Time  1.508 ( 1.546)	Data  0.038 ( 0.057)	InnerLoop  0.640 ( 0.655)	Loss 4.7287e-01 (4.8234e-01)	Acc@1  82.76 ( 83.24)
The current update step is 17640
GPU_0_using curriculum 20 with window 20
Epoch: [588][20/30]	Time  1.517 ( 1.552)	Data  0.039 ( 0.044)	InnerLoop  0.651 ( 0.668)	Loss 4.8116e-01 (4.9125e-01)	Acc@1  83.47 ( 82.42)
The current update step is 17670
GPU_0_using curriculum 20 with window 20
Epoch: [589][20/30]	Time  1.511 ( 1.546)	Data  0.039 ( 0.070)	InnerLoop  0.641 ( 0.641)	Loss 5.1430e-01 (4.6646e-01)	Acc@1  81.88 ( 83.87)
The current update step is 17700
The current seed is 8704686586555802251
The current lr is: 0.0012
Testing Results:
 *   Acc@1 67.461
 *   Acc@1 67.486
 *   Acc@1 72.882
 *   Acc@1 72.582
 *   Acc@1 73.053
 *   Acc@1 72.961
 *   Acc@1 65.145
 *   Acc@1 65.680
 *   Acc@1 63.842
 *   Acc@1 64.009
 *   Acc@1 65.855
 *   Acc@1 66.041
Training for 300 epoch: 66.30263157894737
Training for 600 epoch: 68.36184210526316
Training for 1000 epoch: 69.45394736842105
Training for 300 epoch: 66.58291666666668
Training for 600 epoch: 68.29541666666667
Training for 1000 epoch: 69.50083333333333
[[66.30263157894737, 68.36184210526316, 69.45394736842105], [66.58291666666668, 68.29541666666667, 69.50083333333333]]
train loss 1.0870128377278645, epoch 589, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [590][20/30]	Time  1.669 ( 1.581)	Data  0.038 ( 0.072)	InnerLoop  0.791 ( 0.662)	Loss 4.1583e-01 (4.3729e-01)	Acc@1  85.30 ( 84.69)
The current update step is 17730
GPU_0_using curriculum 20 with window 20
Epoch: [591][20/30]	Time  1.546 ( 1.579)	Data  0.042 ( 0.060)	InnerLoop  0.657 ( 0.668)	Loss 4.0462e-01 (4.4903e-01)	Acc@1  85.82 ( 84.43)
The current update step is 17760
GPU_0_using curriculum 20 with window 20
Epoch: [592][20/30]	Time  1.539 ( 1.569)	Data  0.039 ( 0.046)	InnerLoop  0.650 ( 0.678)	Loss 4.2362e-01 (4.6270e-01)	Acc@1  85.94 ( 83.54)
The current update step is 17790
GPU_0_using curriculum 20 with window 20
Epoch: [593][20/30]	Time  1.532 ( 1.571)	Data  0.040 ( 0.053)	InnerLoop  0.653 ( 0.670)	Loss 4.3986e-01 (4.6879e-01)	Acc@1  84.99 ( 83.84)
The current update step is 17820
GPU_0_using curriculum 20 with window 20
Epoch: [594][20/30]	Time  1.705 ( 1.569)	Data  0.158 ( 0.072)	InnerLoop  0.705 ( 0.656)	Loss 4.3859e-01 (4.6321e-01)	Acc@1  84.30 ( 83.79)
The current update step is 17850
The current seed is 13642349319192213964
The current lr is: 0.0012
Testing Results:
 *   Acc@1 56.171
 *   Acc@1 56.612
 *   Acc@1 55.579
 *   Acc@1 55.925
 *   Acc@1 54.105
 *   Acc@1 55.133
 *   Acc@1 51.868
 *   Acc@1 51.806
 *   Acc@1 58.513
 *   Acc@1 58.590
 *   Acc@1 53.553
 *   Acc@1 53.843
Training for 300 epoch: 54.01973684210526
Training for 600 epoch: 57.046052631578945
Training for 1000 epoch: 53.828947368421055
Training for 300 epoch: 54.208749999999995
Training for 600 epoch: 57.2575
Training for 1000 epoch: 54.48791666666666
[[54.01973684210526, 57.046052631578945, 53.828947368421055], [54.208749999999995, 57.2575, 54.48791666666666]]
train loss 1.9033361651102703, epoch 594, best loss 0.5412657897313435, best_epoch 539
GPU_0_using curriculum 20 with window 20
Epoch: [595][20/30]	Time  1.554 ( 1.556)	Data  0.037 ( 0.051)	InnerLoop  0.653 ( 0.666)	Loss 4.6639e-01 (4.7353e-01)	Acc@1  83.25 ( 83.56)
The current update step is 17880
GPU_0_using curriculum 20 with window 20
Epoch: [596][20/30]	Time  1.629 ( 1.552)	Data  0.157 ( 0.071)	InnerLoop  0.640 ( 0.649)	Loss 4.4858e-01 (4.6283e-01)	Acc@1  84.77 ( 83.94)
The current update step is 17910
GPU_0_using curriculum 20 with window 20
Epoch: [597][20/30]	Time  1.525 ( 1.546)	Data  0.040 ( 0.057)	InnerLoop  0.644 ( 0.653)	Loss 4.7925e-01 (4.6251e-01)	Acc@1  83.91 ( 83.82)
The current update step is 17940
GPU_0_using curriculum 20 with window 20
Epoch: [598][20/30]	Time  1.512 ( 1.543)	Data  0.039 ( 0.044)	InnerLoop  0.631 ( 0.665)	Loss 4.3400e-01 (4.5860e-01)	Acc@1  84.50 ( 83.97)
The current update step is 17970
GPU_0_using curriculum 20 with window 20
Epoch: [599][20/30]	Time  1.527 ( 1.539)	Data  0.039 ( 0.069)	InnerLoop  0.643 ( 0.636)	Loss 4.3293e-01 (4.5867e-01)	Acc@1  85.55 ( 84.03)
The current update step is 18000
The current seed is 657500763373903978
The current lr is: 0.0012
Testing Results:
 *   Acc@1 41.763
 *   Acc@1 41.300
 *   Acc@1 43.421
 *   Acc@1 42.846
 *   Acc@1 44.724
 *   Acc@1 44.562
 *   Acc@1 64.842
 *   Acc@1 64.689
 *   Acc@1 55.158
 *   Acc@1 55.849
 *   Acc@1 67.526
 *   Acc@1 67.463
Training for 300 epoch: 53.30263157894737
Training for 600 epoch: 49.28947368421052
Training for 1000 epoch: 56.125
Training for 300 epoch: 52.99458333333333
Training for 600 epoch: 49.3475
Training for 1000 epoch: 56.01291666666667
[[53.30263157894737, 49.28947368421052, 56.125], [52.99458333333333, 49.3475, 56.01291666666667]]
train loss 0.9081433684984843, epoch 599, best loss 0.5412657897313435, best_epoch 599
=== Final results:
{'acc': 77.80263157894737, 'test': [77.80263157894737, 77.72368421052632, 77.53947368421052], 'train': [77.80263157894737, 77.72368421052632, 77.53947368421052], 'ind': 0, 'epoch': 405, 'data': array([[-0.10957529, -0.02588302,  0.01915899, ...,  0.11926731,
         0.04126216, -0.02963255],
       [-0.11386707,  0.04010703, -0.00527214, ..., -0.17789215,
        -0.02428762, -0.01067814],
       [-0.0107055 ,  0.05727176, -0.0024039 , ..., -0.00928462,
         0.08876618,  0.03398048],
       ...,
       [ 0.08362001,  0.02212837,  0.03580522, ...,  0.06135035,
        -0.07082989, -0.07173821],
       [-0.10318656,  0.07884981, -0.07325279, ...,  0.01226638,
        -0.07904759, -0.03279303],
       [ 0.09176227,  0.13870785,  0.08885272, ...,  0.09215812,
         0.02260846,  0.16304806]], shape=(200, 768), dtype=float32)}
