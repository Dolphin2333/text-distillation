Torch Seed Specified with rank: 0
Dataset: agnews_emb
Dataset Path: ./scripts
Namespace(seed=0, mp_distributed=False, world_size=1, rank=0, dist_url='tcp://224.66.41.62:23456', dist_backend='nccl', workers=0, gpu=None, root='./scripts', dataset='agnews_emb', arch='text_transformer', width=256, lr=0.001, inner_optim='Adam', outer_optim='Adam', inner_lr=0.001, label_lr_scale=1, num_per_class=5, batch_per_class=5, task_sampler_nc=4, window=20, minwindow=0, totwindow=20, num_train_eval=4, train_y=False, batch_size=2048, eps=1e-08, wd=0, test_freq=5, print_freq=20, start_epoch=0, epochs=100, ddtype='standard', cctype=0, zca=False, wandb=False, clip_coef=0.9, fname='out_step5_agnews_tf_ipc5_s0', name='agnews_step5_s0_tf', comp_aug=False, comp_aug_real=False, syn_strategy='none', real_strategy='none', ckptname='none', limit_train=False, load_ckpt=False, complete_random=False, boost_dd=False, boost_init_from='none', boost_beta=1.0, stage=0, distributed=False, data_root='./scripts/agnews_emb')
==> Preparing data..
None None
Dataset: number of classes: 4
Training set size: 120000
Image size: channel 1, height 768, width 1
Synthetic images, not_single False, keys []
==> Building model..
Initialized data with size, x: torch.Size([20, 768]), y:torch.Size([20])
TextTransformer(
  (input_proj): Linear(in_features=192, out_features=256, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls_head): Linear(in_features=256, out_features=4, bias=True)
)
use data parallel only
GPU_0_using curriculum 20 with window 20
Epoch: [0][20/59]	Time  0.793 ( 0.912)	Data  0.019 ( 0.023)	Loss 1.9026e+00 (2.1946e+00)	Acc@1  45.70 ( 36.00)
Epoch: [0][40/59]	Time  0.810 ( 0.860)	Data  0.019 ( 0.024)	Loss 2.5432e+00 (1.8975e+00)	Acc@1  31.35 ( 41.54)
The current update step is 59
GPU_0_using curriculum 20 with window 20
Epoch: [1][20/59]	Time  0.810 ( 0.794)	Data  0.019 ( 0.019)	Loss 1.0096e+00 (1.1153e+00)	Acc@1  62.74 ( 59.65)
Epoch: [1][40/59]	Time  0.790 ( 0.798)	Data  0.019 ( 0.022)	Loss 8.2243e-01 (1.0177e+00)	Acc@1  71.34 ( 62.11)
The current update step is 118
GPU_0_using curriculum 20 with window 20
Epoch: [2][20/59]	Time  0.780 ( 0.799)	Data  0.018 ( 0.032)	Loss 9.4694e-01 (9.8250e-01)	Acc@1  58.40 ( 63.53)
Epoch: [2][40/59]	Time  0.784 ( 0.798)	Data  0.018 ( 0.028)	Loss 1.0646e+00 (9.6593e-01)	Acc@1  57.37 ( 64.15)
The current update step is 177
GPU_0_using curriculum 20 with window 20
Epoch: [3][20/59]	Time  0.782 ( 0.789)	Data  0.019 ( 0.025)	Loss 1.2033e+00 (8.8176e-01)	Acc@1  53.37 ( 66.96)
Epoch: [3][40/59]	Time  0.785 ( 0.794)	Data  0.018 ( 0.022)	Loss 1.0105e+00 (9.5510e-01)	Acc@1  61.52 ( 64.79)
The current update step is 236
GPU_0_using curriculum 20 with window 20
Epoch: [4][20/59]	Time  0.789 ( 0.786)	Data  0.017 ( 0.019)	Loss 1.6094e+00 (1.1098e+00)	Acc@1  53.91 ( 60.96)
Epoch: [4][40/59]	Time  0.774 ( 0.789)	Data  0.018 ( 0.022)	Loss 7.3224e-01 (1.0007e+00)	Acc@1  75.44 ( 64.76)
The current update step is 295
The current seed is 12053268040790999796
The current lr is: 0.001
Testing Results:
 *   Acc@1 74.211
 *   Acc@1 74.118
 *   Acc@1 72.382
 *   Acc@1 71.980
 *   Acc@1 71.092
 *   Acc@1 71.072
 *   Acc@1 69.487
 *   Acc@1 70.037
 *   Acc@1 66.882
 *   Acc@1 66.878
 *   Acc@1 67.079
 *   Acc@1 67.103
 *   Acc@1 67.145
 *   Acc@1 67.549
 *   Acc@1 66.697
 *   Acc@1 67.194
 *   Acc@1 64.329
 *   Acc@1 64.478
 *   Acc@1 65.921
 *   Acc@1 65.620
 *   Acc@1 65.487
 *   Acc@1 65.424
 *   Acc@1 61.882
 *   Acc@1 61.852
 *   Acc@1 74.539
 *   Acc@1 73.690
 *   Acc@1 71.566
 *   Acc@1 71.840
 *   Acc@1 70.947
 *   Acc@1 70.891
 *   Acc@1 70.197
 *   Acc@1 70.336
Training for 300 epoch: 69.99013157894737
Training for 600 epoch: 69.23684210526316
Training for 1000 epoch: 68.66776315789474
Training for 3000 epoch: 67.0657894736842
Training for 300 epoch: 69.79125
Training for 600 epoch: 69.135625
Training for 1000 epoch: 68.73395833333333
Training for 3000 epoch: 67.35458333333332
[[69.99013157894737, 69.23684210526316, 68.66776315789474, 67.0657894736842], [69.79125, 69.135625, 68.73395833333333, 67.35458333333332]]
train loss 0.2166831847667694, epoch 4, best loss 0.2166831847667694, best_epoch 4
GPU_0_using curriculum 20 with window 20
Epoch: [5][20/59]	Time  0.774 ( 0.774)	Data  0.019 ( 0.025)	Loss 6.4180e-01 (9.1938e-01)	Acc@1  77.29 ( 65.46)
Epoch: [5][40/59]	Time  0.756 ( 0.772)	Data  0.017 ( 0.021)	Loss 9.6500e-01 (8.8200e-01)	Acc@1  61.72 ( 67.30)
The current update step is 354
GPU_0_using curriculum 20 with window 20
Epoch: [6][20/59]	Time  0.767 ( 0.768)	Data  0.020 ( 0.019)	Loss 9.4394e-01 (7.8361e-01)	Acc@1  61.33 ( 70.46)
Epoch: [6][40/59]	Time  0.752 ( 0.771)	Data  0.019 ( 0.022)	Loss 8.4480e-01 (8.2284e-01)	Acc@1  70.65 ( 69.61)
The current update step is 413
GPU_0_using curriculum 20 with window 20
Epoch: [7][20/59]	Time  0.761 ( 0.770)	Data  0.019 ( 0.019)	Loss 8.6452e-01 (8.7900e-01)	Acc@1  67.48 ( 66.74)
Epoch: [7][40/59]	Time  0.763 ( 0.772)	Data  0.017 ( 0.022)	Loss 6.5473e-01 (8.3326e-01)	Acc@1  76.32 ( 68.80)
The current update step is 472
GPU_0_using curriculum 20 with window 20
Epoch: [8][20/59]	Time  0.754 ( 0.774)	Data  0.018 ( 0.024)	Loss 6.2165e-01 (8.9057e-01)	Acc@1  76.86 ( 67.38)
Epoch: [8][40/59]	Time  0.884 ( 0.774)	Data  0.141 ( 0.027)	Loss 6.6673e-01 (9.1084e-01)	Acc@1  74.85 ( 66.37)
The current update step is 531
GPU_0_using curriculum 20 with window 20
Epoch: [9][20/59]	Time  0.878 ( 0.775)	Data  0.136 ( 0.036)	Loss 7.1665e-01 (8.3578e-01)	Acc@1  72.85 ( 69.30)
Epoch: [9][40/59]	Time  0.761 ( 0.771)	Data  0.017 ( 0.027)	Loss 8.8170e-01 (8.1926e-01)	Acc@1  59.38 ( 69.27)
The current update step is 590
The current seed is 17992339767418511180
The current lr is: 0.001
Testing Results:
 *   Acc@1 52.132
 *   Acc@1 52.267
 *   Acc@1 51.553
 *   Acc@1 51.452
 *   Acc@1 51.724
 *   Acc@1 51.612
 *   Acc@1 51.342
 *   Acc@1 51.040
 *   Acc@1 68.303
 *   Acc@1 67.867
 *   Acc@1 69.816
 *   Acc@1 69.353
 *   Acc@1 68.526
 *   Acc@1 68.174
 *   Acc@1 64.921
 *   Acc@1 64.266
 *   Acc@1 74.026
 *   Acc@1 74.623
 *   Acc@1 72.197
 *   Acc@1 72.427
 *   Acc@1 70.566
 *   Acc@1 71.156
 *   Acc@1 66.947
 *   Acc@1 67.021
 *   Acc@1 73.763
 *   Acc@1 73.862
 *   Acc@1 72.289
 *   Acc@1 72.297
 *   Acc@1 71.197
 *   Acc@1 71.558
 *   Acc@1 70.250
 *   Acc@1 70.602
Training for 300 epoch: 67.05592105263158
Training for 600 epoch: 66.46381578947368
Training for 1000 epoch: 65.50328947368422
Training for 3000 epoch: 63.36513157894737
Training for 300 epoch: 67.15458333333333
Training for 600 epoch: 66.38187500000001
Training for 1000 epoch: 65.62479166666667
Training for 3000 epoch: 63.232083333333335
[[67.05592105263158, 66.46381578947368, 65.50328947368422, 63.36513157894737], [67.15458333333333, 66.38187500000001, 65.62479166666667, 63.232083333333335]]
train loss 0.19796963089307149, epoch 9, best loss 0.19796963089307149, best_epoch 9
GPU_0_using curriculum 20 with window 20
Epoch: [10][20/59]	Time  0.770 ( 0.776)	Data  0.023 ( 0.030)	Loss 7.9304e-01 (8.2301e-01)	Acc@1  69.19 ( 68.36)
Epoch: [10][40/59]	Time  0.788 ( 0.778)	Data  0.018 ( 0.027)	Loss 8.0519e-01 (8.3662e-01)	Acc@1  68.85 ( 67.93)
The current update step is 649
GPU_0_using curriculum 20 with window 20
Epoch: [11][20/59]	Time  0.754 ( 0.772)	Data  0.017 ( 0.019)	Loss 6.6474e-01 (7.9702e-01)	Acc@1  74.90 ( 70.40)
Epoch: [11][40/59]	Time  0.757 ( 0.772)	Data  0.019 ( 0.019)	Loss 7.8479e-01 (7.7588e-01)	Acc@1  70.46 ( 71.23)
The current update step is 708
GPU_0_using curriculum 20 with window 20
Epoch: [12][20/59]	Time  0.755 ( 0.773)	Data  0.018 ( 0.018)	Loss 8.4623e-01 (8.7755e-01)	Acc@1  68.26 ( 67.69)
Epoch: [12][40/59]	Time  0.784 ( 0.774)	Data  0.017 ( 0.018)	Loss 1.1158e+00 (8.6195e-01)	Acc@1  59.33 ( 67.80)
The current update step is 767
GPU_0_using curriculum 20 with window 20
Epoch: [13][20/59]	Time  0.753 ( 0.771)	Data  0.018 ( 0.024)	Loss 7.3254e-01 (8.6749e-01)	Acc@1  74.46 ( 66.88)
Epoch: [13][40/59]	Time  0.765 ( 0.772)	Data  0.017 ( 0.021)	Loss 9.2303e-01 (8.3683e-01)	Acc@1  66.31 ( 67.89)
The current update step is 826
GPU_0_using curriculum 20 with window 20
Epoch: [14][20/59]	Time  0.757 ( 0.767)	Data  0.018 ( 0.018)	Loss 6.1333e-01 (8.5278e-01)	Acc@1  78.66 ( 68.55)
Epoch: [14][40/59]	Time  0.754 ( 0.771)	Data  0.019 ( 0.021)	Loss 6.9861e-01 (8.2596e-01)	Acc@1  74.56 ( 69.06)
The current update step is 885
The current seed is 8297033641499271589
The current lr is: 0.001
Testing Results:
 *   Acc@1 47.987
 *   Acc@1 48.042
 *   Acc@1 48.092
 *   Acc@1 47.910
 *   Acc@1 49.382
 *   Acc@1 49.728
 *   Acc@1 52.184
 *   Acc@1 52.604
 *   Acc@1 64.487
 *   Acc@1 64.950
 *   Acc@1 60.132
 *   Acc@1 60.307
 *   Acc@1 56.592
 *   Acc@1 56.704
 *   Acc@1 53.355
 *   Acc@1 54.125
 *   Acc@1 50.171
 *   Acc@1 50.546
 *   Acc@1 47.013
 *   Acc@1 47.523
 *   Acc@1 44.158
 *   Acc@1 44.615
 *   Acc@1 43.658
 *   Acc@1 43.903
 *   Acc@1 53.882
 *   Acc@1 53.962
 *   Acc@1 50.329
 *   Acc@1 51.088
 *   Acc@1 49.434
 *   Acc@1 49.384
 *   Acc@1 44.895
 *   Acc@1 44.885
Training for 300 epoch: 54.13157894736842
Training for 600 epoch: 51.391447368421055
Training for 1000 epoch: 49.89144736842105
Training for 3000 epoch: 48.52302631578947
Training for 300 epoch: 54.374791666666674
Training for 600 epoch: 51.706875000000004
Training for 1000 epoch: 50.10791666666667
Training for 3000 epoch: 48.879374999999996
[[54.13157894736842, 51.391447368421055, 49.89144736842105, 48.52302631578947], [54.374791666666674, 51.706875000000004, 50.10791666666667, 48.879374999999996]]
train loss 0.5365344215393066, epoch 14, best loss 0.19796963089307149, best_epoch 9
GPU_0_using curriculum 20 with window 20
Epoch: [15][20/59]	Time  0.746 ( 0.758)	Data  0.019 ( 0.030)	Loss 7.7409e-01 (7.8765e-01)	Acc@1  68.07 ( 69.60)
Epoch: [15][40/59]	Time  0.771 ( 0.760)	Data  0.018 ( 0.027)	Loss 6.6997e-01 (7.6935e-01)	Acc@1  76.12 ( 70.41)
The current update step is 944
GPU_0_using curriculum 20 with window 20
Epoch: [16][20/59]	Time  0.749 ( 0.758)	Data  0.018 ( 0.018)	Loss 1.0743e+00 (7.4538e-01)	Acc@1  61.43 ( 72.08)
Epoch: [16][40/59]	Time  0.744 ( 0.759)	Data  0.017 ( 0.018)	Loss 1.0524e+00 (7.4570e-01)	Acc@1  60.60 ( 71.94)
The current update step is 1003
GPU_0_using curriculum 20 with window 20
Epoch: [17][20/59]	Time  0.756 ( 0.759)	Data  0.018 ( 0.018)	Loss 6.0252e-01 (7.5877e-01)	Acc@1  77.54 ( 71.45)
Epoch: [17][40/59]	Time  0.749 ( 0.760)	Data  0.016 ( 0.018)	Loss 6.4042e-01 (7.8800e-01)	Acc@1  76.32 ( 70.57)
The current update step is 1062
GPU_0_using curriculum 20 with window 20
Epoch: [18][20/59]	Time  0.774 ( 0.760)	Data  0.017 ( 0.024)	Loss 6.0368e-01 (7.5326e-01)	Acc@1  79.05 ( 71.53)
Epoch: [18][40/59]	Time  0.772 ( 0.761)	Data  0.017 ( 0.021)	Loss 7.8251e-01 (7.5910e-01)	Acc@1  69.63 ( 71.16)
The current update step is 1121
GPU_0_using curriculum 20 with window 20
Epoch: [19][20/59]	Time  0.747 ( 0.747)	Data  0.018 ( 0.018)	Loss 7.0140e-01 (7.6693e-01)	Acc@1  74.61 ( 71.46)
Epoch: [19][40/59]	Time  0.736 ( 0.749)	Data  0.017 ( 0.021)	Loss 7.7484e-01 (7.5177e-01)	Acc@1  70.56 ( 72.08)
The current update step is 1180
The current seed is 11113076953880092571
The current lr is: 0.001
Testing Results:
 *   Acc@1 64.026
 *   Acc@1 64.037
 *   Acc@1 61.079
 *   Acc@1 61.490
 *   Acc@1 59.961
 *   Acc@1 60.441
 *   Acc@1 53.368
 *   Acc@1 53.484
 *   Acc@1 75.474
 *   Acc@1 75.853
 *   Acc@1 73.711
 *   Acc@1 74.165
 *   Acc@1 72.513
 *   Acc@1 73.082
 *   Acc@1 69.934
 *   Acc@1 70.167
 *   Acc@1 54.066
 *   Acc@1 54.617
 *   Acc@1 56.342
 *   Acc@1 56.685
 *   Acc@1 56.974
 *   Acc@1 57.467
 *   Acc@1 55.947
 *   Acc@1 56.529
 *   Acc@1 74.132
 *   Acc@1 74.803
 *   Acc@1 69.039
 *   Acc@1 69.250
 *   Acc@1 61.829
 *   Acc@1 61.720
 *   Acc@1 53.684
 *   Acc@1 53.875
Training for 300 epoch: 66.92434210526316
Training for 600 epoch: 65.04276315789474
Training for 1000 epoch: 62.81907894736841
Training for 3000 epoch: 58.233552631578945
Training for 300 epoch: 67.3275
Training for 600 epoch: 65.39750000000001
Training for 1000 epoch: 63.177499999999995
Training for 3000 epoch: 58.51375
[[66.92434210526316, 65.04276315789474, 62.81907894736841, 58.233552631578945], [67.3275, 65.39750000000001, 63.177499999999995, 58.51375]]
train loss 0.2989581895192464, epoch 19, best loss 0.19796963089307149, best_epoch 9
GPU_0_using curriculum 20 with window 20
Epoch: [20][20/59]	Time  0.737 ( 0.755)	Data  0.019 ( 0.030)	Loss 6.4544e-01 (7.1256e-01)	Acc@1  75.44 ( 73.70)
Epoch: [20][40/59]	Time  0.736 ( 0.753)	Data  0.018 ( 0.027)	Loss 8.4594e-01 (7.3461e-01)	Acc@1  64.70 ( 72.48)
The current update step is 1239
GPU_0_using curriculum 20 with window 20
Epoch: [21][20/59]	Time  0.738 ( 0.751)	Data  0.017 ( 0.018)	Loss 7.2871e-01 (7.1275e-01)	Acc@1  70.17 ( 72.71)
Epoch: [21][40/59]	Time  0.747 ( 0.751)	Data  0.017 ( 0.018)	Loss 7.2773e-01 (7.2441e-01)	Acc@1  70.80 ( 72.38)
The current update step is 1298
GPU_0_using curriculum 20 with window 20
Epoch: [22][20/59]	Time  0.740 ( 0.751)	Data  0.017 ( 0.018)	Loss 8.0795e-01 (7.7293e-01)	Acc@1  69.09 ( 70.81)
Epoch: [22][40/59]	Time  0.755 ( 0.750)	Data  0.017 ( 0.017)	Loss 7.7279e-01 (7.3320e-01)	Acc@1  70.95 ( 72.37)
The current update step is 1357
GPU_0_using curriculum 20 with window 20
Epoch: [23][20/59]	Time  0.737 ( 0.748)	Data  0.018 ( 0.024)	Loss 6.2913e-01 (7.5411e-01)	Acc@1  76.12 ( 71.70)
Epoch: [23][40/59]	Time  0.736 ( 0.750)	Data  0.016 ( 0.020)	Loss 6.4880e-01 (7.3678e-01)	Acc@1  75.73 ( 72.32)
The current update step is 1416
GPU_0_using curriculum 20 with window 20
Epoch: [24][20/59]	Time  0.752 ( 0.744)	Data  0.018 ( 0.018)	Loss 7.0755e-01 (7.0848e-01)	Acc@1  73.10 ( 73.64)
Epoch: [24][40/59]	Time  0.730 ( 0.750)	Data  0.017 ( 0.021)	Loss 8.7979e-01 (7.1547e-01)	Acc@1  67.97 ( 73.21)
The current update step is 1475
The current seed is 6550085231871443725
The current lr is: 0.001
Testing Results:
 *   Acc@1 76.132
 *   Acc@1 75.904
 *   Acc@1 76.355
 *   Acc@1 76.341
 *   Acc@1 75.289
 *   Acc@1 75.544
 *   Acc@1 72.013
 *   Acc@1 72.517
 *   Acc@1 78.605
 *   Acc@1 78.743
 *   Acc@1 77.645
 *   Acc@1 78.152
 *   Acc@1 76.539
 *   Acc@1 76.623
 *   Acc@1 72.342
 *   Acc@1 72.799
 *   Acc@1 73.263
 *   Acc@1 74.255
 *   Acc@1 71.737
 *   Acc@1 72.139
 *   Acc@1 70.066
 *   Acc@1 71.007
 *   Acc@1 70.605
 *   Acc@1 71.167
 *   Acc@1 63.237
 *   Acc@1 63.307
 *   Acc@1 59.474
 *   Acc@1 59.663
 *   Acc@1 57.711
 *   Acc@1 58.100
 *   Acc@1 52.276
 *   Acc@1 52.611
Training for 300 epoch: 72.8092105263158
Training for 600 epoch: 71.30263157894737
Training for 1000 epoch: 69.90131578947368
Training for 3000 epoch: 66.8092105263158
Training for 300 epoch: 73.05229166666666
Training for 600 epoch: 71.57354166666667
Training for 1000 epoch: 70.31854166666668
Training for 3000 epoch: 67.27354166666667
[[72.8092105263158, 71.30263157894737, 69.90131578947368, 66.8092105263158], [73.05229166666666, 71.57354166666667, 70.31854166666668, 67.27354166666667]]
train loss 0.41541933809916176, epoch 24, best loss 0.19796963089307149, best_epoch 9
GPU_0_using curriculum 20 with window 20
Epoch: [25][20/59]	Time  0.733 ( 0.756)	Data  0.018 ( 0.029)	Loss 5.8233e-01 (7.2412e-01)	Acc@1  77.34 ( 73.25)
Epoch: [25][40/59]	Time  0.741 ( 0.755)	Data  0.018 ( 0.027)	Loss 8.8888e-01 (7.1381e-01)	Acc@1  65.33 ( 73.29)
The current update step is 1534
GPU_0_using curriculum 20 with window 20
Epoch: [26][20/59]	Time  0.738 ( 0.754)	Data  0.017 ( 0.017)	Loss 6.3232e-01 (6.8262e-01)	Acc@1  76.90 ( 74.15)
Epoch: [26][40/59]	Time  0.738 ( 0.756)	Data  0.018 ( 0.018)	Loss 5.5472e-01 (7.1054e-01)	Acc@1  81.40 ( 73.27)
The current update step is 1593
GPU_0_using curriculum 20 with window 20
Epoch: [27][20/59]	Time  0.764 ( 0.753)	Data  0.018 ( 0.018)	Loss 5.9746e-01 (6.8312e-01)	Acc@1  78.22 ( 74.35)
Epoch: [27][40/59]	Time  0.737 ( 0.754)	Data  0.018 ( 0.018)	Loss 7.9718e-01 (7.1888e-01)	Acc@1  69.24 ( 72.73)
The current update step is 1652
GPU_0_using curriculum 20 with window 20
Epoch: [28][20/59]	Time  0.732 ( 0.750)	Data  0.018 ( 0.024)	Loss 7.9113e-01 (6.9017e-01)	Acc@1  69.73 ( 73.11)
Epoch: [28][40/59]	Time  0.733 ( 0.750)	Data  0.017 ( 0.021)	Loss 1.0139e+00 (6.8788e-01)	Acc@1  64.84 ( 73.86)
The current update step is 1711
GPU_0_using curriculum 20 with window 20
Epoch: [29][20/59]	Time  0.746 ( 0.748)	Data  0.021 ( 0.018)	Loss 1.2060e+00 (7.5468e-01)	Acc@1  65.04 ( 72.22)
Epoch: [29][40/59]	Time  0.730 ( 0.749)	Data  0.017 ( 0.021)	Loss 1.0229e+00 (7.3986e-01)	Acc@1  61.04 ( 72.63)
The current update step is 1770
The current seed is 14867878387240297720
The current lr is: 0.001
Testing Results:
 *   Acc@1 73.908
 *   Acc@1 74.196
 *   Acc@1 72.618
 *   Acc@1 73.050
 *   Acc@1 71.316
 *   Acc@1 71.768
 *   Acc@1 67.684
 *   Acc@1 68.823
 *   Acc@1 77.184
 *   Acc@1 77.430
 *   Acc@1 76.026
 *   Acc@1 76.218
 *   Acc@1 74.211
 *   Acc@1 74.788
 *   Acc@1 72.829
 *   Acc@1 73.551
 *   Acc@1 77.684
 *   Acc@1 77.944
 *   Acc@1 75.934
 *   Acc@1 76.528
 *   Acc@1 75.000
 *   Acc@1 74.960
 *   Acc@1 66.434
 *   Acc@1 67.134
 *   Acc@1 69.776
 *   Acc@1 69.896
 *   Acc@1 67.329
 *   Acc@1 67.873
 *   Acc@1 66.382
 *   Acc@1 67.004
 *   Acc@1 59.921
 *   Acc@1 60.766
Training for 300 epoch: 74.63815789473685
Training for 600 epoch: 72.97697368421052
Training for 1000 epoch: 71.72697368421053
Training for 3000 epoch: 66.7171052631579
Training for 300 epoch: 74.86645833333333
Training for 600 epoch: 73.41729166666667
Training for 1000 epoch: 72.13020833333333
Training for 3000 epoch: 67.56833333333333
[[74.63815789473685, 72.97697368421052, 71.72697368421053, 66.7171052631579], [74.86645833333333, 73.41729166666667, 72.13020833333333, 67.56833333333333]]
train loss 0.3122147953033447, epoch 29, best loss 0.19796963089307149, best_epoch 9
GPU_0_using curriculum 20 with window 20
Epoch: [30][20/59]	Time  0.739 ( 0.754)	Data  0.018 ( 0.029)	Loss 5.5857e-01 (6.8556e-01)	Acc@1  80.66 ( 74.58)
Epoch: [30][40/59]	Time  0.732 ( 0.753)	Data  0.017 ( 0.026)	Loss 5.1556e-01 (6.9572e-01)	Acc@1  81.05 ( 73.98)
The current update step is 1829
GPU_0_using curriculum 20 with window 20
Epoch: [31][20/59]	Time  0.748 ( 0.749)	Data  0.017 ( 0.017)	Loss 6.3703e-01 (6.6866e-01)	Acc@1  74.12 ( 74.78)
Epoch: [31][40/59]	Time  0.735 ( 0.748)	Data  0.017 ( 0.017)	Loss 6.3758e-01 (6.9371e-01)	Acc@1  74.32 ( 73.86)
The current update step is 1888
GPU_0_using curriculum 20 with window 20
Epoch: [32][20/59]	Time  0.730 ( 0.750)	Data  0.017 ( 0.018)	Loss 5.5309e-01 (6.6593e-01)	Acc@1  79.74 ( 75.44)
Epoch: [32][40/59]	Time  0.741 ( 0.750)	Data  0.017 ( 0.018)	Loss 7.7396e-01 (6.7644e-01)	Acc@1  69.43 ( 74.79)
The current update step is 1947
GPU_0_using curriculum 20 with window 20
Epoch: [33][20/59]	Time  0.734 ( 0.751)	Data  0.018 ( 0.024)	Loss 6.2643e-01 (6.8748e-01)	Acc@1  72.66 ( 73.83)
Epoch: [33][40/59]	Time  0.736 ( 0.754)	Data  0.017 ( 0.021)	Loss 5.9733e-01 (7.0983e-01)	Acc@1  77.49 ( 72.94)
The current update step is 2006
GPU_0_using curriculum 20 with window 20
Epoch: [34][20/59]	Time  0.738 ( 0.740)	Data  0.017 ( 0.017)	Loss 7.4176e-01 (7.6458e-01)	Acc@1  73.83 ( 70.92)
Epoch: [34][40/59]	Time  0.734 ( 0.746)	Data  0.017 ( 0.020)	Loss 4.8041e-01 (7.3864e-01)	Acc@1  83.50 ( 71.93)
The current update step is 2065
The current seed is 12606018103740262315
The current lr is: 0.001
Testing Results:
 *   Acc@1 77.868
 *   Acc@1 78.169
 *   Acc@1 75.987
 *   Acc@1 75.998
 *   Acc@1 74.500
 *   Acc@1 74.116
 *   Acc@1 71.158
 *   Acc@1 71.386
 *   Acc@1 72.145
 *   Acc@1 72.381
 *   Acc@1 72.066
 *   Acc@1 72.534
 *   Acc@1 73.105
 *   Acc@1 73.612
 *   Acc@1 74.526
 *   Acc@1 74.895
 *   Acc@1 75.197
 *   Acc@1 75.453
 *   Acc@1 72.434
 *   Acc@1 72.717
 *   Acc@1 71.237
 *   Acc@1 71.769
 *   Acc@1 70.434
 *   Acc@1 70.885
 *   Acc@1 66.868
 *   Acc@1 67.612
 *   Acc@1 63.184
 *   Acc@1 64.006
 *   Acc@1 63.000
 *   Acc@1 63.578
 *   Acc@1 61.105
 *   Acc@1 61.934
Training for 300 epoch: 73.01973684210526
Training for 600 epoch: 70.91776315789474
Training for 1000 epoch: 70.46052631578948
Training for 3000 epoch: 69.30592105263158
Training for 300 epoch: 73.40395833333334
Training for 600 epoch: 71.31354166666667
Training for 1000 epoch: 70.76895833333333
Training for 3000 epoch: 69.775
[[73.01973684210526, 70.91776315789474, 70.46052631578948, 69.30592105263158], [73.40395833333334, 71.31354166666667, 70.76895833333333, 69.775]]
train loss 0.3022529051780701, epoch 34, best loss 0.19796963089307149, best_epoch 9
GPU_0_using curriculum 20 with window 20
Epoch: [35][20/59]	Time  0.739 ( 0.757)	Data  0.017 ( 0.029)	Loss 5.6704e-01 (7.1628e-01)	Acc@1  79.44 ( 72.83)
Epoch: [35][40/59]	Time  0.736 ( 0.754)	Data  0.018 ( 0.026)	Loss 6.7345e-01 (7.0825e-01)	Acc@1  75.44 ( 73.23)
The current update step is 2124
GPU_0_using curriculum 20 with window 20
Epoch: [36][20/59]	Time  0.740 ( 0.750)	Data  0.018 ( 0.018)	Loss 5.7642e-01 (6.5576e-01)	Acc@1  80.37 ( 75.78)
Epoch: [36][40/59]	Time  0.737 ( 0.752)	Data  0.018 ( 0.018)	Loss 1.1198e+00 (6.8684e-01)	Acc@1  64.84 ( 74.61)
The current update step is 2183
GPU_0_using curriculum 20 with window 20
Epoch: [37][20/59]	Time  0.735 ( 0.747)	Data  0.018 ( 0.017)	Loss 6.2015e-01 (7.2349e-01)	Acc@1  74.66 ( 72.76)
Epoch: [37][40/59]	Time  0.732 ( 0.750)	Data  0.017 ( 0.018)	Loss 7.8192e-01 (7.7928e-01)	Acc@1  70.90 ( 70.97)
The current update step is 2242
GPU_0_using curriculum 20 with window 20
Epoch: [38][20/59]	Time  0.740 ( 0.749)	Data  0.018 ( 0.024)	Loss 7.7085e-01 (7.1379e-01)	Acc@1  69.82 ( 73.09)
Epoch: [38][40/59]	Time  0.746 ( 0.749)	Data  0.016 ( 0.021)	Loss 7.7941e-01 (7.1619e-01)	Acc@1  70.56 ( 73.24)
The current update step is 2301
GPU_0_using curriculum 20 with window 20
Epoch: [39][20/59]	Time  0.746 ( 0.742)	Data  0.018 ( 0.017)	Loss 6.9237e-01 (7.2149e-01)	Acc@1  75.20 ( 72.88)
Epoch: [39][40/59]	Time  0.734 ( 0.747)	Data  0.017 ( 0.020)	Loss 5.6597e-01 (7.1643e-01)	Acc@1  79.49 ( 73.20)
The current update step is 2360
The current seed is 9184641146216238956
The current lr is: 0.001
Testing Results:
 *   Acc@1 62.263
 *   Acc@1 62.542
 *   Acc@1 52.461
 *   Acc@1 53.367
 *   Acc@1 49.789
 *   Acc@1 50.167
 *   Acc@1 47.816
 *   Acc@1 48.167
 *   Acc@1 70.987
 *   Acc@1 71.071
 *   Acc@1 71.276
 *   Acc@1 71.881
 *   Acc@1 71.987
 *   Acc@1 72.172
 *   Acc@1 70.895
 *   Acc@1 70.927
 *   Acc@1 67.000
 *   Acc@1 67.079
 *   Acc@1 67.711
 *   Acc@1 67.838
 *   Acc@1 68.171
 *   Acc@1 68.473
 *   Acc@1 69.026
 *   Acc@1 69.389
 *   Acc@1 47.276
 *   Acc@1 47.629
 *   Acc@1 47.842
 *   Acc@1 47.731
 *   Acc@1 48.684
 *   Acc@1 48.831
 *   Acc@1 46.829
 *   Acc@1 46.982
Training for 300 epoch: 61.881578947368425
Training for 600 epoch: 59.82236842105263
Training for 1000 epoch: 59.65789473684211
Training for 3000 epoch: 58.641447368421055
Training for 300 epoch: 62.08020833333333
Training for 600 epoch: 60.20395833333333
Training for 1000 epoch: 59.91104166666666
Training for 3000 epoch: 58.86625000000001
[[61.881578947368425, 59.82236842105263, 59.65789473684211, 58.641447368421055], [62.08020833333333, 60.20395833333333, 59.91104166666666, 58.86625000000001]]
train loss 0.34928017886479695, epoch 39, best loss 0.19796963089307149, best_epoch 9
GPU_0_using curriculum 20 with window 20
Epoch: [40][20/59]	Time  0.734 ( 0.747)	Data  0.017 ( 0.028)	Loss 5.4882e-01 (7.0847e-01)	Acc@1  79.39 ( 73.72)
Epoch: [40][40/59]	Time  0.763 ( 0.749)	Data  0.018 ( 0.026)	Loss 6.2935e-01 (7.0656e-01)	Acc@1  77.78 ( 73.78)
The current update step is 2419
GPU_0_using curriculum 20 with window 20
Epoch: [41][20/59]	Time  0.736 ( 0.749)	Data  0.017 ( 0.018)	Loss 6.8139e-01 (6.5581e-01)	Acc@1  75.34 ( 75.40)
Epoch: [41][40/59]	Time  0.730 ( 0.751)	Data  0.017 ( 0.017)	Loss 1.3560e+00 (6.7399e-01)	Acc@1  48.44 ( 74.90)
The current update step is 2478
GPU_0_using curriculum 20 with window 20
Epoch: [42][20/59]	Time  0.738 ( 0.756)	Data  0.018 ( 0.019)	Loss 8.3616e-01 (6.8858e-01)	Acc@1  66.41 ( 73.94)
Epoch: [42][40/59]	Time  0.753 ( 0.755)	Data  0.017 ( 0.018)	Loss 6.8184e-01 (6.7532e-01)	Acc@1  75.63 ( 74.72)
The current update step is 2537
GPU_0_using curriculum 20 with window 20
Epoch: [43][20/59]	Time  0.733 ( 0.751)	Data  0.018 ( 0.024)	Loss 6.0657e-01 (7.0925e-01)	Acc@1  78.42 ( 73.57)
Epoch: [43][40/59]	Time  0.735 ( 0.752)	Data  0.018 ( 0.021)	Loss 5.8122e-01 (7.1520e-01)	Acc@1  79.10 ( 72.71)
The current update step is 2596
GPU_0_using curriculum 20 with window 20
Epoch: [44][20/59]	Time  0.740 ( 0.743)	Data  0.017 ( 0.017)	Loss 7.9522e-01 (7.2101e-01)	Acc@1  71.58 ( 72.97)
Epoch: [44][40/59]	Time  0.740 ( 0.746)	Data  0.018 ( 0.020)	Loss 8.2064e-01 (7.0957e-01)	Acc@1  64.99 ( 73.44)
The current update step is 2655
The current seed is 6720117622585630023
The current lr is: 0.001
Testing Results:
 *   Acc@1 73.658
 *   Acc@1 73.547
 *   Acc@1 74.092
 *   Acc@1 73.911
 *   Acc@1 73.539
 *   Acc@1 73.962
 *   Acc@1 73.461
 *   Acc@1 73.842
 *   Acc@1 66.000
 *   Acc@1 66.047
 *   Acc@1 65.105
 *   Acc@1 65.189
 *   Acc@1 65.474
 *   Acc@1 65.627
 *   Acc@1 62.421
 *   Acc@1 62.865
 *   Acc@1 71.461
 *   Acc@1 71.661
 *   Acc@1 72.447
 *   Acc@1 72.703
 *   Acc@1 72.961
 *   Acc@1 73.225
 *   Acc@1 73.763
 *   Acc@1 73.626
 *   Acc@1 77.066
 *   Acc@1 77.508
 *   Acc@1 78.368
 *   Acc@1 78.154
 *   Acc@1 77.868
 *   Acc@1 77.989
 *   Acc@1 76.211
 *   Acc@1 76.261
Training for 300 epoch: 72.04605263157895
Training for 600 epoch: 72.5032894736842
Training for 1000 epoch: 72.46052631578947
Training for 3000 epoch: 71.46381578947368
Training for 300 epoch: 72.19104166666666
Training for 600 epoch: 72.489375
Training for 1000 epoch: 72.700625
Training for 3000 epoch: 71.64833333333334
[[72.04605263157895, 72.5032894736842, 72.46052631578947, 71.46381578947368], [72.19104166666666, 72.489375, 72.700625, 71.64833333333334]]
train loss 0.1955831355889638, epoch 44, best loss 0.1955831355889638, best_epoch 44
GPU_0_using curriculum 20 with window 20
Epoch: [45][20/59]	Time  0.735 ( 0.748)	Data  0.017 ( 0.029)	Loss 1.0328e+00 (6.8257e-01)	Acc@1  64.16 ( 75.21)
Epoch: [45][40/59]	Time  0.743 ( 0.750)	Data  0.018 ( 0.026)	Loss 6.4957e-01 (6.7420e-01)	Acc@1  74.27 ( 75.02)
The current update step is 2714
GPU_0_using curriculum 20 with window 20
Epoch: [46][20/59]	Time  0.732 ( 0.751)	Data  0.017 ( 0.018)	Loss 7.3994e-01 (6.8007e-01)	Acc@1  73.49 ( 74.90)
Epoch: [46][40/59]	Time  0.733 ( 0.751)	Data  0.017 ( 0.018)	Loss 7.7345e-01 (6.6683e-01)	Acc@1  70.75 ( 75.18)
The current update step is 2773
GPU_0_using curriculum 20 with window 20
Epoch: [47][20/59]	Time  0.746 ( 0.749)	Data  0.020 ( 0.018)	Loss 7.7799e-01 (7.2755e-01)	Acc@1  67.58 ( 72.72)
Epoch: [47][40/59]	Time  0.732 ( 0.749)	Data  0.016 ( 0.018)	Loss 7.9942e-01 (6.8998e-01)	Acc@1  68.12 ( 74.23)
The current update step is 2832
GPU_0_using curriculum 20 with window 20
Epoch: [48][20/59]	Time  0.736 ( 0.752)	Data  0.018 ( 0.024)	Loss 6.2075e-01 (6.4277e-01)	Acc@1  74.95 ( 75.47)
Epoch: [48][40/59]	Time  0.735 ( 0.751)	Data  0.017 ( 0.021)	Loss 5.2237e-01 (6.5454e-01)	Acc@1  81.69 ( 75.24)
The current update step is 2891
GPU_0_using curriculum 20 with window 20
Epoch: [49][20/59]	Time  0.732 ( 0.746)	Data  0.017 ( 0.018)	Loss 5.7959e-01 (6.6246e-01)	Acc@1  80.08 ( 75.49)
Epoch: [49][40/59]	Time  0.727 ( 0.748)	Data  0.017 ( 0.021)	Loss 5.6029e-01 (6.3895e-01)	Acc@1  79.25 ( 76.44)
The current update step is 2950
The current seed is 6930607604543387372
The current lr is: 0.001
Testing Results:
 *   Acc@1 78.487
 *   Acc@1 78.878
 *   Acc@1 78.250
 *   Acc@1 78.547
 *   Acc@1 78.079
 *   Acc@1 78.198
 *   Acc@1 77.461
 *   Acc@1 77.612
 *   Acc@1 76.789
 *   Acc@1 77.276
 *   Acc@1 76.039
 *   Acc@1 76.384
 *   Acc@1 76.908
 *   Acc@1 77.096
 *   Acc@1 78.408
 *   Acc@1 78.700
 *   Acc@1 71.934
 *   Acc@1 72.780
 *   Acc@1 72.539
 *   Acc@1 72.834
 *   Acc@1 71.934
 *   Acc@1 72.012
 *   Acc@1 68.776
 *   Acc@1 69.440
 *   Acc@1 58.947
 *   Acc@1 59.225
 *   Acc@1 61.395
 *   Acc@1 61.744
 *   Acc@1 62.395
 *   Acc@1 62.808
 *   Acc@1 64.184
 *   Acc@1 64.491
Training for 300 epoch: 71.53947368421052
Training for 600 epoch: 72.05592105263158
Training for 1000 epoch: 72.32894736842105
Training for 3000 epoch: 72.20723684210526
Training for 300 epoch: 72.03979166666667
Training for 600 epoch: 72.3775
Training for 1000 epoch: 72.52854166666667
Training for 3000 epoch: 72.560625
[[71.53947368421052, 72.05592105263158, 72.32894736842105, 72.20723684210526], [72.03979166666667, 72.3775, 72.52854166666667, 72.560625]]
train loss 0.24718435114224752, epoch 49, best loss 0.1955831355889638, best_epoch 44
GPU_0_using curriculum 20 with window 20
Epoch: [50][20/59]	Time  0.733 ( 0.759)	Data  0.018 ( 0.029)	Loss 6.1385e-01 (6.3342e-01)	Acc@1  77.20 ( 77.03)
Epoch: [50][40/59]	Time  0.735 ( 0.758)	Data  0.017 ( 0.026)	Loss 5.4443e-01 (6.1467e-01)	Acc@1  81.45 ( 77.51)
The current update step is 3009
GPU_0_using curriculum 20 with window 20
Epoch: [51][20/59]	Time  0.735 ( 0.753)	Data  0.017 ( 0.017)	Loss 5.6373e-01 (6.0648e-01)	Acc@1  80.81 ( 77.71)
Epoch: [51][40/59]	Time  0.738 ( 0.752)	Data  0.017 ( 0.017)	Loss 6.0186e-01 (6.1064e-01)	Acc@1  78.56 ( 77.61)
The current update step is 3068
GPU_0_using curriculum 20 with window 20
Epoch: [52][20/59]	Time  0.745 ( 0.751)	Data  0.017 ( 0.017)	Loss 7.4722e-01 (6.3656e-01)	Acc@1  69.68 ( 76.68)
Epoch: [52][40/59]	Time  0.736 ( 0.753)	Data  0.016 ( 0.017)	Loss 7.0801e-01 (6.5497e-01)	Acc@1  72.95 ( 75.63)
The current update step is 3127
GPU_0_using curriculum 20 with window 20
Epoch: [53][20/59]	Time  0.745 ( 0.752)	Data  0.018 ( 0.024)	Loss 6.2378e-01 (6.0305e-01)	Acc@1  77.15 ( 77.80)
Epoch: [53][40/59]	Time  0.740 ( 0.753)	Data  0.017 ( 0.021)	Loss 6.3076e-01 (6.2127e-01)	Acc@1  75.59 ( 77.04)
The current update step is 3186
GPU_0_using curriculum 20 with window 20
Epoch: [54][20/59]	Time  0.728 ( 0.747)	Data  0.017 ( 0.017)	Loss 5.8315e-01 (5.9153e-01)	Acc@1  78.91 ( 78.25)
Epoch: [54][40/59]	Time  0.753 ( 0.751)	Data  0.018 ( 0.020)	Loss 5.9142e-01 (6.3246e-01)	Acc@1  78.81 ( 76.93)
The current update step is 3245
The current seed is 8049836135137281639
The current lr is: 0.001
Testing Results:
 *   Acc@1 78.671
 *   Acc@1 79.104
 *   Acc@1 78.947
 *   Acc@1 79.107
 *   Acc@1 78.211
 *   Acc@1 78.393
 *   Acc@1 77.026
 *   Acc@1 77.475
 *   Acc@1 74.461
 *   Acc@1 74.801
 *   Acc@1 71.671
 *   Acc@1 71.901
 *   Acc@1 70.882
 *   Acc@1 71.011
 *   Acc@1 69.579
 *   Acc@1 69.712
 *   Acc@1 64.000
 *   Acc@1 64.308
 *   Acc@1 64.237
 *   Acc@1 64.309
 *   Acc@1 62.803
 *   Acc@1 63.147
 *   Acc@1 62.237
 *   Acc@1 62.572
 *   Acc@1 68.671
 *   Acc@1 68.635
 *   Acc@1 63.447
 *   Acc@1 63.633
 *   Acc@1 62.526
 *   Acc@1 62.783
 *   Acc@1 60.974
 *   Acc@1 61.222
Training for 300 epoch: 71.45065789473685
Training for 600 epoch: 69.57565789473685
Training for 1000 epoch: 68.60526315789474
Training for 3000 epoch: 67.45394736842105
Training for 300 epoch: 71.711875
Training for 600 epoch: 69.73729166666666
Training for 1000 epoch: 68.83354166666666
Training for 3000 epoch: 67.745
[[71.45065789473685, 69.57565789473685, 68.60526315789474, 67.45394736842105], [71.711875, 69.73729166666666, 68.83354166666666, 67.745]]
train loss 0.29116330822308856, epoch 54, best loss 0.1955831355889638, best_epoch 44
GPU_0_using curriculum 20 with window 20
Epoch: [55][20/59]	Time  0.733 ( 0.751)	Data  0.019 ( 0.029)	Loss 5.2370e-01 (6.4512e-01)	Acc@1  81.59 ( 76.26)
Epoch: [55][40/59]	Time  0.743 ( 0.751)	Data  0.018 ( 0.026)	Loss 5.2300e-01 (6.4478e-01)	Acc@1  81.25 ( 76.25)
The current update step is 3304
GPU_0_using curriculum 20 with window 20
Epoch: [56][20/59]	Time  0.737 ( 0.752)	Data  0.018 ( 0.017)	Loss 5.7640e-01 (6.1260e-01)	Acc@1  79.30 ( 77.34)
Epoch: [56][40/59]	Time  0.744 ( 0.751)	Data  0.017 ( 0.017)	Loss 6.1560e-01 (6.4104e-01)	Acc@1  77.59 ( 76.35)
The current update step is 3363
GPU_0_using curriculum 20 with window 20
Epoch: [57][20/59]	Time  0.729 ( 0.754)	Data  0.018 ( 0.018)	Loss 5.5023e-01 (6.4236e-01)	Acc@1  80.32 ( 76.18)
Epoch: [57][40/59]	Time  0.745 ( 0.751)	Data  0.016 ( 0.017)	Loss 7.1913e-01 (6.3908e-01)	Acc@1  73.49 ( 76.61)
The current update step is 3422
GPU_0_using curriculum 20 with window 20
Epoch: [58][20/59]	Time  0.740 ( 0.753)	Data  0.018 ( 0.023)	Loss 6.7558e-01 (6.8754e-01)	Acc@1  73.88 ( 75.14)
Epoch: [58][40/59]	Time  0.756 ( 0.752)	Data  0.017 ( 0.020)	Loss 7.5947e-01 (6.7811e-01)	Acc@1  70.12 ( 75.23)
The current update step is 3481
GPU_0_using curriculum 20 with window 20
Epoch: [59][20/59]	Time  0.731 ( 0.746)	Data  0.018 ( 0.018)	Loss 1.0625e+00 (6.2823e-01)	Acc@1  60.89 ( 76.43)
Epoch: [59][40/59]	Time  0.730 ( 0.750)	Data  0.017 ( 0.021)	Loss 5.5177e-01 (6.2315e-01)	Acc@1  79.64 ( 76.82)
The current update step is 3540
The current seed is 1157195114083397971
The current lr is: 0.001
Testing Results:
 *   Acc@1 76.645
 *   Acc@1 76.486
 *   Acc@1 73.895
 *   Acc@1 74.039
 *   Acc@1 71.934
 *   Acc@1 72.075
 *   Acc@1 70.632
 *   Acc@1 70.457
 *   Acc@1 65.645
 *   Acc@1 65.439
 *   Acc@1 66.697
 *   Acc@1 66.795
 *   Acc@1 68.118
 *   Acc@1 67.737
 *   Acc@1 68.539
 *   Acc@1 68.448
 *   Acc@1 62.197
 *   Acc@1 63.319
 *   Acc@1 61.211
 *   Acc@1 62.164
 *   Acc@1 61.605
 *   Acc@1 61.737
 *   Acc@1 60.171
 *   Acc@1 60.445
 *   Acc@1 73.145
 *   Acc@1 72.862
 *   Acc@1 72.368
 *   Acc@1 72.221
 *   Acc@1 71.829
 *   Acc@1 71.697
 *   Acc@1 70.658
 *   Acc@1 70.877
Training for 300 epoch: 69.40789473684211
Training for 600 epoch: 68.54276315789474
Training for 1000 epoch: 68.3717105263158
Training for 3000 epoch: 67.5
Training for 300 epoch: 69.52645833333334
Training for 600 epoch: 68.80479166666666
Training for 1000 epoch: 68.31145833333333
Training for 3000 epoch: 67.55645833333332
[[69.40789473684211, 68.54276315789474, 68.3717105263158, 67.5], [69.52645833333334, 68.80479166666666, 68.31145833333333, 67.55645833333332]]
train loss 0.26655597523053487, epoch 59, best loss 0.1955831355889638, best_epoch 44
GPU_0_using curriculum 20 with window 20
Epoch: [60][20/59]	Time  0.737 ( 0.750)	Data  0.018 ( 0.028)	Loss 6.7918e-01 (5.8421e-01)	Acc@1  72.85 ( 78.51)
Epoch: [60][40/59]	Time  0.731 ( 0.751)	Data  0.017 ( 0.026)	Loss 5.7184e-01 (5.9596e-01)	Acc@1  78.96 ( 78.14)
The current update step is 3599
GPU_0_using curriculum 20 with window 20
Epoch: [61][20/59]	Time  0.729 ( 0.752)	Data  0.018 ( 0.018)	Loss 8.7814e-01 (6.2557e-01)	Acc@1  64.36 ( 77.06)
Epoch: [61][40/59]	Time  0.741 ( 0.752)	Data  0.017 ( 0.018)	Loss 6.0232e-01 (6.1809e-01)	Acc@1  77.69 ( 77.42)
The current update step is 3658
GPU_0_using curriculum 20 with window 20
Epoch: [62][20/59]	Time  0.734 ( 0.748)	Data  0.017 ( 0.018)	Loss 4.9899e-01 (6.2857e-01)	Acc@1  82.47 ( 77.27)
Epoch: [62][40/59]	Time  0.748 ( 0.753)	Data  0.017 ( 0.018)	Loss 5.0339e-01 (5.9857e-01)	Acc@1  82.86 ( 78.21)
The current update step is 3717
GPU_0_using curriculum 20 with window 20
Epoch: [63][20/59]	Time  0.768 ( 0.755)	Data  0.017 ( 0.023)	Loss 5.1004e-01 (6.0540e-01)	Acc@1  82.23 ( 77.89)
Epoch: [63][40/59]	Time  0.776 ( 0.755)	Data  0.017 ( 0.021)	Loss 8.3050e-01 (6.5009e-01)	Acc@1  71.97 ( 75.92)
The current update step is 3776
GPU_0_using curriculum 20 with window 20
Epoch: [64][20/59]	Time  0.731 ( 0.747)	Data  0.017 ( 0.018)	Loss 5.4922e-01 (6.2495e-01)	Acc@1  81.74 ( 77.29)
Epoch: [64][40/59]	Time  0.749 ( 0.750)	Data  0.022 ( 0.021)	Loss 5.5735e-01 (6.1505e-01)	Acc@1  79.69 ( 77.53)
The current update step is 3835
The current seed is 17403890022999698124
The current lr is: 0.001
Testing Results:
 *   Acc@1 66.553
 *   Acc@1 66.821
 *   Acc@1 68.276
 *   Acc@1 68.429
 *   Acc@1 72.118
 *   Acc@1 72.745
 *   Acc@1 73.197
 *   Acc@1 73.668
 *   Acc@1 70.211
 *   Acc@1 70.566
 *   Acc@1 68.513
 *   Acc@1 69.093
 *   Acc@1 68.987
 *   Acc@1 69.588
 *   Acc@1 68.303
 *   Acc@1 68.784
 *   Acc@1 70.974
 *   Acc@1 71.305
 *   Acc@1 70.987
 *   Acc@1 71.438
 *   Acc@1 70.592
 *   Acc@1 71.245
 *   Acc@1 69.013
 *   Acc@1 69.029
 *   Acc@1 80.855
 *   Acc@1 81.157
 *   Acc@1 80.132
 *   Acc@1 80.564
 *   Acc@1 79.750
 *   Acc@1 79.875
 *   Acc@1 76.908
 *   Acc@1 78.111
Training for 300 epoch: 72.14802631578948
Training for 600 epoch: 71.97697368421053
Training for 1000 epoch: 72.86184210526315
Training for 3000 epoch: 71.85526315789474
Training for 300 epoch: 72.46229166666666
Training for 600 epoch: 72.38104166666666
Training for 1000 epoch: 73.36333333333334
Training for 3000 epoch: 72.39791666666666
[[72.14802631578948, 71.97697368421053, 72.86184210526315, 71.85526315789474], [72.46229166666666, 72.38104166666666, 73.36333333333334, 72.39791666666666]]
train loss 0.17140410820643107, epoch 64, best loss 0.17140410820643107, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [65][20/59]	Time  0.745 ( 0.750)	Data  0.017 ( 0.029)	Loss 6.9240e-01 (6.0313e-01)	Acc@1  72.31 ( 77.25)
Epoch: [65][40/59]	Time  0.728 ( 0.751)	Data  0.018 ( 0.026)	Loss 5.5859e-01 (6.1952e-01)	Acc@1  79.39 ( 77.08)
The current update step is 3894
GPU_0_using curriculum 20 with window 20
Epoch: [66][20/59]	Time  0.752 ( 0.755)	Data  0.018 ( 0.018)	Loss 6.2860e-01 (6.2129e-01)	Acc@1  75.44 ( 77.16)
Epoch: [66][40/59]	Time  0.737 ( 0.754)	Data  0.017 ( 0.018)	Loss 6.2243e-01 (6.2464e-01)	Acc@1  76.76 ( 76.77)
The current update step is 3953
GPU_0_using curriculum 20 with window 20
Epoch: [67][20/59]	Time  0.744 ( 0.753)	Data  0.017 ( 0.018)	Loss 4.9705e-01 (6.1236e-01)	Acc@1  82.52 ( 77.26)
Epoch: [67][40/59]	Time  0.746 ( 0.754)	Data  0.017 ( 0.018)	Loss 5.3709e-01 (6.2557e-01)	Acc@1  80.42 ( 76.61)
The current update step is 4012
GPU_0_using curriculum 20 with window 20
Epoch: [68][20/59]	Time  0.741 ( 0.748)	Data  0.018 ( 0.023)	Loss 5.3452e-01 (6.1319e-01)	Acc@1  80.32 ( 77.64)
Epoch: [68][40/59]	Time  0.741 ( 0.749)	Data  0.016 ( 0.020)	Loss 6.4456e-01 (6.0871e-01)	Acc@1  76.32 ( 77.77)
The current update step is 4071
GPU_0_using curriculum 20 with window 20
Epoch: [69][20/59]	Time  0.746 ( 0.744)	Data  0.022 ( 0.017)	Loss 6.6346e-01 (6.3521e-01)	Acc@1  76.51 ( 76.53)
Epoch: [69][40/59]	Time  0.755 ( 0.747)	Data  0.018 ( 0.020)	Loss 6.1964e-01 (6.2397e-01)	Acc@1  77.64 ( 77.05)
The current update step is 4130
The current seed is 5293766071850973240
The current lr is: 0.001
Testing Results:
 *   Acc@1 67.934
 *   Acc@1 67.819
 *   Acc@1 69.408
 *   Acc@1 69.519
 *   Acc@1 71.461
 *   Acc@1 71.653
 *   Acc@1 73.500
 *   Acc@1 73.604
 *   Acc@1 70.842
 *   Acc@1 71.093
 *   Acc@1 70.697
 *   Acc@1 71.078
 *   Acc@1 71.829
 *   Acc@1 71.918
 *   Acc@1 73.289
 *   Acc@1 73.442
 *   Acc@1 77.605
 *   Acc@1 77.586
 *   Acc@1 77.118
 *   Acc@1 77.333
 *   Acc@1 76.868
 *   Acc@1 77.332
 *   Acc@1 76.882
 *   Acc@1 77.257
 *   Acc@1 68.763
 *   Acc@1 69.488
 *   Acc@1 65.592
 *   Acc@1 65.613
 *   Acc@1 63.592
 *   Acc@1 63.589
 *   Acc@1 61.671
 *   Acc@1 62.165
Training for 300 epoch: 71.28618421052632
Training for 600 epoch: 70.70394736842104
Training for 1000 epoch: 70.9375
Training for 3000 epoch: 71.33552631578948
Training for 300 epoch: 71.49645833333334
Training for 600 epoch: 70.88604166666667
Training for 1000 epoch: 71.12291666666665
Training for 3000 epoch: 71.61687500000001
[[71.28618421052632, 70.70394736842104, 70.9375, 71.33552631578948], [71.49645833333334, 70.88604166666667, 71.12291666666665, 71.61687500000001]]
train loss 0.30848178709348045, epoch 69, best loss 0.17140410820643107, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [70][20/59]	Time  0.731 ( 0.749)	Data  0.018 ( 0.029)	Loss 5.9233e-01 (6.0541e-01)	Acc@1  79.49 ( 77.82)
Epoch: [70][40/59]	Time  0.744 ( 0.750)	Data  0.018 ( 0.026)	Loss 5.2200e-01 (6.0839e-01)	Acc@1  80.52 ( 77.55)
The current update step is 4189
GPU_0_using curriculum 20 with window 20
Epoch: [71][20/59]	Time  0.744 ( 0.752)	Data  0.018 ( 0.018)	Loss 6.8831e-01 (6.2443e-01)	Acc@1  74.37 ( 76.65)
Epoch: [71][40/59]	Time  0.736 ( 0.752)	Data  0.018 ( 0.018)	Loss 5.4618e-01 (6.0842e-01)	Acc@1  80.37 ( 77.28)
The current update step is 4248
GPU_0_using curriculum 20 with window 20
Epoch: [72][20/59]	Time  0.736 ( 0.761)	Data  0.017 ( 0.018)	Loss 5.7480e-01 (6.4360e-01)	Acc@1  80.37 ( 75.92)
Epoch: [72][40/59]	Time  0.746 ( 0.758)	Data  0.017 ( 0.018)	Loss 8.0898e-01 (6.2097e-01)	Acc@1  69.92 ( 77.08)
The current update step is 4307
GPU_0_using curriculum 20 with window 20
Epoch: [73][20/59]	Time  0.738 ( 0.752)	Data  0.018 ( 0.023)	Loss 5.8911e-01 (5.8492e-01)	Acc@1  78.32 ( 78.10)
Epoch: [73][40/59]	Time  0.730 ( 0.751)	Data  0.016 ( 0.020)	Loss 5.7146e-01 (5.7782e-01)	Acc@1  79.98 ( 78.70)
The current update step is 4366
GPU_0_using curriculum 20 with window 20
Epoch: [74][20/59]	Time  0.765 ( 0.747)	Data  0.017 ( 0.017)	Loss 5.5450e-01 (5.9737e-01)	Acc@1  79.44 ( 78.05)
Epoch: [74][40/59]	Time  0.736 ( 0.750)	Data  0.017 ( 0.020)	Loss 6.1295e-01 (6.0325e-01)	Acc@1  78.66 ( 77.58)
The current update step is 4425
The current seed is 4880132200977852418
The current lr is: 0.001
Testing Results:
 *   Acc@1 77.566
 *   Acc@1 78.189
 *   Acc@1 77.645
 *   Acc@1 78.364
 *   Acc@1 77.789
 *   Acc@1 78.016
 *   Acc@1 75.829
 *   Acc@1 75.922
 *   Acc@1 81.842
 *   Acc@1 81.704
 *   Acc@1 80.737
 *   Acc@1 80.502
 *   Acc@1 79.763
 *   Acc@1 79.863
 *   Acc@1 79.303
 *   Acc@1 79.551
 *   Acc@1 75.000
 *   Acc@1 75.490
 *   Acc@1 74.013
 *   Acc@1 74.504
 *   Acc@1 74.842
 *   Acc@1 75.051
 *   Acc@1 76.066
 *   Acc@1 76.248
 *   Acc@1 76.618
 *   Acc@1 76.992
 *   Acc@1 76.289
 *   Acc@1 76.536
 *   Acc@1 75.368
 *   Acc@1 75.997
 *   Acc@1 75.697
 *   Acc@1 75.930
Training for 300 epoch: 77.75657894736841
Training for 600 epoch: 77.17105263157895
Training for 1000 epoch: 76.9407894736842
Training for 3000 epoch: 76.72368421052633
Training for 300 epoch: 78.09375
Training for 600 epoch: 77.47666666666666
Training for 1000 epoch: 77.23166666666667
Training for 3000 epoch: 76.9125
[[77.75657894736841, 77.17105263157895, 76.9407894736842, 76.72368421052633], [78.09375, 77.47666666666666, 77.23166666666667, 76.9125]]
train loss 0.19563417746225992, epoch 74, best loss 0.17140410820643107, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [75][20/59]	Time  0.727 ( 0.748)	Data  0.017 ( 0.029)	Loss 6.1033e-01 (5.8418e-01)	Acc@1  76.32 ( 78.64)
Epoch: [75][40/59]	Time  0.757 ( 0.751)	Data  0.017 ( 0.026)	Loss 5.0191e-01 (5.8749e-01)	Acc@1  81.88 ( 78.39)
The current update step is 4484
GPU_0_using curriculum 20 with window 20
Epoch: [76][20/59]	Time  0.735 ( 0.761)	Data  0.022 ( 0.018)	Loss 5.5149e-01 (6.0266e-01)	Acc@1  79.88 ( 77.67)
Epoch: [76][40/59]	Time  0.747 ( 0.757)	Data  0.017 ( 0.018)	Loss 5.1277e-01 (6.2918e-01)	Acc@1  81.79 ( 76.79)
The current update step is 4543
GPU_0_using curriculum 20 with window 20
Epoch: [77][20/59]	Time  0.758 ( 0.754)	Data  0.017 ( 0.018)	Loss 5.4906e-01 (5.7770e-01)	Acc@1  80.62 ( 78.82)
Epoch: [77][40/59]	Time  0.753 ( 0.753)	Data  0.017 ( 0.018)	Loss 5.6376e-01 (5.7993e-01)	Acc@1  80.32 ( 78.78)
The current update step is 4602
GPU_0_using curriculum 20 with window 20
Epoch: [78][20/59]	Time  0.737 ( 0.755)	Data  0.018 ( 0.024)	Loss 5.6364e-01 (6.3739e-01)	Acc@1  79.39 ( 76.59)
Epoch: [78][40/59]	Time  0.742 ( 0.756)	Data  0.017 ( 0.021)	Loss 5.4003e-01 (6.2384e-01)	Acc@1  80.76 ( 77.03)
The current update step is 4661
GPU_0_using curriculum 20 with window 20
Epoch: [79][20/59]	Time  0.752 ( 0.750)	Data  0.017 ( 0.018)	Loss 6.3822e-01 (6.5650e-01)	Acc@1  76.17 ( 75.45)
Epoch: [79][40/59]	Time  0.733 ( 0.752)	Data  0.018 ( 0.021)	Loss 5.5280e-01 (6.3523e-01)	Acc@1  78.47 ( 76.29)
The current update step is 4720
The current seed is 2309368646208971959
The current lr is: 0.001
Testing Results:
 *   Acc@1 56.000
 *   Acc@1 56.501
 *   Acc@1 55.618
 *   Acc@1 55.877
 *   Acc@1 55.658
 *   Acc@1 55.758
 *   Acc@1 54.368
 *   Acc@1 55.013
 *   Acc@1 73.776
 *   Acc@1 73.814
 *   Acc@1 70.645
 *   Acc@1 70.921
 *   Acc@1 71.632
 *   Acc@1 71.885
 *   Acc@1 71.474
 *   Acc@1 71.588
 *   Acc@1 66.868
 *   Acc@1 66.881
 *   Acc@1 64.171
 *   Acc@1 64.425
 *   Acc@1 62.987
 *   Acc@1 63.174
 *   Acc@1 61.908
 *   Acc@1 62.227
 *   Acc@1 61.355
 *   Acc@1 62.076
 *   Acc@1 58.276
 *   Acc@1 59.056
 *   Acc@1 56.053
 *   Acc@1 56.886
 *   Acc@1 51.579
 *   Acc@1 51.867
Training for 300 epoch: 64.5
Training for 600 epoch: 62.17763157894737
Training for 1000 epoch: 61.58223684210526
Training for 3000 epoch: 59.83223684210526
Training for 300 epoch: 64.81791666666666
Training for 600 epoch: 62.569583333333334
Training for 1000 epoch: 61.92583333333333
Training for 3000 epoch: 60.17395833333334
[[64.5, 62.17763157894737, 61.58223684210526, 59.83223684210526], [64.81791666666666, 62.569583333333334, 61.92583333333333, 60.17395833333334]]
train loss 0.4276801177183787, epoch 79, best loss 0.17140410820643107, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [80][20/59]	Time  0.737 ( 0.752)	Data  0.018 ( 0.029)	Loss 5.0392e-01 (6.3407e-01)	Acc@1  81.49 ( 77.05)
Epoch: [80][40/59]	Time  0.739 ( 0.751)	Data  0.018 ( 0.026)	Loss 6.0212e-01 (6.2257e-01)	Acc@1  79.00 ( 77.14)
The current update step is 4779
GPU_0_using curriculum 20 with window 20
Epoch: [81][20/59]	Time  0.738 ( 0.751)	Data  0.018 ( 0.018)	Loss 6.2427e-01 (6.1546e-01)	Acc@1  77.00 ( 77.45)
Epoch: [81][40/59]	Time  0.734 ( 0.750)	Data  0.017 ( 0.018)	Loss 7.7416e-01 (6.3063e-01)	Acc@1  68.90 ( 76.57)
The current update step is 4838
GPU_0_using curriculum 20 with window 20
Epoch: [82][20/59]	Time  0.737 ( 0.749)	Data  0.017 ( 0.018)	Loss 7.1785e-01 (6.0657e-01)	Acc@1  73.44 ( 77.21)
Epoch: [82][40/59]	Time  0.746 ( 0.751)	Data  0.016 ( 0.018)	Loss 6.0380e-01 (6.0240e-01)	Acc@1  76.56 ( 77.49)
The current update step is 4897
GPU_0_using curriculum 20 with window 20
Epoch: [83][20/59]	Time  0.730 ( 0.748)	Data  0.017 ( 0.024)	Loss 6.4118e-01 (6.2385e-01)	Acc@1  75.29 ( 77.17)
Epoch: [83][40/59]	Time  0.736 ( 0.749)	Data  0.017 ( 0.020)	Loss 7.5258e-01 (6.1692e-01)	Acc@1  69.73 ( 77.28)
The current update step is 4956
GPU_0_using curriculum 20 with window 20
Epoch: [84][20/59]	Time  0.733 ( 0.744)	Data  0.018 ( 0.018)	Loss 6.7665e-01 (6.1377e-01)	Acc@1  76.37 ( 77.24)
Epoch: [84][40/59]	Time  0.735 ( 0.745)	Data  0.018 ( 0.021)	Loss 5.4037e-01 (5.9861e-01)	Acc@1  81.01 ( 77.89)
The current update step is 5015
The current seed is 7789504037710017370
The current lr is: 0.001
Testing Results:
 *   Acc@1 55.513
 *   Acc@1 56.223
 *   Acc@1 53.368
 *   Acc@1 54.029
 *   Acc@1 52.526
 *   Acc@1 52.854
 *   Acc@1 51.276
 *   Acc@1 51.591
 *   Acc@1 72.250
 *   Acc@1 72.422
 *   Acc@1 72.000
 *   Acc@1 72.025
 *   Acc@1 70.368
 *   Acc@1 70.457
 *   Acc@1 69.039
 *   Acc@1 68.647
 *   Acc@1 80.961
 *   Acc@1 81.127
 *   Acc@1 76.711
 *   Acc@1 77.094
 *   Acc@1 74.355
 *   Acc@1 74.582
 *   Acc@1 72.711
 *   Acc@1 72.820
 *   Acc@1 74.868
 *   Acc@1 75.156
 *   Acc@1 72.697
 *   Acc@1 73.326
 *   Acc@1 73.118
 *   Acc@1 73.233
 *   Acc@1 72.645
 *   Acc@1 73.093
Training for 300 epoch: 70.89802631578948
Training for 600 epoch: 68.69407894736842
Training for 1000 epoch: 67.59210526315789
Training for 3000 epoch: 66.41776315789474
Training for 300 epoch: 71.23208333333334
Training for 600 epoch: 69.11854166666667
Training for 1000 epoch: 67.78145833333333
Training for 3000 epoch: 66.53791666666666
[[70.89802631578948, 68.69407894736842, 67.59210526315789, 66.41776315789474], [71.23208333333334, 69.11854166666667, 67.78145833333333, 66.53791666666666]]
train loss 0.19993908460140228, epoch 84, best loss 0.17140410820643107, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [85][20/59]	Time  0.749 ( 0.759)	Data  0.017 ( 0.030)	Loss 6.0727e-01 (6.4483e-01)	Acc@1  78.17 ( 76.25)
Epoch: [85][40/59]	Time  0.746 ( 0.759)	Data  0.017 ( 0.027)	Loss 5.9537e-01 (6.2922e-01)	Acc@1  77.44 ( 76.69)
The current update step is 5074
GPU_0_using curriculum 20 with window 20
Epoch: [86][20/59]	Time  0.746 ( 0.759)	Data  0.018 ( 0.019)	Loss 4.8996e-01 (6.1735e-01)	Acc@1  82.81 ( 77.48)
Epoch: [86][40/59]	Time  0.734 ( 0.756)	Data  0.017 ( 0.018)	Loss 5.2359e-01 (6.1680e-01)	Acc@1  81.30 ( 77.40)
The current update step is 5133
GPU_0_using curriculum 20 with window 20
Epoch: [87][20/59]	Time  0.748 ( 0.752)	Data  0.017 ( 0.018)	Loss 5.6951e-01 (6.2250e-01)	Acc@1  77.25 ( 76.91)
Epoch: [87][40/59]	Time  0.763 ( 0.756)	Data  0.016 ( 0.018)	Loss 5.5087e-01 (6.0926e-01)	Acc@1  80.08 ( 77.36)
The current update step is 5192
GPU_0_using curriculum 20 with window 20
Epoch: [88][20/59]	Time  0.733 ( 0.752)	Data  0.018 ( 0.024)	Loss 6.0002e-01 (6.2732e-01)	Acc@1  77.59 ( 76.46)
Epoch: [88][40/59]	Time  0.741 ( 0.754)	Data  0.018 ( 0.021)	Loss 5.4517e-01 (6.1573e-01)	Acc@1  80.52 ( 77.06)
The current update step is 5251
GPU_0_using curriculum 20 with window 20
Epoch: [89][20/59]	Time  0.740 ( 0.748)	Data  0.018 ( 0.018)	Loss 6.1959e-01 (6.0004e-01)	Acc@1  76.61 ( 77.95)
Epoch: [89][40/59]	Time  0.750 ( 0.751)	Data  0.018 ( 0.021)	Loss 5.7513e-01 (5.8331e-01)	Acc@1  79.39 ( 78.62)
The current update step is 5310
The current seed is 11527669776667295426
The current lr is: 0.001
Testing Results:
 *   Acc@1 79.145
 *   Acc@1 79.176
 *   Acc@1 78.750
 *   Acc@1 78.702
 *   Acc@1 78.776
 *   Acc@1 79.254
 *   Acc@1 78.750
 *   Acc@1 78.884
 *   Acc@1 67.711
 *   Acc@1 67.580
 *   Acc@1 66.579
 *   Acc@1 67.301
 *   Acc@1 68.382
 *   Acc@1 68.483
 *   Acc@1 70.803
 *   Acc@1 71.258
 *   Acc@1 78.395
 *   Acc@1 78.920
 *   Acc@1 79.553
 *   Acc@1 80.376
 *   Acc@1 79.816
 *   Acc@1 80.203
 *   Acc@1 78.408
 *   Acc@1 78.947
 *   Acc@1 77.066
 *   Acc@1 77.444
 *   Acc@1 76.105
 *   Acc@1 76.545
 *   Acc@1 75.039
 *   Acc@1 75.921
 *   Acc@1 69.645
 *   Acc@1 70.040
Training for 300 epoch: 75.57894736842105
Training for 600 epoch: 75.24671052631578
Training for 1000 epoch: 75.5032894736842
Training for 3000 epoch: 74.40131578947368
Training for 300 epoch: 75.78
Training for 600 epoch: 75.73083333333334
Training for 1000 epoch: 75.96520833333334
Training for 3000 epoch: 74.78229166666668
[[75.57894736842105, 75.24671052631578, 75.5032894736842, 74.40131578947368], [75.78, 75.73083333333334, 75.96520833333334, 74.78229166666668]]
train loss 0.23663348140716553, epoch 89, best loss 0.17140410820643107, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [90][20/59]	Time  0.738 ( 0.751)	Data  0.018 ( 0.029)	Loss 6.1677e-01 (6.3480e-01)	Acc@1  78.32 ( 77.11)
Epoch: [90][40/59]	Time  0.740 ( 0.753)	Data  0.017 ( 0.026)	Loss 5.9845e-01 (6.2974e-01)	Acc@1  77.98 ( 76.86)
The current update step is 5369
GPU_0_using curriculum 20 with window 20
Epoch: [91][20/59]	Time  0.740 ( 0.755)	Data  0.017 ( 0.018)	Loss 5.9583e-01 (5.7627e-01)	Acc@1  79.59 ( 78.96)
Epoch: [91][40/59]	Time  0.739 ( 0.754)	Data  0.018 ( 0.018)	Loss 6.0420e-01 (5.7181e-01)	Acc@1  75.20 ( 78.97)
The current update step is 5428
GPU_0_using curriculum 20 with window 20
Epoch: [92][20/59]	Time  0.732 ( 0.757)	Data  0.017 ( 0.018)	Loss 6.0003e-01 (6.0778e-01)	Acc@1  78.56 ( 77.51)
Epoch: [92][40/59]	Time  0.735 ( 0.755)	Data  0.017 ( 0.018)	Loss 5.0873e-01 (5.9404e-01)	Acc@1  82.28 ( 78.07)
The current update step is 5487
GPU_0_using curriculum 20 with window 20
Epoch: [93][20/59]	Time  0.755 ( 0.755)	Data  0.019 ( 0.024)	Loss 5.2830e-01 (5.9167e-01)	Acc@1  81.45 ( 78.42)
Epoch: [93][40/59]	Time  0.741 ( 0.752)	Data  0.018 ( 0.021)	Loss 5.3813e-01 (6.1031e-01)	Acc@1  79.74 ( 77.76)
The current update step is 5546
GPU_0_using curriculum 20 with window 20
Epoch: [94][20/59]	Time  0.759 ( 0.745)	Data  0.017 ( 0.018)	Loss 5.1665e-01 (6.1615e-01)	Acc@1  81.59 ( 77.21)
Epoch: [94][40/59]	Time  0.729 ( 0.748)	Data  0.017 ( 0.021)	Loss 8.7604e-01 (6.0987e-01)	Acc@1  64.50 ( 77.46)
The current update step is 5605
The current seed is 7115847789557442061
The current lr is: 0.001
Testing Results:
 *   Acc@1 74.105
 *   Acc@1 74.491
 *   Acc@1 72.658
 *   Acc@1 72.884
 *   Acc@1 72.382
 *   Acc@1 73.360
 *   Acc@1 73.263
 *   Acc@1 73.813
 *   Acc@1 70.763
 *   Acc@1 71.449
 *   Acc@1 68.737
 *   Acc@1 69.267
 *   Acc@1 68.711
 *   Acc@1 69.028
 *   Acc@1 69.763
 *   Acc@1 69.993
 *   Acc@1 74.632
 *   Acc@1 75.489
 *   Acc@1 74.408
 *   Acc@1 74.971
 *   Acc@1 71.487
 *   Acc@1 72.358
 *   Acc@1 65.908
 *   Acc@1 66.115
 *   Acc@1 61.882
 *   Acc@1 62.320
 *   Acc@1 61.132
 *   Acc@1 61.733
 *   Acc@1 61.737
 *   Acc@1 62.561
 *   Acc@1 63.092
 *   Acc@1 63.474
Training for 300 epoch: 70.34539473684211
Training for 600 epoch: 69.23355263157895
Training for 1000 epoch: 68.57894736842107
Training for 3000 epoch: 68.00657894736842
Training for 300 epoch: 70.93729166666667
Training for 600 epoch: 69.71395833333334
Training for 1000 epoch: 69.32645833333333
Training for 3000 epoch: 68.34875000000001
[[70.34539473684211, 69.23355263157895, 68.57894736842107, 68.00657894736842], [70.93729166666667, 69.71395833333334, 69.32645833333333, 68.34875000000001]]
train loss 0.32454125412305196, epoch 94, best loss 0.17140410820643107, best_epoch 64
GPU_0_using curriculum 20 with window 20
Epoch: [95][20/59]	Time  0.737 ( 0.750)	Data  0.017 ( 0.029)	Loss 8.9824e-01 (6.2716e-01)	Acc@1  66.60 ( 76.45)
Epoch: [95][40/59]	Time  0.740 ( 0.753)	Data  0.018 ( 0.026)	Loss 7.7920e-01 (6.2386e-01)	Acc@1  67.77 ( 76.53)
The current update step is 5664
GPU_0_using curriculum 20 with window 20
Epoch: [96][20/59]	Time  0.763 ( 0.758)	Data  0.019 ( 0.018)	Loss 4.9573e-01 (5.8659e-01)	Acc@1  82.28 ( 78.52)
Epoch: [96][40/59]	Time  0.737 ( 0.765)	Data  0.018 ( 0.018)	Loss 5.8990e-01 (5.8367e-01)	Acc@1  78.96 ( 78.56)
The current update step is 5723
GPU_0_using curriculum 20 with window 20
Epoch: [97][20/59]	Time  0.741 ( 0.752)	Data  0.018 ( 0.018)	Loss 6.2788e-01 (6.1692e-01)	Acc@1  78.03 ( 76.83)
Epoch: [97][40/59]	Time  0.739 ( 0.754)	Data  0.018 ( 0.018)	Loss 5.6394e-01 (6.0296e-01)	Acc@1  79.83 ( 77.56)
The current update step is 5782
GPU_0_using curriculum 20 with window 20
Epoch: [98][20/59]	Time  0.738 ( 0.752)	Data  0.018 ( 0.024)	Loss 7.2116e-01 (5.7542e-01)	Acc@1  70.80 ( 78.83)
Epoch: [98][40/59]	Time  0.741 ( 0.752)	Data  0.017 ( 0.021)	Loss 6.8006e-01 (5.8290e-01)	Acc@1  75.24 ( 78.51)
The current update step is 5841
GPU_0_using curriculum 20 with window 20
Epoch: [99][20/59]	Time  0.731 ( 0.745)	Data  0.018 ( 0.018)	Loss 5.2086e-01 (5.8650e-01)	Acc@1  80.66 ( 78.44)
Epoch: [99][40/59]	Time  0.742 ( 0.747)	Data  0.018 ( 0.021)	Loss 6.6090e-01 (5.9575e-01)	Acc@1  75.59 ( 77.98)
The current update step is 5900
The current seed is 7865060334466706236
The current lr is: 0.001
Testing Results:
 *   Acc@1 75.382
 *   Acc@1 75.446
 *   Acc@1 75.447
 *   Acc@1 75.597
 *   Acc@1 75.105
 *   Acc@1 75.195
 *   Acc@1 72.803
 *   Acc@1 73.105
 *   Acc@1 75.382
 *   Acc@1 75.482
 *   Acc@1 73.211
 *   Acc@1 74.008
 *   Acc@1 71.316
 *   Acc@1 71.880
 *   Acc@1 65.105
 *   Acc@1 65.346
 *   Acc@1 73.697
 *   Acc@1 74.650
 *   Acc@1 74.145
 *   Acc@1 75.046
 *   Acc@1 74.974
 *   Acc@1 75.537
 *   Acc@1 75.316
 *   Acc@1 75.646
 *   Acc@1 70.289
 *   Acc@1 70.693
 *   Acc@1 63.355
 *   Acc@1 64.743
 *   Acc@1 61.658
 *   Acc@1 62.718
 *   Acc@1 59.882
 *   Acc@1 60.922
Training for 300 epoch: 73.6875
Training for 600 epoch: 71.53947368421052
Training for 1000 epoch: 70.76315789473684
Training for 3000 epoch: 68.27631578947368
Training for 300 epoch: 74.06770833333334
Training for 600 epoch: 72.34854166666668
Training for 1000 epoch: 71.33270833333333
Training for 3000 epoch: 68.75458333333333
[[73.6875, 71.53947368421052, 70.76315789473684, 68.27631578947368], [74.06770833333334, 72.34854166666668, 71.33270833333333, 68.75458333333333]]
train loss 0.28912862334251405, epoch 99, best loss 0.17140410820643107, best_epoch 64
=== Final results:
{'acc': 77.75657894736841, 'test': [77.75657894736841, 77.17105263157895, 76.9407894736842, 76.72368421052633], 'train': [77.75657894736841, 77.17105263157895, 76.9407894736842, 76.72368421052633], 'ind': 0, 'epoch': 75, 'data': array([[-0.13618569, -0.06269418,  0.01874579, ...,  0.088214  ,
         0.00292933,  0.00029217],
       [-0.0802004 ,  0.00740849,  0.07787484, ..., -0.00906077,
        -0.01677181,  0.03334358],
       [-0.09989087,  0.06900544, -0.04457225, ...,  0.00470384,
         0.07977749, -0.00703216],
       ...,
       [-0.0280369 ,  0.05761309, -0.01754107, ..., -0.03785583,
        -0.09365493,  0.00902658],
       [-0.05859071,  0.05422846, -0.00394955, ...,  0.01160921,
        -0.07901961,  0.00984956],
       [ 0.06058909,  0.0375042 ,  0.01146739, ...,  0.0439359 ,
        -0.0005664 , -0.10047384]], shape=(20, 768), dtype=float32)}
