# Embarrassingly Simple Dataset Distillation For Natural Language Processing ğŸ“ŠğŸ’¡

## ğŸ” Project Overview
This repository accompanies the NYU CDS project **â€œEmbarrassingly Simple Dataset Distillation for Natural Language Processingâ€**. Building on the RaTâ€‘BPTT and Boostâ€‘DD strategies from the CV literature, we distill small sets of *continuous CLS embeddings* that allow lightweight student models (Text MLP or Transformer) to match or outperform randomly sampled real subsets on MRPC and AG News.

Key ideas:
- Distillation is formulated as a bilevel problem. The inner loop trains a student model on synthetic embeddings, while the outer loop updates the embeddings via (RaT-)BPTT.
- RaTâ€‘BPTT stabilizes gradients by randomly truncating long unrolls, keeping runtime manageable even for Transformer students.
- Boost-DD warm-starts higher IPC stages with previously distilled checkpoints, yielding more stable solutions for large synthetic budgets.
- All experiments run entirely in embedding space using frozen sentence encoders, which makes the approach â€œembarrassingly simpleâ€ to deploy for new NLP datasets.

## ğŸ“ Tutorial & Repository Structure

```
text-distillation/
â”œâ”€â”€ main.py                     # Unified training entry point (MLP or Transformer students)
â”œâ”€â”€ framework/                  # Core distillation components (datasets, optimizers, utilities)
â”œâ”€â”€ scripts/agnews_emb/         # Frozen CLS embeddings for AG News (generated via step0)
â”œâ”€â”€ sbatch/                     # SLURM batch scripts for every experiment stage
â”œâ”€â”€ eval_*.py                   # Evaluation utilities for distilled vs. random baselines
â”œâ”€â”€ train_real_baseline.py      # Optional real-data baseline training script
â””â”€â”€ logs/                       # Saved stdout/stderr for every sbatch job
```

### ğŸ§© Core Code Components
- **`main.py`** â€“ Parses CLI flags and launches dataset distillation (supports RaT-BPTT, Full-BPTT, Boost-DD warm starts).
- **`framework/`**
  - `base.py` â€“ Bilevel training loop, evaluation routines, checkpointing.
  - `config.py` â€“ Model/dataset configs and scheduling utilities.
  - `util.py` â€“ Augmentation helpers, projection utilities, Boost-DD helpers.
- **Evaluation scripts**
  - `eval_step5_distill.py` â€“ Re-trains student models on `.h5` distilled sets and reports mean Â± std accuracy across seeds.
  - `eval_step5_train.py` / `eval_step5_baseline_random_agnews.py` â€“ Support scripts for random baseline comparisons.
- **Embedding scripts** (`sbatch/step0_prepare_embeddings.sbatch`) â€“ Generates CLS tensors for new datasets via frozen encoders.

### Running the Pipeline (HPC Workflow)
1. **Prepare embeddings**  
   `sbatch sbatch/step0_prepare_embeddings.sbatch`
2. **Text MLP RaT-BPTT & Boost-DD stages**  
   `sbatch sbatch/mlp_ratbptt_ipc1_10_50.sbatch`, `sbatch sbatch/mlp_ratbptt_ipc5_20.sbatch`, etc.
3. **Text MLP Full-BPTT stages**  
   `sbatch sbatch/mlp_fullbptt_ipc1_10_50.sbatch`, `sbatch sbatch/mlp_fullbptt_ipc5_20.sbatch`.
4. **Transformer Full-BPTT or RaT-BPTT**  
   `sbatch sbatch/tf_fullbptt_ipc1_5_10_20_50.sbatch`, `sbatch/tf_ratbptt_ipc1_5_10_20_50.sbatch`, plus extended-epoch variants.
5. **Evaluation**  
   `sbatch sbatch/eval_tf_fullbptt_ipc1_5_10_20_50.sbatch`, `sbatch/sbatch/eval_mlp_*`, or `python eval_step5_distill.py ...`.

Each SLURM script writes `.out/.err` logs under `logs/`, and distilled checkpoints (`*.h5`, `*.pth`) reside in `./checkpoints/`.

## References
The project draws on the literature cited in the accompanying report â€œSmall_data_Project (6).pdfâ€:

1. Brown, T. B., Mann, B., Ryder, N., *et al.* (2020). **Language Models are Few-Shot Learners.** *NeurIPS.*
2. Carlini, N., Tramer, F., Wallace, E., *et al.* (2021). **Extracting Training Data from Large Language Models.** *USENIX Security Symposium.*
3. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.** *NAACL.*
4. Feng, W., Huang, Y., Liu, Y., Zhang, Z., Li, Y., & Wang, L. (2023). **Embarrassingly Simple Dataset Distillation.** *ICLR.*
5. Maekawa, T., Tanaka, S., & Okazaki, N. (2023). **Dataset Distillation with Attention Labels for Fine-Tuning BERT.** *Findings of ACL.*
6. Schick, T., & SchÃ¼tze, H. (2020). **Itâ€™s Not Just Size That Matters: Small Language Models are also Few-Shot Learners.** *NAACL.*
7. Wang, T., Zhu, J.-Y., Torralba, A., & Efros, A. A. (2018). **Dataset Distillation.** *CVPR.*
8. Williams, R. J., & Peng, J. (1990). **An Efficient Gradient-Based Algorithm for On-line Training of Recurrent Network Trajectories.** *Neural Computation.*

---
For full experimental details, plots, and scripted commands, refer to the `analysis/` artifacts and the PDF report included in this repository.
